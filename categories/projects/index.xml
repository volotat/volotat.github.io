<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects on Alexey Borsky</title><link>https://volotat.github.io/categories/projects/</link><description>Recent content in Projects on Alexey Borsky</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 15 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://volotat.github.io/categories/projects/index.xml" rel="self" type="application/rss+xml"/><item><title>4D Volumetric Retina Simulation</title><link>https://volotat.github.io/projects/2025-05-15-4d-volumetric-retina-simulation/</link><pubDate>Thu, 15 May 2025 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2025-05-15-4d-volumetric-retina-simulation/</guid><description>&lt;img src="https://volotat.github.io/projects/2025-05-15-4d-volumetric-retina-simulation/4D_eye_preview.png" alt="Featured image of post 4D Volumetric Retina Simulation" /&gt;&lt;p&gt;A physics-based 4D path tracing simulation visualizing how a four-dimensional being might perceive the world through a 3D volumetric retina.&lt;/p&gt;
&lt;p&gt;Current visualizations of 4D space often rely on wireframe projections or simple 3D cross-sections. This project takes a more biologically plausible approach: analogous to how we 3D beings perceive our world via 2D retinas, a 4D creature would likely possess a 3D (volumetric) retina.&lt;/p&gt;
&lt;p&gt;This simulation implements a custom 4D path tracing engine (using Python and Taichi for GPU acceleration) to model light interactions within a hyper-scene containing a rotating tesseract. It simulates image formation by casting 4D rays onto a defined 3D retinal volume.&lt;/p&gt;
&lt;p&gt;The simulation features physically-based rendering that models light bounces, shadows, and perspective in four spatial dimensions. Simulates a 3D sensor array rather than a flat plane. Implements a Gaussian fall-off for retinal sensitivity, mimicking foveal vision where the center of the 3D gaze is most acute. To make this comprehensible to human eyes, the 3D retinal image is composited from multiple depth slices, additively blended to represent the density of information a 4D being would process simultaneously.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/4DRender" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt; | &lt;a class="link" href="https://www.youtube.com/shorts/T697zlLvvHw" target="_blank" rel="noopener"
&gt;Video Showcase&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Anagnorisis. Part 3: Why Should You Go Local?</title><link>https://volotat.github.io/p/anagnorisis-part-3-why-should-you-go-local/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://volotat.github.io/p/anagnorisis-part-3-why-should-you-go-local/</guid><description>&lt;img src="https://volotat.github.io/p/anagnorisis-part-3-why-should-you-go-local/External-Hard-Drive-Hero.png" alt="Featured image of post Anagnorisis. Part 3: Why Should You Go Local?" /&gt;&lt;h2 id="introduction"&gt;Introduction
&lt;/h2&gt;&lt;p&gt;In today&amp;rsquo;s digital age, an increasing amount of personal data is stored on cloud services, often beyond our direct control. While these platforms offer convenience, they also pose significant risks regarding data ownership, privacy, and longevity. Recent incidents, such as the cyberattacks on the &lt;a class="link" href="https://web.archive.org/" target="_blank" rel="noopener"
&gt;Internet Archive&lt;/a&gt;, highlight the vulnerability of digital repositories. In October 2024, the Internet Archive suffered a data breach that &lt;a class="link" href="https://www.wired.com/story/internet-archive-hacked/" target="_blank" rel="noopener"
&gt;exposed&lt;/a&gt; information from over 31 million user accounts. This breach was accompanied by DDoS attacks, leading to intermittent service &lt;a class="link" href="https://www.theverge.com/2024/10/14/24269741/internet-archive-online-read-only-data-breach-outage" target="_blank" rel="noopener"
&gt;disruptions&lt;/a&gt;. Furthermore in March 2023, a U.S. District Court ruled that the Internet Archive&amp;rsquo;s practice of lending digitized books online constituted copyright infringement, leading to a permanent &lt;a class="link" href="https://en.wikipedia.org/wiki/Hachette_v._Internet_Archive" target="_blank" rel="noopener"
&gt;injunction&lt;/a&gt; against such activities. These events underscore how external factors can &lt;a class="link" href="https://brownstone.org/articles/they-are-scrubbing-the-internet-right-now/" target="_blank" rel="noopener"
&gt;jeopardize&lt;/a&gt; access to information stored online.&lt;/p&gt;
&lt;p&gt;Moreover, data loss from cloud services is not uncommon. Service outages, cyberattacks, and policy changes can result in the sudden unavailability of personal data. For instance, breaches targeting cloud storage providers have exposed sensitive user information, emphasizing the precarious nature of entrusting data to third-party platforms.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An external hard drive could provide up to &lt;a class="link" href="https://www.theverge.com/2024/12/18/24324143/seagate-32tb-hamr-hard-drive" target="_blank" rel="noopener"
&gt;32TB of storage&lt;/a&gt; as of january 2025&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="your-data-as-a-reflection-of-your-personality"&gt;Your Data as a Reflection of Your Personality
&lt;/h2&gt;&lt;p&gt;Every digital interaction we engage in - be it social media activity, spending habits, or smartphone usage - leaves behind a &lt;a class="link" href="https://www.psychologicalscience.org/news/releases/spending-data-personality.html" target="_blank" rel="noopener"
&gt;footprint&lt;/a&gt; that mirrors aspects of our personality. Similarly, patterns in smartphone usage &lt;a class="link" href="https://pubmed.ncbi.nlm.nih.gov/36738137/" target="_blank" rel="noopener"
&gt;have been linked&lt;/a&gt; to the Big Five personality traits, providing insights into an individual&amp;rsquo;s behavior and preferences.
These digital traces are not just passive records, they actively contribute to the construction of our digital personas. Machine learning algorithms can analyze this data to &lt;a class="link" href="https://pubmed.ncbi.nlm.nih.gov/38753373/" target="_blank" rel="noopener"
&gt;predict personality traits&lt;/a&gt;, highlighting the intimate connection between our data and our identities.&lt;/p&gt;
&lt;h2 id="the-case-for-localstorage"&gt;The Case for Local Storage
&lt;/h2&gt;&lt;p&gt;To mitigate these risks, transitioning to local storage solutions is a prudent strategy. By maintaining data on personal devices or dedicated local servers, individuals retain full control over their information, ensuring accessibility regardless of external circumstances. This approach not only safeguards against unforeseen disruptions but also enhances privacy and data security.&lt;/p&gt;
&lt;p&gt;Recognizing the need for accessible local storage management, the &lt;a class="link" href="https://github.com/volotat/Anagnorisis" target="_blank" rel="noopener"
&gt;Anagnorisis&lt;/a&gt; project is developing as a way to assist users in organizing and preserving their data effectively. The idea behind Anagnorisis is to offer a set of tools that facilitate migration from cloud services such as Notion, Spotify, Pinterest or Youtube to local environments, allowing users to easily find, rate and enjoy their data just as effectively as with any of these services. By providing a user-friendly interface and AI-powered features, Anagnorisis ensures that personal data remains under the user&amp;rsquo;s control, protected from external threats and accessible for future generations.&lt;/p&gt;
&lt;h2 id="digital-immortality-through-data-preservation"&gt;Digital Immortality Through Data Preservation
&lt;/h2&gt;&lt;p&gt;While preserving the human mind in its entirety remains beyond current technological capabilities, each piece of data we create serves as a fragment of our identity. By conscientiously preserving this data, we offer future generations the opportunity to reconstruct a digital approximation of our personalities. This form of digital immortality is attainable today, allowing us to leave behind a legacy that extends beyond our physical existence. The ghost of our past self that can support our descendants way beyond our physical death.&lt;/p&gt;
&lt;p&gt;Moreover, by engaging in this conscious attempt to preserve our personality, we may begin to treat our data differently. Instead of hoarding vast amounts of trivial, mostly situational information, that happened to be stored on our SSD and hard drives, we might prioritize storing only the most important and meaningful parts of it. This approach encourages us to reflect on the value of our contributions and fosters a more intentional interaction with our digital environment. Writing down our thoughts more frequently - perhaps not for others to read but as a way to create more data points. These records can serve as the basis for training future machine learning models, creating digital constructs that closely resemble our true selves.&lt;/p&gt;
&lt;h2 id="in-theend"&gt;In the end
&lt;/h2&gt;&lt;p&gt;While cloud services offer undeniable convenience, the potential risks associated with relinquishing control over personal data cannot be overlooked. Embracing local storage solutions, supported by tools like Anagnorisis, provides a secure and reliable alternative, preserving the integrity and accessibility of our digital lives.&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="link" href="https://github.com/volotat/Anagnorisis" target="_blank" rel="noopener"
&gt;https://github.com/volotat/Anagnorisis&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Anagnorisis. Part 2: The Music Recommendation Algorithm.</title><link>https://volotat.github.io/p/anagnorisis-part-2-the-music-recommendation-algorithm/</link><pubDate>Sun, 22 Sep 2024 00:00:00 +0000</pubDate><guid>https://volotat.github.io/p/anagnorisis-part-2-the-music-recommendation-algorithm/</guid><description>&lt;img src="https://volotat.github.io/p/anagnorisis-part-2-the-music-recommendation-algorithm/AI.jpg" alt="Featured image of post Anagnorisis. Part 2: The Music Recommendation Algorithm." /&gt;&lt;h2 id="introduction"&gt;Introduction
&lt;/h2&gt;&lt;p&gt;One of the main ideas of Anagnorisis is to have completely open recommendation engine available for users to not only view but also modify as they wish. When a user uses the music player integrated into the project, they have the ability to influence how often songs are played by rating them and based on those ratings training personal model that can estimate the ratings of the user.&lt;/p&gt;
&lt;p&gt;In particular the mechanism looks like this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User sets up the path to local music folder&lt;/li&gt;
&lt;li&gt;Rates some of the songs according to their own preferences&lt;/li&gt;
&lt;li&gt;After gathering some data goes to “Train” page and press “Train music evaluator” to train the preference model&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After the training is complete one can enjoy their music, but this time, when the music is selected it will be rated by the model (in case it was not rated by the user already) and therefore adjust the probability of it occurring in the playlist.&lt;/p&gt;
&lt;p&gt;To calculate the actual probability, the music recommendation system combines various factors — user ratings, model predictions, play history, and skips, as well as time since the last play of the song — into a mathematical formula that determines the probability of each song being selected. By balancing these influences, the system provides a personalized and dynamic music recommendation experience. Below is a more detailed explanation of the math behind the algorithm.&lt;/p&gt;
&lt;h2 id="music-recommendation-system"&gt;Music recommendation system
&lt;/h2&gt;&lt;p&gt;Music recommendations are based on the two scores: user scores and internal score. While the user score is static, the internal score is dependent on the act of listening. The initial internal score is always 0, it gets higher if the user listens to the whole song and gets lower if the user skips it.&lt;/p&gt;
&lt;h2 id="calculating-probabilies-for-song-selection"&gt;Calculating probabilies for song selection
&lt;/h2&gt;&lt;h3 id="user-ratings-and-missing-data"&gt;User Ratings and Missing Data
&lt;/h3&gt;&lt;p&gt;The starting point is user ratings. If a song has a rating, it uses that; if not, it uses a fallback mechanism.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;User rating&lt;/strong&gt;: $ R_u $&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model rating&lt;/strong&gt;: $ R_m $&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mean user rating&lt;/strong&gt;: $ R_{\text{mean}} $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For each song, the system checks whether a user or model rating exists. If neither is present, the system assumes the rating is the mean value of all user ratings.&lt;/p&gt;
$$
R = \begin{cases}
R_u &amp; \text{if } R_u \text{ exists} \\
R_m &amp; \text{if } R_u \text{ does not exist and } R_m \text{ exists} \\
R_{\text{mean}} &amp; \text{if neither } R_u \text{ nor } R_m \text{ exists}
\end{cases}
$$&lt;p&gt;This ensures every song gets a score.&lt;/p&gt;
&lt;h3 id="adjusting-and-normalizing-scores"&gt;Adjusting and Normalizing Scores
&lt;/h3&gt;&lt;p&gt;Once the ratings are gathered, they need to be adjusted for fairness. The system normalizes them and adds some weight to higher-rated songs. The normalization process is:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ensure no score is zero&lt;/strong&gt; by adding a small constant (0.1) to the score:&lt;/p&gt;
&lt;p&gt;$ R' = \max(0.1, R) $&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Amplify high ratings&lt;/strong&gt; by normalizing the scores and then squaring them to give higher-rated songs more weight:&lt;/p&gt;
&lt;p&gt;$ R_{\text{adjusted}} = \left( \frac{R'}{10} \right)^2 $&lt;/p&gt;
&lt;p&gt;This squaring makes songs with higher ratings much more likely to be chosen, while ensuring lower-rated songs still have a small chance.&lt;/p&gt;
&lt;h3 id="skip-and-play-history"&gt;Skip and Play History
&lt;/h3&gt;&lt;p&gt;The next adjustment comes from considering how often a song has been played versus skipped:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Full plays&lt;/strong&gt;: $ P_f $&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Skips&lt;/strong&gt;: $ P_s $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;skip score&lt;/strong&gt; is calculated using the difference between full plays and skips, normalized by an empirically chosen factor of 5.&lt;/p&gt;
&lt;p&gt;$ S_{\text{skip}} = \sigma \left( \frac{5 + P_f - P_s}{5} \right) $&lt;/p&gt;
&lt;p&gt;where σ(x) is the &lt;strong&gt;sigmoid function&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$ \sigma(x) = \frac{1}{1 + e^{-x}} $&lt;/p&gt;
&lt;p&gt;This ensures that the skip score stays in a meaningful range between 0 and 1.&lt;/p&gt;
&lt;h3 id="time-since-last-played"&gt;Time Since Last Played
&lt;/h3&gt;&lt;p&gt;To promote variety, the system boosts the chances for songs that haven’t been played recently. The system calculates a &lt;strong&gt;last played score&lt;/strong&gt; based on how recently the song was played compared to other songs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Index of song&lt;/strong&gt; in the list sorted by last played: $ I_{\text{last}} $&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Total number of songs&lt;/strong&gt;: $ N $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The last played score is calculated as:&lt;/p&gt;
&lt;p&gt;$ S_{\text{time}} = \frac{I_{\text{last}}}{N} $&lt;/p&gt;
&lt;p&gt;Songs that haven’t been played in a long time get a higher score, while recently played songs get a lower score.&lt;/p&gt;
&lt;h3 id="final-score-calculation"&gt;Final Score Calculation
&lt;/h3&gt;&lt;p&gt;Now that we have all the factors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adjusted rating: $ R_{\text{adjusted}} $&lt;/li&gt;
&lt;li&gt;Skip score: $ S_{\text{skip}} $&lt;/li&gt;
&lt;li&gt;Last played score: $ S_{\text{time}} $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final score for each song is a product of these three components:&lt;/p&gt;
&lt;p&gt;$ S_{\text{final}} = R_{\text{adjusted}} \times S_{\text{skip}} \times S_{\text{time}} $&lt;/p&gt;
&lt;p&gt;This final score \( S_{\text{final}} \) represents how likely a song is to be selected.&lt;/p&gt;
&lt;h3 id="calculating-probabilities"&gt;Calculating Probabilities
&lt;/h3&gt;&lt;p&gt;Once the final scores are calculated for all songs, they are converted into probabilities. The probability of a song $ i $ being selected is its score divided by the sum of all scores:&lt;/p&gt;
&lt;p&gt;$ P_i = \frac{S_{\text{final}, i}}{\sum_{j=1}^{N} S_{\text{final}, j}} $&lt;/p&gt;
&lt;p&gt;This ensures that the probabilities sum to 1, and each song’s chance of being selected is proportional to its score.&lt;/p&gt;
&lt;h3 id="random-selection"&gt;Random Selection
&lt;/h3&gt;&lt;p&gt;Finally, the system uses these probabilities to randomly select a song. A song with a higher probability $ P_i $ is more likely to be chosen, but even songs with lower probabilities still have a chance based on their final score.&lt;/p&gt;
&lt;h3 id="example-calculation"&gt;Example Calculation
&lt;/h3&gt;&lt;p&gt;Let’s say we have three songs with the following details:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Song&lt;/th&gt;
&lt;th&gt;User Rating&lt;/th&gt;
&lt;th&gt;Full Plays&lt;/th&gt;
&lt;th&gt;Skips&lt;/th&gt;
&lt;th&gt;Last Played (days ago)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;C&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Adjusted Ratings&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Song A: $ R_{\text{adjusted}} = \left( \frac{8}{10} \right)^2 = 0.64 $&lt;/li&gt;
&lt;li&gt;Song B: $ R_{\text{adjusted}} = \left( \frac{6}{10} \right)^2 = 0.36 $&lt;/li&gt;
&lt;li&gt;Song C uses the mean rating (say 7): $ R_{\text{adjusted}} = \left( \frac{7}{10} \right)^2 = 0.49 $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Skip Scores&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Song A: $ S_{\text{skip}} = \sigma \left( \frac{5 + 10 - 2}{5} \right) = \sigma(2.6) \approx 0.93 $&lt;/li&gt;
&lt;li&gt;Song B: $ S_{\text{skip}} = \sigma \left( \frac{5 + 5 - 3}{5} \right) = \sigma(1.4) \approx 0.80 $&lt;/li&gt;
&lt;li&gt;Song C: $ S_{\text{skip}} = \sigma \left( \frac{5 + 2 - 5}{5} \right) = \sigma(0.4) \approx 0.60 $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Last Played Scores&lt;/strong&gt; (let’s assume there are 3 songs total):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Song A (played 7 days ago): $ S_{\text{time}} = \frac{2}{3} \approx 0.67 $&lt;/li&gt;
&lt;li&gt;Song B (played 2 days ago): $ S_{\text{time}} = \frac{1}{3} \approx 0.33 $&lt;/li&gt;
&lt;li&gt;Song C (played 30 days ago): $ S_{\text{time}} = \frac{3}{3} = 1.0 $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Final Scores&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Song A: $ S_{\text{final}} = 0.64 \times 0.93 \times 0.67 \approx 0.40 $&lt;/li&gt;
&lt;li&gt;Song B: $ S_{\text{final}} = 0.36 \times 0.80 \times 0.33 \approx 0.095 $&lt;/li&gt;
&lt;li&gt;Song C: $ S_{\text{final}} = 0.49 \times 0.60 \times 1.0 \approx 0.29 $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Probabilities&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$ P_A = \frac{0.40}{0.40 + 0.095 + 0.29} \approx 0.50 $&lt;br&gt;
$ P_B = \frac{0.095}{0.40 + 0.095 + 0.29} \approx 0.12 $&lt;br&gt;
$ P_C = \frac{0.29}{0.40 + 0.095 + 0.29} \approx 0.36 $&lt;/p&gt;
&lt;p&gt;In this case, Song A has a 50% chance of being selected, Song B has a 12% chance, and Song C has a 36% chance.&lt;/p&gt;
&lt;h2 id="future-plans"&gt;Future plans
&lt;/h2&gt;&lt;p&gt;In future I would also like to add more different options of building a current playlist. One of the ideas is a “chain mode” that takes selected by the user song as a seed and finds the most similar song, based on the embeddings, then finds the most similar to a next song and so on.&lt;/p&gt;
&lt;p&gt;In case there would be a &lt;a class="link" href="https://github.com/openai/CLIP" target="_blank" rel="noopener"
&gt;CLIP&lt;/a&gt;-like model for audio in the future, I would also like to implement prompt-based playlist generation. One could simply describe the mood of the music they would like to listen and the proper playlist be generated on the fly from this description.&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="link" href="https://github.com/volotat/Anagnorisis" target="_blank" rel="noopener"
&gt;https://github.com/volotat/Anagnorisis&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Anagnorisis. Part 1: A Vision for Better Information Management.</title><link>https://volotat.github.io/p/anagnorisis-part-1-a-vision-for-better-information-management/</link><pubDate>Sun, 12 May 2024 00:00:00 +0000</pubDate><guid>https://volotat.github.io/p/anagnorisis-part-1-a-vision-for-better-information-management/</guid><description>&lt;img src="https://volotat.github.io/p/anagnorisis-part-1-a-vision-for-better-information-management/cover.jpg" alt="Featured image of post Anagnorisis. Part 1: A Vision for Better Information Management." /&gt;&lt;h2 id="introduction"&gt;Introduction
&lt;/h2&gt;&lt;p&gt;Many years ago there was a music streaming service called &lt;a class="link" href="https://en.wikipedia.org/wiki/Grooveshark" target="_blank" rel="noopener"
&gt;Grooveshark&lt;/a&gt;. While being one of the first of such services it allowed things that would be unheard of today — free access to a huge music library and an option for any user to upload their own music to the website. This created quite a unique environment where you can easily find even the most obscure music you like to listen to. Furthermore there was a &amp;ldquo;radio mode&amp;rdquo; that automatically selected the music that you would like to listen, basically the same as any other today&amp;rsquo;s streaming service with one little exception — it was actually good. I was able to listen to music for hours and almost always it was the music I like that allowed me to discover many new bands that I like till this day. Unfortunately, on April 30, 2015 the site was shut down as part of a settlement of the copyright infringement lawsuits between the service and Universal Music Group, Sony Music Entertainment, and Warner Music Group.&lt;/p&gt;
&lt;p&gt;Since then I have tried many other alternatives such as spotify, pandora, youtube music and others. None of them ever worked for me as well as Grooveshark. First of all for a long time many of them simply were not working for my country of residence, but even when they start working there was a problem — recommendations worked really badly. Mostly the recommendations were good only for the first few songs, but after that it always started to diverge into &amp;ldquo;popular&amp;rdquo; or rather &amp;ldquo;promoted&amp;rdquo; music that I don&amp;rsquo;t really like. And yes, it did not matter if it was paid or free version. The suspicion started to grow, that many services use music recommendation engines as subtle manipulation to promote some particular artists rather than satisfying my needs as a customer.&lt;/p&gt;
&lt;p&gt;This led me into rethinking the whole idea of recommendation algorithms and how much of it is what we want to see, and how much of it is what the people in control of it want or don&amp;rsquo;t want to show.&lt;/p&gt;
&lt;h2 id="controlling-the-algorithm-not-the-other-wayaround"&gt;Controlling the Algorithm. Not the other way around.
&lt;/h2&gt;&lt;p&gt;There are several different ways of how we can sort information. Some popular social media, such as &lt;a class="link" href="https://www.reddit.com/" target="_blank" rel="noopener"
&gt;Reddit&lt;/a&gt; heavily rely on direct human feedback for it. While this approach is clearly working it also requires a lot of moderation efforts. And because of its nature, posts on reddit get more attention when they satisfy the interest of the whole community rather than a particular person.&lt;/p&gt;
&lt;p&gt;Another widespread approach that recommendation systems use is &lt;a class="link" href="https://en.wikipedia.org/wiki/Collaborative_filtering" target="_blank" rel="noopener"
&gt;Collaborative filtering&lt;/a&gt;. It uses personal feedback from some users to predict preferences of other users. Websites such as Spotify or Youtube in its core heavily rely on this approach or at least relied in the past. The main drawback of this approach that you do need a lot of users and their personal data to make it work and even then there are no guarantee that it will always work for all users as they might have some set of interests that were not seen previously.&lt;/p&gt;
&lt;p&gt;Finally with the rise of Machine Learning a new approach has risen — &lt;a class="link" href="https://arxiv.org/pdf/2310.18608.pdf" target="_blank" rel="noopener"
&gt;Embedding-based recommender systems&lt;/a&gt;. While this is an umbrella term that implies many different techniques, the general idea behind it is to use embeddings generated by some ML model directly from the data to then predict how well this data is suited for a particular user. This is the only approach that could work without a huge user base, using only a single user feedback. As such, it opens a new exciting possibility — a recommendation system that works completely locally on users&amp;rsquo; personal devices. If implemented as an open source project such a recommendation system would be completely open to the user and controlled only by the user.&lt;/p&gt;
&lt;h2 id="the-nextfrontier"&gt;The Next Frontier
&lt;/h2&gt;&lt;p&gt;Let&amp;rsquo;s spend a minute trying to imagine how a &amp;ldquo;perfect&amp;rdquo; recommendation system might work. For a brief moment imagine that our system is totally agnostic to what kind of data it can process. No matter if it is music, videos, news or anything else. To extract embeddings of the data we will use some ML model that could take any data as an input and produce a meaningful embedding of it.&lt;/p&gt;
&lt;p&gt;Here is some basic principle that I would like this system to satisfy:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First of all, it should give valuable recommendations. Obviously. It should allow to sort and filter the vast amount of information.&lt;/li&gt;
&lt;li&gt;All the data about the user and about the user&amp;rsquo;s preferences should stay on their own personal devices and should be accessible only by the users.&lt;/li&gt;
&lt;li&gt;It should learn from the user&amp;rsquo;s feedback and change its recommendations alongside changing interests of the user.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can use our ML model to generate embedding from the big pile of data, and present some of this data to the user in the form of UI interface. Like a music player, for example. Then we collect user feedback about the data and train another ML model that takes embeddings as inputs and predicts some score representing how relevant this data is for the user. We then can use this new model to suggest better recommendations, collect more data and repeat this process again and again, providing a more and more satisfying experience. And all of those steps might be performed locally, just as we wished earlier.&lt;/p&gt;
&lt;p&gt;Here is the little diagram showing how the dataflow in such system may look like:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/anagnorisis-part-1-a-vision-for-better-information-management/Anagnorisis_graph.png"
width="3646"
height="2394"
srcset="https://volotat.github.io/p/anagnorisis-part-1-a-vision-for-better-information-management/Anagnorisis_graph_hu_ba1a9f042994e1b0.png 480w, https://volotat.github.io/p/anagnorisis-part-1-a-vision-for-better-information-management/Anagnorisis_graph_hu_c6e29faaaf258bb3.png 1024w"
loading="lazy"
alt="A rough representation on how such recommendation system might work"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="365px"
&gt;&lt;/p&gt;
&lt;p&gt;I particularly like the option of using &lt;a class="link" href="https://en.wikipedia.org/wiki/Peer-to-peer" target="_blank" rel="noopener"
&gt;p2p Network&lt;/a&gt; as a main datasource of such system. In this case we can completely move away from centralized services that control the flow of information and give users a choice to choose what kind of data they need, without any need for content moderation or sharing their personal data. To speed up calculations and not spend time and resources downloading unnecessary data, the data embeddings might be precalculated on the data provider side. So we can check if the data is valuable for us at first and only then accessing it.&lt;/p&gt;
&lt;p&gt;To give a rating for a particular embedding we can train a model from scratch that takes embeddings as an input and predicts a value that estimates a score that a user would give to the data themselves. Right now Anagnorisis trains a separate evaluation network for each type of data, but in the future I would like to explore a more general approach, for example a multimodal transformer that could take text and embeddings as its input and produce interest scores as an output. While it would be computationally much slower than using some specialized model, it opens up new amazing possibilities that will be discussed later in this series.&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="link" href="https://github.com/volotat/Anagnorisis" target="_blank" rel="noopener"
&gt;https://github.com/volotat/Anagnorisis&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Anagnorisis</title><link>https://volotat.github.io/projects/2023-10-08-anagnorisis/</link><pubDate>Sun, 08 Oct 2023 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2023-10-08-anagnorisis/</guid><description>&lt;img src="https://volotat.github.io/projects/2023-10-08-anagnorisis/anagnorisis-screenshot%20from%202026-01-02.png" alt="Featured image of post Anagnorisis" /&gt;&lt;p&gt;Completely local data-management platform with built in trainable recommendation engine.&lt;/p&gt;
&lt;p&gt;The core idea is to create a self-hosted local-first media platform where you can rate your data, and the system trains a personal model to understand your preferences. This model then sorts your data based on your predicted interest, creating a personalized filter for any type of media you might have — images, music, videos, articles, and more.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Local First:&lt;/strong&gt; All data always stays on your device only. All models are trained and inferenced locally.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AI Powered:&lt;/strong&gt; Uses advanced embeddings (CLAP, SigLIP, Jina) to understand, search and filter your content and estimate preferences.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FullStack:&lt;/strong&gt; Built with Flask, Bulma, Transformers, and PyTorch. Uses simple Docker setup for easy deployment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open Source:&lt;/strong&gt; AGPL-3.0 license. Contributions, feedback and support are always welcome!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Active Development.&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/Anagnorisis" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>SD-CN-Animation</title><link>https://volotat.github.io/projects/2023-03-17-sd-cn-animation/</link><pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2023-03-17-sd-cn-animation/</guid><description>&lt;img src="https://volotat.github.io/projects/2023-03-17-sd-cn-animation/ui_preview.png" alt="Featured image of post SD-CN-Animation" /&gt;&lt;p&gt;This project was developed as an extension for the &lt;a class="link" href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" target="_blank" rel="noopener"
&gt;Automatic1111 web UI&lt;/a&gt; to automate video stylization and enable text-to-video generation using &lt;a class="link" href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5" target="_blank" rel="noopener"
&gt;Stable Diffusion 1.5&lt;/a&gt; backbones. At the time of development, generated videos often suffered from severe flickering and temporal inconsistency. This framework addressed those issues by integrating the &lt;a class="link" href="https://github.com/princeton-vl/RAFT" target="_blank" rel="noopener"
&gt;RAFT&lt;/a&gt; optical flow estimation algorithm. By calculating the motion flow between frames, the system could warp the previously generated frame to match the motion of the next one, creating a stable base for the diffusion model. This process, combined with occlusion masks, ensured that only new parts of the scene were generated while maintaining the consistency of existing objects.&lt;/p&gt;
&lt;p&gt;The tool supported both video-to-video stylization and experimental text-to-video generation. In video-to-video mode, users could apply &lt;a class="link" href="https://huggingface.co/lllyasviel/ControlNet" target="_blank" rel="noopener"
&gt;ControlNet&lt;/a&gt; to guide the structure of the output, allowing for stable transformations like turning a real-life video into a watercolor painting or digital art while preserving the original motion. The text-to-video mode employed a custom &amp;ldquo;FloweR&amp;rdquo; method to hallucinate optical flow from static noise, attempting to generate continuous motion from text prompts alone.&lt;/p&gt;
&lt;p&gt;Development on this project was eventually discontinued as the field rapidly advanced. The emergence of modern, end-to-end text-to-video models provided much more coherent and faithful results than could be achieved by hacking image-based diffusion models, rendering this approach largely obsolete for general use cases.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Not Maintained&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/SD-CN-Animation" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Can Humans Speak the Language of the Machines?</title><link>https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/</link><pubDate>Tue, 06 Sep 2022 00:00:00 +0000</pubDate><guid>https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/</guid><description>&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_Tpm0MlWfLdNMGQ9b.png" alt="Featured image of post Can Humans Speak the Language of the Machines?" /&gt;&lt;h2 id="introduction"&gt;Introduction
&lt;/h2&gt;&lt;p&gt;In the movie Arrival (2016) a group of scientists tries to learn the language of extraterrestrial aliens that arrived on Earth and by doing so they change the way they think which allows the main character to obtain some extraordinary power. While such power is in the domain of fiction, the theory that our language dictates the way we think &lt;a class="link" href="https://en.wikipedia.org/wiki/Linguistic_relativity" target="_blank" rel="noopener"
&gt;has been widely explored in the literature&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There is a main crucial distinction in how people and current AI systems process information. All major human languages are based on discrete bags of words, while ML models almost always obtain and output information in terms of lists of real values. Because of this, all NLP models have a process of “vectorization” when a whole word, part of it, or single characters obtain their own vector representation that conveys their internal meaning accessible for further processing within the models. The main benefit of vectorization is that these vectors are continuous, therefore, could be adjusted (trained) through backpropagation. In a world of NLP models, the words “planet” and “moon” have some distance between each of them and could be gradually transformed from one to another.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_OiPWVzJqYD5qTEf7.png"
width="1400"
height="538"
srcset="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_OiPWVzJqYD5qTEf7_hu_3158852279ffec97.png 480w, https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_OiPWVzJqYD5qTEf7_hu_84228caddcd61232.png 1024w"
loading="lazy"
alt="Vector representations of words in latent space of NLP models"
class="gallery-image"
data-flex-grow="260"
data-flex-basis="624px"
&gt;&lt;/p&gt;
&lt;p&gt;Here is where the first glimpse of a new superpower will show up. Because of the discrete nature of our language people tend to think in terms of types, even when these types cannot be properly defined. The most obvious examples of it are in cosmology. There are some types of objects that at first glance fit very well into our discrete view of the world: planets, moons, stars, comets, asteroids, black holes and so on. But there are also some in-betweeny objects such as black dwarfs that are neither a planet nor a star but have properties of both depending from which angle you look at it. And there is Pluto… that was a planet, but lost its status when the definition of a planet was changed. The thing is, there is an ongoing debate that this new definition is not purely scientific and based on a recent observation of Pluto’s geological activity, &lt;a class="link" href="https://www.sciencedirect.com/science/article/pii/S0019103521004206" target="_blank" rel="noopener"
&gt;it should be a planet after all&lt;/a&gt;. The type system breakes when we find something that lies on the boundary between the types. And most of the time, a new type is created alongside a bunch of new boundaries that wait to be filled with new exceptions. It seems that it would be beneficial to have a language that does not have these boundaries in its nature and works more closely with how ML models work. So, here comes the question: &lt;strong&gt;could we create a language that is understandable by a human but works in a continuous fashion?&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="vector-based-language"&gt;Vector Based Language
&lt;/h2&gt;&lt;p&gt;In some sense, we already have a continuous language in the form of images. If we ask one human to draw a house and ask another what this image means, we have a high chance that the guess would be right. And a list of images might tell stories just as a sentence does. But there is a problem, for any word in a language there are infinite possibilities of depicting it in the form of an image, and then there are even more possibilities of interpreting the image back to words. So the language made of regular images would not be very reliable in conveying an exact meaning that was originally intended. To overcome this problem, we need a system that could generate some sort of an image for every word given and could guarantee that this image might be converted back to the original word exactly.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_qJX7THDcOFH_XBpi.png"
width="625"
height="790"
srcset="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_qJX7THDcOFH_XBpi_hu_f266c63775a7004b.png 480w, https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_qJX7THDcOFH_XBpi_hu_8301af913c7f316f.png 1024w"
loading="lazy"
alt="An example of a story told entirely with images"
class="gallery-image"
data-flex-grow="79"
data-flex-basis="189px"
&gt;&lt;/p&gt;
&lt;p&gt;The main tool we are going to use in the pursuit of developing such language is, once again, machine learning. The setup will look like this: there is a pre-trained language model that takes a word or a sentence as an input and produces some vector representation of it, there is also a second model that transforms this vector into some human-readable format, and lastly, there is a human who has to guess the original word that has been given. The human is allowed to take any time necessary to learn how the system works.&lt;/p&gt;
&lt;h2 id="architecture"&gt;Architecture
&lt;/h2&gt;&lt;p&gt;The general thing we want to achieve is to have a model that generates some embedding of the given text and then produces a visualization of these embeddings. To obtain embeddings of text, we will use &lt;a class="link" href="https://arxiv.org/abs/1803.11175" target="_blank" rel="noopener"
&gt;universal sentence encoder&lt;/a&gt; from Google, and to generate the visualization we will train generator network G. To do so, we need to have some approximation of the human visual system to make resulting images human-readable. In our case, this approximation would be a network V that we will initialize with mobile-net-v2 weights and allow these weights to change at the training stage.&lt;/p&gt;
&lt;p&gt;We are leaving the vision model trainable as it would bring another layer of regularization to the system. If we don’t do that and freeze the model, the decoding network tends to exploit inaccuracies in the visual model and settles on the solution that has a good reconstruction rate, yet all images are still pretty much the same, and only subtle differences are present that are very hard for a human to notice.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_ZY2XQdsCwSDpn3-aXAzv9w.png"
width="798"
height="904"
srcset="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_ZY2XQdsCwSDpn3-aXAzv9w_hu_cacc4d1e0d1d4571.png 480w, https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_ZY2XQdsCwSDpn3-aXAzv9w_hu_77ff654ceee10bbb.png 1024w"
loading="lazy"
alt="General representation of the system’s architecture"
class="gallery-image"
data-flex-grow="88"
data-flex-basis="211px"
&gt;&lt;/p&gt;
&lt;p&gt;We train this architecture with the following loss function:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_Rz4gSkKEn2uc4SFVKCEGSg.png"
width="1295"
height="619"
srcset="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_Rz4gSkKEn2uc4SFVKCEGSg_hu_9fef1db847a0b0ff.png 480w, https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_Rz4gSkKEn2uc4SFVKCEGSg_hu_c3e748cf832aff92.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="502px"
&gt;&lt;/p&gt;
&lt;p&gt;Where G — generator network, V — vision network, D — decoder network. I_1 and I_2 are embeddings of two randomly generated sentences.&lt;/p&gt;
&lt;p&gt;Here, the purpose of L_1 term is to make sure that distances in visual latent space are roughly the same as distances in word embeddings space. L_2 keeps images generated by G as far apart as possible in RBG space. L_3 keeps images generated by G as far apart as possible in visual space while keeping its mean at 0. L_4, the most important term, makes sure that reconstructed word embeddings are the same as initial ones. Values for α, β, γ are found empirically.&lt;/p&gt;
&lt;p&gt;To make the generator to be able to produce a meaningful image from any point of embedding space of the universal sentence encoder and prevent overfitting, we have to probe the space as densely as possible at the training stage. To do so, we will generate random sentences with the help of &lt;a class="link" href="https://www.nltk.org/" target="_blank" rel="noopener"
&gt;nltk&lt;/a&gt; library. Here are the examples of such generated sentences: ‘ideophone beanfield tritonymphal fatuism preambulate nonostentation overstrictly pachyhaemous’, ‘hyperapophysial’, ‘southern’, ‘episynaloephe subgenerically gleaning reformeress’, ‘trigonelline’, ‘commorant saltspoon’, ‘nonpopularity mammaliferous isobathythermal phenylglyoxylic insulate aortomalaxis desacralize spooky’, ‘speed garn nunciatory neologism’, ‘podobranchial fencible’, ‘epeirid gibaro’, ‘sleeved’, ‘demonographer probetting subduingly’, ‘velociously calpacked invaccinate acushla amixia unicolor’ and so on.&lt;/p&gt;
&lt;h2 id="testing"&gt;Testing
&lt;/h2&gt;&lt;p&gt;To test if our methods are doing well we are going to use the reconstruction rate metric. To calculate the metric, we need to collect &lt;a class="link" href="https://www.talkenglish.com/vocabulary/top-2000-vocabulary.aspx" target="_blank" rel="noopener"
&gt;2265 most common English words&lt;/a&gt; and expand this list with 10 digits. A random word gets converted to its embedding with the &lt;a class="link" href="https://arxiv.org/abs/1803.11175" target="_blank" rel="noopener"
&gt;universal sentence encoder&lt;/a&gt; and passed to the generator that generates the image representing the word. Then, depending on who is the test subject (human or machine) we use one of the following:&lt;/p&gt;
&lt;p&gt;For a machine, we pass generated images to the pre-trained visual network and then decode it with decoder network D to get back the word embedding. We compare this new embedding with all embeddings in the dictionary, and if the euclidian distance from the new embedding to the original embedding is the smallest, we give the point to the system. After checking all the words in such a way, we obtain:&lt;/p&gt;
&lt;p&gt;reconstruction rate = correct answers / number of words * 100%.&lt;/p&gt;
&lt;figure&gt;
&lt;div style="display: flex; justify-content: center; gap: 10px;"&gt;
&lt;img src="1_R_GQTs5EsJCZTU5SZP3HQg.png" style="width: 48%;"&gt;
&lt;img src="1_x59gDtlLr5RG36bl_HlpJw.png" style="width: 48%;"&gt;
&lt;/div&gt;
&lt;figcaption&gt;Presentation of Human Testing Interface&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;For human testing, we give a generated image to the test subject with a visual interface that presents the image with four possible answers. Human should learn through trial and error what word each image represents and report their best score. To make this task more convenient, at the beginning of the session, the testing interface asks what size of the dictionary the participant wants to use. The dictionary itself consists in a way that it’s growing from the most to less common English words. Volume of the dictionary is separated by the following levels: 10, 20, 40, 100, 200, 400, 1000, and 2275 (full dictionary). Here is a sample of the first 30 words presented in the dictionary:&lt;/p&gt;
&lt;p&gt;0, 1, 2, 3, 4, 5, 6, 7, 8, 9, the, of, and, to, a, in, is, you, are, for, that, or, it, as, be, on, your, with, can, have… and so on.&lt;/p&gt;
&lt;p&gt;In the human case, the reconstruction rate is calculated by the exact same formula. To check the accuracy, the test subject is supposed to answer 100 questions with full dictionary size. Although it is worth noting that the random baseline for humans is 25% as it is always possible to simply guess the right answer from the 4 options presented.&lt;/p&gt;
&lt;h2 id="results"&gt;Results
&lt;/h2&gt;&lt;p&gt;After training the system, it was producing the following images for respective input sentences:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_lgXj1-BGYkDVqLytrz9D9A.png"
width="1340"
height="1274"
srcset="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_lgXj1-BGYkDVqLytrz9D9A_hu_e6c6f08ff1eb2e9a.png 480w, https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_lgXj1-BGYkDVqLytrz9D9A_hu_d7b727979f31604d.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="105"
data-flex-basis="252px"
&gt;&lt;/p&gt;
&lt;p&gt;The reconstruction rate for the decoder network at the end of the training was 100%, and the loss value was 0.02.&lt;/p&gt;
&lt;p&gt;I did the experiment of learning this language on myself and was able to achieve a 43% reconstruction rate after two weeks of training. It was beneficial for me to start from a small number of words and gradually increase this number with less and less common words. The promise was, that at some point you do not really need to remember each image, the meaning of the word should be deducible from the geometry presented in the image. So learning such a language should be much easier than learning a typical natural language. While learning I used the 85% threshold to make sure that I’m ready to increase the volume of the dictionary.&lt;/p&gt;
&lt;p&gt;Here is how a two-week learning progress looked like for different dictionary sizes:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_aN_F1IG-q6nR7ECNjJRoyQ.png"
width="1059"
height="697"
srcset="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_aN_F1IG-q6nR7ECNjJRoyQ_hu_8c9399ef05ae7684.png 480w, https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_aN_F1IG-q6nR7ECNjJRoyQ_hu_204ad6b8ef0ccee0.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="151"
data-flex-basis="364px"
&gt;&lt;/p&gt;
&lt;p&gt;While bigger dictionaries became progressively harder to learn, the upward trend is clearly visible in all cases. This proves the most important point that answers the question that was set at the beginning. &lt;strong&gt;We can create a language that works in a continuous fashion and such language is learnable by a human.&lt;/strong&gt; While implications of this are yet to be known, in the next section, I will speculate on some that might be interesting to investigate further.&lt;/p&gt;
&lt;p&gt;The source code and pre-trained models are available on the project’s GitHub page: &lt;a class="link" href="https://github.com/volotat/Vector-Based-Language" target="_blank" rel="noopener"
&gt;https://github.com/volotat/Vector-Based-Language&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Full training history is available here: &lt;a class="link" href="https://github.com/volotat/Vector-Based-Language/blob/main/article/history.csv" target="_blank" rel="noopener"
&gt;https://github.com/volotat/Vector-Based-Language/blob/main/article/history.csv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="thoughts"&gt;Thoughts
&lt;/h2&gt;&lt;p&gt;To my knowledge, that is the only example of a synthetic language that possesses a notion of continuity where words and even sentences could blend into each other. I suggest that such language should be interesting even as an example that such a thing is possible in principle. I hope this may make someone fascinated by the idea to explore it further in their own right.&lt;/p&gt;
&lt;p&gt;I think that the most promising aspect of such language is the combination of it with some sort of brain-computer interface. The obvious downside of the language at the current stage is that there is no clear way to produce the image directly from the intent rather than words. With good enough BCI, this problem could be solved. Therefore such language might be the first way to store thoughts in human-readable format.&lt;/p&gt;
&lt;p&gt;With the rise of foundation models, and especially text2image generators, such as &lt;a class="link" href="https://github.com/CompVis/stable-diffusion" target="_blank" rel="noopener"
&gt;Stable Diffusion&lt;/a&gt;, there is a need to produce and store concepts that could not be easily conveyed with words but still could be represented as embeddings. There is recent work called &lt;a class="link" href="https://github.com/rinongal/textual_inversion" target="_blank" rel="noopener"
&gt;Textual Inversion&lt;/a&gt; that tries to solve this exact problem. With language like this, we could visualize these embeddings in a meaningful way and even use it as a common concept space for many models in the future.&lt;/p&gt;
&lt;p&gt;The language might be significantly improved to be more human-friendly. For example, we can use more advanced models such as CLIP (to be more specific, an image encoder part of it) as a visual model at the training stage to produce more readable images. Or we could use a set of predefined text-image pairs as anchors for more controllable outputs.&lt;/p&gt;
&lt;p&gt;And lastly, to really show that generated images represent the meaning of the words rather than the words themselves…&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_0yCretXbrfvCMniTT8T8LA.png"
width="1400"
height="284"
srcset="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_0yCretXbrfvCMniTT8T8LA_hu_61fbd085c439aa35.png 480w, https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_0yCretXbrfvCMniTT8T8LA_hu_1af17d4d56362676.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="492"
data-flex-basis="1183px"
&gt;&lt;/p&gt;</description></item><item><title>Vector Based Language</title><link>https://volotat.github.io/projects/2022-08-27-vector-based-language/</link><pubDate>Sat, 27 Aug 2022 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2022-08-27-vector-based-language/</guid><description>&lt;img src="https://volotat.github.io/projects/2022-08-27-vector-based-language/1_lgXj1-BGYkDVqLytrz9D9A.png" alt="Featured image of post Vector Based Language" /&gt;&lt;p&gt;Research project exploring the possibility of creating a continuous, visual language that hint at the possibility of direct understanding of the embedding spaces produced by ML models.&lt;/p&gt;
&lt;p&gt;Unlike traditional discrete languages, this system uses machine learning to generate unique visual representations (images) for any given text embedding. The goal is to create a language where concepts can blend into each other continuously, mirroring how neural networks process information.&lt;/p&gt;
&lt;p&gt;Words and sentences are represented as points in a continuous vector space, allowing for smooth transitions between concepts. Experiments show that humans can learn to interpret these generated visual embeddings with increasing accuracy over time. The visual language is designed to be perfectly reconstructible back into the original text embeddings by a decoder network crating a bridge between human cognition and the &amp;ldquo;black box&amp;rdquo; mechanized interpretation of the data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/" &gt;Read the Article&lt;/a&gt; | &lt;a class="link" href="https://github.com/volotat/Vector-Based-Language" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Generative Art Synthesizer</title><link>https://volotat.github.io/projects/2021-12-04-generative-art-synthesizer/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2021-12-04-generative-art-synthesizer/</guid><description>&lt;img src="https://volotat.github.io/projects/2021-12-04-generative-art-synthesizer/gas_preview.png" alt="Featured image of post Generative Art Synthesizer" /&gt;&lt;p&gt;A Python program that generates Python programs that generate generative art.&lt;/p&gt;
&lt;p&gt;Most generative art relies on stochastic processes where the initial seed and specific parameters are often lost, making exact reproduction difficult. GAS takes a different approach: instead of storing just the output or the parameters, it generates a fully deterministic, standalone Python script for each artwork. This ensures complete reproducibility—if you have the script, you have the art.&lt;/p&gt;
&lt;p&gt;The core mechanism involves initializing a tensor with coordinate data and then applying a random sequence of mathematical transformations (like &lt;code&gt;transit&lt;/code&gt;, &lt;code&gt;sin&lt;/code&gt;, &lt;code&gt;magnitude&lt;/code&gt;, &lt;code&gt;shift&lt;/code&gt;) to its channels. These operations are restricted to the [-1, 1] range to ensure stability. The final result is a composition of these channels converted into color space.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Self-Contained Art:&lt;/strong&gt; Each generated piece is a runnable Python script with zero external dependencies beyond standard scientific libraries (numpy, PIL).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deterministic:&lt;/strong&gt; The generated scripts contain no random elements; running the same script always produces the exact same image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Method-Based generation:&lt;/strong&gt; Uses a palette of composable mathematical functions (&lt;code&gt;sin&lt;/code&gt;, &lt;code&gt;prod&lt;/code&gt;, &lt;code&gt;soft_min&lt;/code&gt;, etc.) to &amp;ldquo;sculpt&amp;rdquo; the image in a high-dimensional channel space.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Aesthetic Scoring:&lt;/strong&gt; Includes a simple scoring model to estimate the visual quality of generated outputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/GAS" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>The ARC Game</title><link>https://volotat.github.io/projects/2021-07-26-the-arc-game/</link><pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2021-07-26-the-arc-game/</guid><description>&lt;img src="https://volotat.github.io/projects/2021-07-26-the-arc-game/level_example.jpg" alt="Featured image of post The ARC Game" /&gt;&lt;p&gt;The Abstraction and Reasoning Corpus made into a web game.&lt;/p&gt;
&lt;p&gt;The aim of this project is to create an easy-to-use interface for François Chollet&amp;rsquo;s &lt;a class="link" href="https://github.com/fchollet/ARC-AGI" target="_blank" rel="noopener"
&gt;Abstraction and Reasoning Corpus (ARC-AGI)&lt;/a&gt;, designed so that children as young as three years old can play with it. This tool explores the potential of using ARC as educational material for developing abstraction and reasoning skills in young kids, challenging cognitive abilities such as pattern recognition, logical reasoning, and problem-solving.&lt;/p&gt;
&lt;p&gt;The game presents visual tasks consisting of grid pairs that represent a transformation (input → output). The player must deduce the transformation rule from the examples and apply it to a test grid.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Child-Friendly Interface:&lt;/strong&gt; Simplified controls allow for painting, dragging to fill, and copying grids, making it accessible for early childhood development.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fixed Grid Sizes:&lt;/strong&gt; Unlike the original ARC where output size is part of the solution, here the output grid size is pre-set to reduce complexity and focus on the transformation logic.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Printable Version:&lt;/strong&gt; Tasks can be automatically formatted and printed on paper with adjusted colors (e.g., swapping black for white) for offline solving with markers or pencils.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Not Maintained.&lt;/p&gt;
&lt;h3 id="cited-in-scientific-literature"&gt;Cited in Scientific Literature
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="link" href="https://pub.tik.ee.ethz.ch/students/2021-HS/BA-2021-38.pdf" target="_blank" rel="noopener"
&gt;Abstraction and Reasoning Challenge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://arxiv.org/abs/2403.11793" target="_blank" rel="noopener"
&gt;Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://arxiv.org/pdf/2410.07866" target="_blank" rel="noopener"
&gt;System 2 Reasoning for Human-AI Alignment: Generality and Adaptivity via ARC-AG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://dl.acm.org/doi/abs/10.1145/3711896.3736831" target="_blank" rel="noopener"
&gt;Addressing and Visualizing Misalignments in Human Task-Solving Trajectories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class="link" href="https://volotat.github.io/ARC-Game/" target="_blank" rel="noopener"
&gt;Play the Game&lt;/a&gt; | &lt;a class="link" href="https://github.com/volotat/ARC-Game" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Differentiable Morphing</title><link>https://volotat.github.io/projects/2021-01-02-differentiable-morphing/</link><pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2021-01-02-differentiable-morphing/</guid><description>&lt;img src="https://volotat.github.io/projects/2021-01-02-differentiable-morphing/formula.jpg" alt="Featured image of post Differentiable Morphing" /&gt;&lt;p&gt;Image morphing without reference points by optimizing warp maps via gradient descent.&lt;/p&gt;
&lt;p&gt;This project introduces a &amp;ldquo;differentiable morphing&amp;rdquo; algorithm that can smoothly transition between any two images without requiring manual reference points or landmarks. Unlike traditional generative models that learn a distribution from a dataset, this approach uses a neural network as a temporary functional mapping to solve a specific optimization problem for a single pair of images.&lt;/p&gt;
&lt;p&gt;The algorithm finds a set of maps that transform the source image into the target image.&lt;/p&gt;
&lt;!--The algorithm finds a set of maps that transform the source image ($A$) into the target image ($B$) using the following logic:
$$ B = (A \times \text{mult\_map} + \text{add\_map}) \circ \text{warp\_map} $$
* **Mult Map:** Removes unnecessary parts and adjusts localized color balance.
* **Add Map:** Introduces new colors not present in the original image.
* **Warp Map:** Distorts the image geometry to handle rotation, scaling, and movement of objects.--&gt;
&lt;p&gt;By interpolating the strength of these maps, the system produces a smooth, seamless animation where features transform fluidly from one state to another.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;h3 id="cited-in-scientific-literature"&gt;Cited in Scientific Literature
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="link" href="https://www.sciencedirect.com/science/article/pii/S0167739X22000838" target="_blank" rel="noopener"
&gt;The explainability paradox: Challenges for xAI in digital pathology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://arxiv.org/abs/2311.06792" target="_blank" rel="noopener"
&gt;IMPUS: Image Morphing with Perceptually-Uniform Sampling Using Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://arxiv.org/abs/2507.01953" target="_blank" rel="noopener"
&gt;FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/DiffMorph" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>ConGAN</title><link>https://volotat.github.io/projects/2018-01-29-congan/</link><pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2018-01-29-congan/</guid><description>&lt;img src="https://volotat.github.io/projects/2018-01-29-congan/faces_table.png" alt="Featured image of post ConGAN" /&gt;&lt;p&gt;Continuous Adversarial Image Generator that can produce high-quality images with relatively few examples and resolution independence.&lt;/p&gt;
&lt;p&gt;This project implements a Generative Adversarial Network (GAN) that learns a continuous function mapping coordinates (x, y) and a latent vector z to an RGB color, rather than generating a fixed grid of pixels. By treating images as continuous functions, the model creates a &amp;ldquo;vector-like&amp;rdquo; representation that can be sampled at any resolution.&lt;/p&gt;
&lt;p&gt;The architecture differs from traditional GANs by using a coordinate-based generator. The system includes an &amp;ldquo;Ident&amp;rdquo; network to map images to a latent space, a Generator that predicts colors based on position and latent codes, and a Discriminator that validates pixel data in context.&lt;/p&gt;
&lt;p&gt;Results demonstrated that the model could generalize well from small datasets (10-20 images is enough for reasonable results, while training the network completely from the scratch) and produce smooth interpolations in the latent space, effectively hallucinating plausible variations between training examples.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/ConGAN" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Low Dimensional Autoencoder</title><link>https://volotat.github.io/projects/2017-11-18-low-dimensional-autoencoder/</link><pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2017-11-18-low-dimensional-autoencoder/</guid><description>&lt;img src="https://volotat.github.io/projects/2017-11-18-low-dimensional-autoencoder/latent_space.png" alt="Featured image of post Low Dimensional Autoencoder" /&gt;&lt;p&gt;An experimental approach to training autoencoders for meaningful low-dimensional data representation.&lt;/p&gt;
&lt;p&gt;This project was one of my first experiments in machine learning, serving as a deep dive into how autoencoders process information. The goal was to address the difficulty of encoding high-dimensional data (like images) into very low-dimensional latent spaces (like 2D) without the model getting stuck in local minima. Usually it takes at least 16 or more dimensions for bottleneck embeddings to get decent results, but I wanted to see if it was possible to achieve this in just 2 dimensions.&lt;/p&gt;
&lt;p&gt;The method involves training the autoencoder in a typical fashion, alongside with the encoder that trains on a dynamic set of target representations. It works by initially setting random points for each datapoint and then, at each step, stretching the latent space to touch the predefined space boundaries while applying a repulsion force to keep datapoints distinct. This prevents points from clustering too densely and encourages a more uniform distribution.&lt;/p&gt;
&lt;p&gt;The result, as seen in the project image, is a continuous 2D embedding space that successfully compacts the entire MNIST dataset, allowing for smooth transitions between different digits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/Low-Dimensional-Autoencoder" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item></channel></rss>