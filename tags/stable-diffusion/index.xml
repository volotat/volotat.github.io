<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Stable Diffusion on Alexey Borsky</title><link>https://volotat.github.io/tags/stable-diffusion/</link><description>Recent content in Stable Diffusion on Alexey Borsky</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 17 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://volotat.github.io/tags/stable-diffusion/index.xml" rel="self" type="application/rss+xml"/><item><title>SD-CN-Animation</title><link>https://volotat.github.io/projects/2023-03-17-sd-cn-animation/</link><pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2023-03-17-sd-cn-animation/</guid><description>&lt;img src="https://volotat.github.io/projects/2023-03-17-sd-cn-animation/ui_preview.png" alt="Featured image of post SD-CN-Animation" /&gt;&lt;p&gt;This project was developed as an extension for the &lt;a class="link" href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" target="_blank" rel="noopener"
&gt;Automatic1111 web UI&lt;/a&gt; to automate video stylization and enable text-to-video generation using &lt;a class="link" href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5" target="_blank" rel="noopener"
&gt;Stable Diffusion 1.5&lt;/a&gt; backbones. At the time of development, generated videos often suffered from severe flickering and temporal inconsistency. This framework addressed those issues by integrating the &lt;a class="link" href="https://github.com/princeton-vl/RAFT" target="_blank" rel="noopener"
&gt;RAFT&lt;/a&gt; optical flow estimation algorithm. By calculating the motion flow between frames, the system could warp the previously generated frame to match the motion of the next one, creating a stable base for the diffusion model. This process, combined with occlusion masks, ensured that only new parts of the scene were generated while maintaining the consistency of existing objects.&lt;/p&gt;
&lt;p&gt;The tool supported both video-to-video stylization and experimental text-to-video generation. In video-to-video mode, users could apply &lt;a class="link" href="https://huggingface.co/lllyasviel/ControlNet" target="_blank" rel="noopener"
&gt;ControlNet&lt;/a&gt; to guide the structure of the output, allowing for stable transformations like turning a real-life video into a watercolor painting or digital art while preserving the original motion. The text-to-video mode employed a custom &amp;ldquo;FloweR&amp;rdquo; method to hallucinate optical flow from static noise, attempting to generate continuous motion from text prompts alone.&lt;/p&gt;
&lt;p&gt;Development on this project was eventually discontinued as the field rapidly advanced. The emergence of modern, end-to-end text-to-video models provided much more coherent and faithful results than could be achieved by hacking image-based diffusion models, rendering this approach largely obsolete for general use cases.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Not Maintained&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/SD-CN-Animation" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Combining GPT-3 and Stable Diffusion to imagine a next level game engine.</title><link>https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/</link><pubDate>Fri, 16 Dec 2022 00:00:00 +0000</pubDate><guid>https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/</guid><description>&lt;p&gt;The Holy Grail of all RPG games always was an AI that could reliably reply to what the player says and react to his actions. This was openly pronounced right from the beginning of the genre when text-based games started being developed. However, despite decades of research, there still has not been a perfected AI for RPGs that can truly understand and react to natural language. With the appearance of GPT-3 and other large language models a glimmer of hope appeared. Yet as the world of RPGs is so large and open-ended, it requires an AI with a vast amount of understanding and context about the environment and the player’s actions in order to be able to produce realistic responses. This means that creating an AI which can reliably interact with players is an extremely difficult task — one which has yet to be achieved. Although &lt;a class="link" href="https://aidungeon.io/" target="_blank" rel="noopener"
&gt;AI Dungeon&lt;/a&gt; seems to be a good step forward.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/0_Tu0Tpn1RQg20HLYq.png"
width="640"
height="480"
srcset="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/0_Tu0Tpn1RQg20HLYq_hu_de769f9e24fd305a.png 480w, https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/0_Tu0Tpn1RQg20HLYq_hu_35f571e181ac915.png 1024w"
loading="lazy"
alt="In Fallout 1 players could try to ask characters about anything using the “Tell me about” feature. It didn’t work well."
class="gallery-image"
data-flex-grow="133"
data-flex-basis="320px"
&gt;&lt;/p&gt;
&lt;p&gt;There is another genre of games that could be a perfect petri dish for such an AI — &lt;a class="link" href="https://en.wikipedia.org/wiki/Dating_sim" target="_blank" rel="noopener"
&gt;&lt;strong&gt;Dating sims&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; Relying heavily on the text they are much closer to situations to what GPT-3 is used to deal with. They usually have a few characters and most of the time do not present any real challenge to the player, rather just telling a story the author of the game had in mind. Another selling point of dating sims is the great art that the game presents to the player. Here is where our new cool kid — Stable Diffusion — could show himself in full glory. The idea seems obvious, we should use GPT-3 to generate interactive stories and prompts for Stable Diffusion, that would generate images for that story. Even better, with technology like &lt;a class="link" href="https://dreambooth.github.io/" target="_blank" rel="noopener"
&gt;DreamBooth&lt;/a&gt; that allows us to introduce new concepts to image generation model, we should be able to produce coherent locations, characters and style of the images. But will it really work in practice?
To test this idea I decided to emulate a dating game within a simple text document.&lt;/p&gt;
&lt;p&gt;The GPT-3 prompt I used was looking in the following way:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The following is the dialog between the character Alice Lake (Jessica Chastain) and the player.
Alice Lake (Jessica Chastain) does not know the player yet, but is very happy to meet him and help with anything he is asking.
The following happened in the big house where the player woke up with huge pain in his head.
…
Short summary of the dialog so far: &amp;ldquo;[First GPT API call, do not need to call it if it is the first message]&amp;rdquo;
Alice: &amp;ldquo;[Second GPT API call]&amp;rdquo;
Description of an image shown to the player: &amp;ldquo;[Third GPT API call]&amp;rdquo;
Player: &amp;ldquo;[player&amp;rsquo;s input]&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;“…” — represents the previous state of the dialog. Because we have a “Short summary of the dialog so far” section that works like a long-term memory, we do not need to store more than the last two messages.&lt;/p&gt;
&lt;p&gt;Here is the first message from the character, description of an image shown to the player and the actual image generated from this description, as well as player’s answer to the character:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_S9ugIxbEZaTfCkO9FY-p8g.png"
width="911"
height="802"
srcset="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_S9ugIxbEZaTfCkO9FY-p8g_hu_9d35179417b5b4a8.png 480w, https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_S9ugIxbEZaTfCkO9FY-p8g_hu_4ed6039dbbe29053.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="113"
data-flex-basis="272px"
&gt;&lt;/p&gt;
&lt;p&gt;I used the &lt;a class="link" href="https://huggingface.co/Linaqruf/anything-v3.0" target="_blank" rel="noopener"
&gt;Anything-V3&lt;/a&gt; model as it could generate visually pleasing results consistently, without too much prompt engineering. To not waste time training the DreamBooth model for coherent rendering of characters I simply used a famous person, in my case this was Jessica Chastain, as a character anchor and vague description of the clothes in the prompt. In the end the prompt I used to generate images was the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prompt: [Description of an image from GPT-3] + “Detailed photo-realistic anime style. Inside a house. Tight red dress.”&lt;/p&gt;
&lt;p&gt;Negative: (((ugly))), fake image, blurry image, blur, corrupted image, old black and white photo, out of frame, without head, too close, cropped, collage, split image&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I tried to keep the first image that was generated, although I did allow myself to regenerate an image if it was completely off as it happened a couple of times.&lt;/p&gt;
&lt;p&gt;The first pleasant surprise I got was when I asked Alice’s character to give me a cup of coffee. I was expected to see another simple close up view of the character but instead got this:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_ZtlE7YdSKg6ILO5cmfKMOw.png"
width="911"
height="802"
srcset="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_ZtlE7YdSKg6ILO5cmfKMOw_hu_750145c0cbffb0b0.png 480w, https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_ZtlE7YdSKg6ILO5cmfKMOw_hu_c013a077f71182ce.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="113"
data-flex-basis="272px"
&gt;&lt;/p&gt;
&lt;p&gt;As we could see the description of an image does not contain two cups of coffee, so this was just a coincidence, although it was not the last such a surprise. When I asked Alice to find my documents I got an image of her actually looking around the house.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_Q8LnZMyGUOaQffr-0ggEEQ.png"
width="908"
height="826"
srcset="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_Q8LnZMyGUOaQffr-0ggEEQ_hu_2169cbde4fe0b059.png 480w, https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_Q8LnZMyGUOaQffr-0ggEEQ_hu_b9270b905cc19bd0.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="109"
data-flex-basis="263px"
&gt;&lt;/p&gt;
&lt;p&gt;At this point it was clear that the idea in general was working. As the next step I tried to see limitations of the system. I tried to “get rid” of the character and make it show me some inanimate object. I hoped that GPT-3 would generate a description of an image that does not contain Alice, but it kept her in nevertheless.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_u-WuVDmnsfDaArk_2u2T_Q.png"
width="910"
height="852"
srcset="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_u-WuVDmnsfDaArk_2u2T_Q_hu_a017c9382dfeeb3e.png 480w, https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_u-WuVDmnsfDaArk_2u2T_Q_hu_cdea1ddae66e02cf.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="106"
data-flex-basis="256px"
&gt;&lt;/p&gt;
&lt;p&gt;At some point GPT-3 decided to put the player into the image. Here is where the most important limitation became clear — for a coherent story not only the image should depend on the dialog, the dialog itself should also depend on the generated images.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_8If70qcQa3Ub-vG-V9GgMA.png"
width="910"
height="798"
srcset="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_8If70qcQa3Ub-vG-V9GgMA_hu_fd3fe64e2d02a1f8.png 480w, https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_8If70qcQa3Ub-vG-V9GgMA_hu_709a4e0c1854e2bf.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="114"
data-flex-basis="273px"
&gt;&lt;/p&gt;
&lt;p&gt;The main takeaway of this little experiment that I got is that we are very close to creating believable characters powered by an AI. The only thing we really need is a multimodal approach as powerful as GPT-3 and Stable Diffusion combined, able to generate text and images as a coherent stream of tokens. It would also be beneficial to have some sort of memory in the form of learned special tokens that contain information of the previous events in a compressed form as the “Short summary of the dialog so far” section does. The good news, that new models like &lt;a class="link" href="https://arxiv.org/pdf/2204.14198.pdf" target="_blank" rel="noopener"
&gt;Flamingo&lt;/a&gt; from DeepMind could do exactly that.&lt;/p&gt;
&lt;p&gt;The whole story I got by interacting with “the game” is available in &lt;a class="link" href="https://docs.google.com/document/d/1G4f0ta9f2T_WtRbGY7qFEBbh2dH2zrnVt7yNK-KjKnk/" target="_blank" rel="noopener"
&gt;this document&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s see what the future holds for us…&lt;/p&gt;</description></item></channel></rss>