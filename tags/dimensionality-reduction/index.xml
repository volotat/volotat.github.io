<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Dimensionality Reduction on Alexey Borsky</title><link>https://volotat.github.io/tags/dimensionality-reduction/</link><description>Recent content in Dimensionality Reduction on Alexey Borsky</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 18 Nov 2017 00:00:00 +0000</lastBuildDate><atom:link href="https://volotat.github.io/tags/dimensionality-reduction/index.xml" rel="self" type="application/rss+xml"/><item><title>Low Dimensional Autoencoder</title><link>https://volotat.github.io/projects/2017-11-18-low-dimensional-autoencoder/</link><pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2017-11-18-low-dimensional-autoencoder/</guid><description>&lt;img src="https://volotat.github.io/projects/2017-11-18-low-dimensional-autoencoder/latent_space.png" alt="Featured image of post Low Dimensional Autoencoder" /&gt;&lt;p&gt;An experimental approach to training autoencoders for meaningful low-dimensional data representation.&lt;/p&gt;
&lt;p&gt;This project was one of my first experiments in machine learning, serving as a deep dive into how autoencoders process information. The goal was to address the difficulty of encoding high-dimensional data (like images) into very low-dimensional latent spaces (like 2D) without the model getting stuck in local minima. Usually it takes at least 16 or more dimensions for bottleneck embeddings to get decent results, but I wanted to see if it was possible to achieve this in just 2 dimensions.&lt;/p&gt;
&lt;p&gt;The method involves training the autoencoder in a typical fashion, alongside with the encoder that trains on a dynamic set of target representations. It works by initially setting random points for each datapoint and then, at each step, stretching the latent space to touch the predefined space boundaries while applying a repulsion force to keep datapoints distinct. This prevents points from clustering too densely and encourages a more uniform distribution.&lt;/p&gt;
&lt;p&gt;The result, as seen in the project image, is a continuous 2D embedding space that successfully compacts the entire MNIST dataset, allowing for smooth transitions between different digits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/Low-Dimensional-Autoencoder" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item></channel></rss>