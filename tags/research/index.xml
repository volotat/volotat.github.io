<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Research on Alexey Borsky</title><link>https://volotat.github.io/tags/research/</link><description>Recent content in Research on Alexey Borsky</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 15 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://volotat.github.io/tags/research/index.xml" rel="self" type="application/rss+xml"/><item><title>4D Volumetric Retina Simulation</title><link>https://volotat.github.io/projects/2025-05-15-4d-volumetric-retina-simulation/</link><pubDate>Thu, 15 May 2025 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2025-05-15-4d-volumetric-retina-simulation/</guid><description>&lt;img src="https://volotat.github.io/projects/2025-05-15-4d-volumetric-retina-simulation/4D_eye_preview.png" alt="Featured image of post 4D Volumetric Retina Simulation" /&gt;&lt;p&gt;A physics-based 4D path tracing simulation visualizing how a four-dimensional being might perceive the world through a 3D volumetric retina.&lt;/p&gt;
&lt;p&gt;Current visualizations of 4D space often rely on wireframe projections or simple 3D cross-sections. This project takes a more biologically plausible approach: analogous to how we 3D beings perceive our world via 2D retinas, a 4D creature would likely possess a 3D (volumetric) retina.&lt;/p&gt;
&lt;p&gt;This simulation implements a custom 4D path tracing engine (using Python and Taichi for GPU acceleration) to model light interactions within a hyper-scene containing a rotating tesseract. It simulates image formation by casting 4D rays onto a defined 3D retinal volume.&lt;/p&gt;
&lt;p&gt;The simulation features physically-based rendering that models light bounces, shadows, and perspective in four spatial dimensions. Simulates a 3D sensor array rather than a flat plane. Implements a Gaussian fall-off for retinal sensitivity, mimicking foveal vision where the center of the 3D gaze is most acute. To make this comprehensible to human eyes, the 3D retinal image is composited from multiple depth slices, additively blended to represent the density of information a 4D being would process simultaneously.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/4DRender" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt; | &lt;a class="link" href="https://www.youtube.com/shorts/T697zlLvvHw" target="_blank" rel="noopener"
&gt;Video Showcase&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Vector Based Language</title><link>https://volotat.github.io/projects/2022-08-27-vector-based-language/</link><pubDate>Sat, 27 Aug 2022 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2022-08-27-vector-based-language/</guid><description>&lt;img src="https://volotat.github.io/projects/2022-08-27-vector-based-language/1_lgXj1-BGYkDVqLytrz9D9A.png" alt="Featured image of post Vector Based Language" /&gt;&lt;p&gt;Research project exploring the possibility of creating a continuous, visual language that hint at the possibility of direct understanding of the embedding spaces produced by ML models.&lt;/p&gt;
&lt;p&gt;Unlike traditional discrete languages, this system uses machine learning to generate unique visual representations (images) for any given text embedding. The goal is to create a language where concepts can blend into each other continuously, mirroring how neural networks process information.&lt;/p&gt;
&lt;p&gt;Words and sentences are represented as points in a continuous vector space, allowing for smooth transitions between concepts. Experiments show that humans can learn to interpret these generated visual embeddings with increasing accuracy over time. The visual language is designed to be perfectly reconstructible back into the original text embeddings by a decoder network crating a bridge between human cognition and the &amp;ldquo;black box&amp;rdquo; mechanized interpretation of the data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/" &gt;Read the Article&lt;/a&gt; | &lt;a class="link" href="https://github.com/volotat/Vector-Based-Language" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Differentiable Morphing</title><link>https://volotat.github.io/projects/2021-01-02-differentiable-morphing/</link><pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2021-01-02-differentiable-morphing/</guid><description>&lt;img src="https://volotat.github.io/projects/2021-01-02-differentiable-morphing/formula.jpg" alt="Featured image of post Differentiable Morphing" /&gt;&lt;p&gt;Image morphing without reference points by optimizing warp maps via gradient descent.&lt;/p&gt;
&lt;p&gt;This project introduces a &amp;ldquo;differentiable morphing&amp;rdquo; algorithm that can smoothly transition between any two images without requiring manual reference points or landmarks. Unlike traditional generative models that learn a distribution from a dataset, this approach uses a neural network as a temporary functional mapping to solve a specific optimization problem for a single pair of images.&lt;/p&gt;
&lt;p&gt;The algorithm finds a set of maps that transform the source image into the target image.&lt;/p&gt;
&lt;!--The algorithm finds a set of maps that transform the source image ($A$) into the target image ($B$) using the following logic:
$$ B = (A \times \text{mult\_map} + \text{add\_map}) \circ \text{warp\_map} $$
* **Mult Map:** Removes unnecessary parts and adjusts localized color balance.
* **Add Map:** Introduces new colors not present in the original image.
* **Warp Map:** Distorts the image geometry to handle rotation, scaling, and movement of objects.--&gt;
&lt;p&gt;By interpolating the strength of these maps, the system produces a smooth, seamless animation where features transform fluidly from one state to another.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;h3 id="cited-in-scientific-literature"&gt;Cited in Scientific Literature
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="link" href="https://www.sciencedirect.com/science/article/pii/S0167739X22000838" target="_blank" rel="noopener"
&gt;The explainability paradox: Challenges for xAI in digital pathology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://arxiv.org/abs/2311.06792" target="_blank" rel="noopener"
&gt;IMPUS: Image Morphing with Perceptually-Uniform Sampling Using Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://arxiv.org/abs/2507.01953" target="_blank" rel="noopener"
&gt;FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/DiffMorph" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item></channel></rss>