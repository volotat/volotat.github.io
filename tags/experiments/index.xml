<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Experiments on Alexey Borsky</title><link>https://volotat.github.io/tags/experiments/</link><description>Recent content in Experiments on Alexey Borsky</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 15 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://volotat.github.io/tags/experiments/index.xml" rel="self" type="application/rss+xml"/><item><title>4D Volumetric Retina Simulation</title><link>https://volotat.github.io/projects/2025-05-15-4d-volumetric-retina-simulation/</link><pubDate>Thu, 15 May 2025 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2025-05-15-4d-volumetric-retina-simulation/</guid><description>&lt;img src="https://volotat.github.io/projects/2025-05-15-4d-volumetric-retina-simulation/4D_eye_preview.png" alt="Featured image of post 4D Volumetric Retina Simulation" /&gt;&lt;p&gt;A physics-based 4D path tracing simulation visualizing how a four-dimensional being might perceive the world through a 3D volumetric retina.&lt;/p&gt;
&lt;p&gt;Current visualizations of 4D space often rely on wireframe projections or simple 3D cross-sections. This project takes a more biologically plausible approach: analogous to how we 3D beings perceive our world via 2D retinas, a 4D creature would likely possess a 3D (volumetric) retina.&lt;/p&gt;
&lt;p&gt;This simulation implements a custom 4D path tracing engine (using Python and Taichi for GPU acceleration) to model light interactions within a hyper-scene containing a rotating tesseract. It simulates image formation by casting 4D rays onto a defined 3D retinal volume.&lt;/p&gt;
&lt;p&gt;The simulation features physically-based rendering that models light bounces, shadows, and perspective in four spatial dimensions. Simulates a 3D sensor array rather than a flat plane. Implements a Gaussian fall-off for retinal sensitivity, mimicking foveal vision where the center of the 3D gaze is most acute. To make this comprehensible to human eyes, the 3D retinal image is composited from multiple depth slices, additively blended to represent the density of information a 4D being would process simultaneously.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/4DRender" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt; | &lt;a class="link" href="https://www.youtube.com/shorts/T697zlLvvHw" target="_blank" rel="noopener"
&gt;Video Showcase&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Generative Art Synthesizer</title><link>https://volotat.github.io/projects/2021-12-04-generative-art-synthesizer/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2021-12-04-generative-art-synthesizer/</guid><description>&lt;img src="https://volotat.github.io/projects/2021-12-04-generative-art-synthesizer/gas_preview.png" alt="Featured image of post Generative Art Synthesizer" /&gt;&lt;p&gt;A Python program that generates Python programs that generate generative art.&lt;/p&gt;
&lt;p&gt;Most generative art relies on stochastic processes where the initial seed and specific parameters are often lost, making exact reproduction difficult. GAS takes a different approach: instead of storing just the output or the parameters, it generates a fully deterministic, standalone Python script for each artwork. This ensures complete reproducibilityâ€”if you have the script, you have the art.&lt;/p&gt;
&lt;p&gt;The core mechanism involves initializing a tensor with coordinate data and then applying a random sequence of mathematical transformations (like &lt;code&gt;transit&lt;/code&gt;, &lt;code&gt;sin&lt;/code&gt;, &lt;code&gt;magnitude&lt;/code&gt;, &lt;code&gt;shift&lt;/code&gt;) to its channels. These operations are restricted to the [-1, 1] range to ensure stability. The final result is a composition of these channels converted into color space.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Self-Contained Art:&lt;/strong&gt; Each generated piece is a runnable Python script with zero external dependencies beyond standard scientific libraries (numpy, PIL).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deterministic:&lt;/strong&gt; The generated scripts contain no random elements; running the same script always produces the exact same image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Method-Based generation:&lt;/strong&gt; Uses a palette of composable mathematical functions (&lt;code&gt;sin&lt;/code&gt;, &lt;code&gt;prod&lt;/code&gt;, &lt;code&gt;soft_min&lt;/code&gt;, etc.) to &amp;ldquo;sculpt&amp;rdquo; the image in a high-dimensional channel space.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Aesthetic Scoring:&lt;/strong&gt; Includes a simple scoring model to estimate the visual quality of generated outputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/GAS" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Differentiable Morphing</title><link>https://volotat.github.io/projects/2021-01-02-differentiable-morphing/</link><pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2021-01-02-differentiable-morphing/</guid><description>&lt;img src="https://volotat.github.io/projects/2021-01-02-differentiable-morphing/formula.jpg" alt="Featured image of post Differentiable Morphing" /&gt;&lt;p&gt;Image morphing without reference points by optimizing warp maps via gradient descent.&lt;/p&gt;
&lt;p&gt;This project introduces a &amp;ldquo;differentiable morphing&amp;rdquo; algorithm that can smoothly transition between any two images without requiring manual reference points or landmarks. Unlike traditional generative models that learn a distribution from a dataset, this approach uses a neural network as a temporary functional mapping to solve a specific optimization problem for a single pair of images.&lt;/p&gt;
&lt;p&gt;The algorithm finds a set of maps that transform the source image into the target image.&lt;/p&gt;
&lt;!--The algorithm finds a set of maps that transform the source image ($A$) into the target image ($B$) using the following logic:
$$ B = (A \times \text{mult\_map} + \text{add\_map}) \circ \text{warp\_map} $$
* **Mult Map:** Removes unnecessary parts and adjusts localized color balance.
* **Add Map:** Introduces new colors not present in the original image.
* **Warp Map:** Distorts the image geometry to handle rotation, scaling, and movement of objects.--&gt;
&lt;p&gt;By interpolating the strength of these maps, the system produces a smooth, seamless animation where features transform fluidly from one state to another.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;h3 id="cited-in-scientific-literature"&gt;Cited in Scientific Literature
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="link" href="https://www.sciencedirect.com/science/article/pii/S0167739X22000838" target="_blank" rel="noopener"
&gt;The explainability paradox: Challenges for xAI in digital pathology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://arxiv.org/abs/2311.06792" target="_blank" rel="noopener"
&gt;IMPUS: Image Morphing with Perceptually-Uniform Sampling Using Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://arxiv.org/abs/2507.01953" target="_blank" rel="noopener"
&gt;FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/DiffMorph" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>ConGAN</title><link>https://volotat.github.io/projects/2018-01-29-congan/</link><pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2018-01-29-congan/</guid><description>&lt;img src="https://volotat.github.io/projects/2018-01-29-congan/faces_table.png" alt="Featured image of post ConGAN" /&gt;&lt;p&gt;Continuous Adversarial Image Generator that can produce high-quality images with relatively few examples and resolution independence.&lt;/p&gt;
&lt;p&gt;This project implements a Generative Adversarial Network (GAN) that learns a continuous function mapping coordinates (x, y) and a latent vector z to an RGB color, rather than generating a fixed grid of pixels. By treating images as continuous functions, the model creates a &amp;ldquo;vector-like&amp;rdquo; representation that can be sampled at any resolution.&lt;/p&gt;
&lt;p&gt;The architecture differs from traditional GANs by using a coordinate-based generator. The system includes an &amp;ldquo;Ident&amp;rdquo; network to map images to a latent space, a Generator that predicts colors based on position and latent codes, and a Discriminator that validates pixel data in context.&lt;/p&gt;
&lt;p&gt;Results demonstrated that the model could generalize well from small datasets (10-20 images is enough for reasonable results, while training the network completely from the scratch) and produce smooth interpolations in the latent space, effectively hallucinating plausible variations between training examples.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/ConGAN" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Low Dimensional Autoencoder</title><link>https://volotat.github.io/projects/2017-11-18-low-dimensional-autoencoder/</link><pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2017-11-18-low-dimensional-autoencoder/</guid><description>&lt;img src="https://volotat.github.io/projects/2017-11-18-low-dimensional-autoencoder/latent_space.png" alt="Featured image of post Low Dimensional Autoencoder" /&gt;&lt;p&gt;An experimental approach to training autoencoders for meaningful low-dimensional data representation.&lt;/p&gt;
&lt;p&gt;This project was one of my first experiments in machine learning, serving as a deep dive into how autoencoders process information. The goal was to address the difficulty of encoding high-dimensional data (like images) into very low-dimensional latent spaces (like 2D) without the model getting stuck in local minima. Usually it takes at least 16 or more dimensions for bottleneck embeddings to get decent results, but I wanted to see if it was possible to achieve this in just 2 dimensions.&lt;/p&gt;
&lt;p&gt;The method involves training the autoencoder in a typical fashion, alongside with the encoder that trains on a dynamic set of target representations. It works by initially setting random points for each datapoint and then, at each step, stretching the latent space to touch the predefined space boundaries while applying a repulsion force to keep datapoints distinct. This prevents points from clustering too densely and encourages a more uniform distribution.&lt;/p&gt;
&lt;p&gt;The result, as seen in the project image, is a continuous 2D embedding space that successfully compacts the entire MNIST dataset, allowing for smooth transitions between different digits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/Low-Dimensional-Autoencoder" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item></channel></rss>