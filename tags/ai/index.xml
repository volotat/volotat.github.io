<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on Alexey Borsky</title><link>https://volotat.github.io/tags/ai/</link><description>Recent content in AI on Alexey Borsky</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 21 Aug 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://volotat.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>There is a way to make training LLMs way cheaper and more accessible.</title><link>https://volotat.github.io/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/</link><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate><guid>https://volotat.github.io/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/</guid><description>&lt;img src="https://volotat.github.io/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/0_4fPJHVhCQeYNef_n.png" alt="Featured image of post There is a way to make training LLMs way cheaper and more accessible." /&gt;&lt;h2 id="introduction"&gt;Introduction
&lt;/h2&gt;&lt;p&gt;What if I told you that a vast amount of the computation that goes into training every new Large Language Model is completely redundant?&lt;/p&gt;
&lt;p&gt;This isn’t just a minor inefficiency, it’s a fundamental flaw in our current approach. We are spending billions of dollars and burning gigawatts of energy for every major AI company to teach their models the same fundamental understanding of the world, over and over again.&lt;/p&gt;
&lt;p&gt;Here is the claim I am going to justify: &lt;strong&gt;the hardest part of what all LLMs learn is NOT next-token prediction, but rather the arbitrary data compression into a meaningful vector representation.&lt;/strong&gt; And here’s the catch — this representation is fundamentally the same for every model (up to symmetries and precision).&lt;/p&gt;
&lt;p&gt;The work that inspires this claim is &lt;a class="link" href="https://arxiv.org/pdf/2405.07987" target="_blank" rel="noopener"
&gt;the &lt;strong&gt;Platonic Representation Hypothesis&lt;/strong&gt;&lt;/a&gt;. This hypothesis suggests that for any given data, there exists a “perfect” or ideal mathematical representation — a Platonic form, if you will. All our current models are simply trying to find their own noisy, imperfect approximation of this one true representation.&lt;/p&gt;
&lt;h2 id="how-a-perfect-llm-should-look"&gt;How a “Perfect” LLM Should Look
&lt;/h2&gt;&lt;p&gt;If the Platonic Representation Hypothesis holds true, it means we’re building our models upside down. We train monolithic, end-to-end models that learn to embed data and reason about it at the same time. Instead, we should separate these concerns.&lt;/p&gt;
&lt;p&gt;The future of efficient AI should be built on a simple, two-part architecture:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;The Universal Encoder:&lt;/strong&gt; A single, global, state-of-the-art model that does only one thing: convert any piece of data (text, images, sound, etc., in a continuous sequence) into its perfect “Platonic” vector embedding. This model would be trained once on a colossal dataset and then frozen, serving as a foundational piece of public infrastructure.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Task Model:&lt;/strong&gt; A much smaller, specialized model that takes these “perfect” embedding (that represents all the current context window that goes to the model) as an input and learns to perform a specific task. This could be next-token prediction, classification, image denoising (for diffusion models), or even complex, one-shot reasoning like question answering or code generation. All personalization, alignment, and RLHF would happen at this much cheaper, more efficient level.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/1_x7iLYZUfjMjV7oNkTjRShA.png"
width="444"
height="532"
srcset="https://volotat.github.io/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/1_x7iLYZUfjMjV7oNkTjRShA_hu_e5f393929ad642bc.png 480w, https://volotat.github.io/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/1_x7iLYZUfjMjV7oNkTjRShA_hu_b60bb99d25994b49.png 1024w"
loading="lazy"
alt="A. — Current approach. B. — Proposed approach."
class="gallery-image"
data-flex-grow="83"
data-flex-basis="200px"
&gt;&lt;/p&gt;
&lt;p&gt;Think of it like this: today, every AI student has to first invent an entire language from scratch — its alphabet, vocabulary, and grammar — before they can even begin to learn to write an essay. In the new paradigm, they all share a common, universal language (the Universal Embeddings) with all the world’s concepts already encoded within it. The students (the Task Models) can then focus immediately on the creative act of writing the essay. Training these task-specific models would be orders of magnitude cheaper and faster.&lt;/p&gt;
&lt;h2 id="universal-relational-embedding-space"&gt;Universal, Relational Embedding Space
&lt;/h2&gt;&lt;p&gt;So, how do we find these universal embeddings and represent them in a way that is stable and suitable for any task?&lt;/p&gt;
&lt;p&gt;Right now, embeddings are encoded as a point in a very high-dimensional space. The problem is that the coordinate system of this space is set arbitrarily during the training process. This means every model has its own unique, incompatible space. Training the same model twice will generate two completely different spaces. You can’t just take an embedding from GPT-4 and use it in Gemini; their internal “languages” are different.&lt;/p&gt;
&lt;p&gt;Recent research on the &lt;a class="link" href="https://arxiv.org/pdf/2505.12540" target="_blank" rel="noopener"
&gt;Universal Geometry of Embeddings&lt;/a&gt; shows that while the coordinate systems are different, the underlying geometric relationships &lt;em&gt;between&lt;/em&gt; the embeddings are remarkably similar. We can remap one model’s space onto another. But this is a patch, not a solution. It proves the point: what truly matters isn’t the absolute coordinates of a concept, but its &lt;em&gt;relationships&lt;/em&gt; to all other concepts.&lt;/p&gt;
&lt;p&gt;What we truly care about are the &lt;strong&gt;distances&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Instead of a vector representing a point in space, what if an embedding vector represented a list of distances?&lt;/p&gt;
&lt;p&gt;Imagine trying to describe the location of your city. You could give its absolute GPS coordinates (e.g., 46.10° N, 19.66° E). This is the current approach — precise, but meaningless without the entire GPS coordinate system.&lt;/p&gt;
&lt;p&gt;Alternatively, you could describe it relationally: “It’s 180km from Budapest, 190km from Belgrade, 560km from Vienna…” This is a &lt;em&gt;distance-based&lt;/em&gt; representation. It’s inherently meaningful.&lt;/p&gt;
&lt;p&gt;This is how our Universal Embedding should work. Each vector would not be a coordinate, but a list of distances to a set of universal, canonical “landmark concepts” in the embedding space.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;embedding(&amp;quot;cat&amp;quot;) = [dist_to_concept_1, dist_to_concept_2, dist_to_concept_3, ...]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/1_Tgc3uuUVrYti-zqfKCahSw.png"
width="1400"
height="700"
srcset="https://volotat.github.io/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/1_Tgc3uuUVrYti-zqfKCahSw_hu_b7f7318b1358607c.png 480w, https://volotat.github.io/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/1_Tgc3uuUVrYti-zqfKCahSw_hu_c5821459ded753bc.png 1024w"
loading="lazy"
alt="captionless image"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
&gt;&lt;/p&gt;
&lt;p&gt;We don’t even need to know what these landmark concepts are explicitly. They can be learned by the model. &lt;strong&gt;The key is that they are ordered by importance, from the most general to the most specific.&lt;/strong&gt; The first value might represent the distance to the abstract concept of “objectness,” the second to “living thing,” the tenth to “mammal,” and so on.&lt;/p&gt;
&lt;p&gt;This structure has two incredible properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;It’s Stable:&lt;/strong&gt; Because it’s based on internal distances, it’s invariant to rotation or translation. Every training run would converge to the same relational representation, creating a stable and universal standard.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It’s Inherently Composable:&lt;/strong&gt; This is very similar to the idea of &lt;a class="link" href="https://huggingface.co/papers/2205.13147" target="_blank" rel="noopener"
&gt;Matryoshka Representation Learning&lt;/a&gt;. If you need a smaller, less precise embedding, you can just take the first N values of the vector! You’re simply using the distances to the most important landmark concepts, giving you a coarse but still highly effective representation.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="we-need-the-embedding-model"&gt;We Need THE Embedding Model
&lt;/h2&gt;&lt;p&gt;The centerpiece of this vision is the &lt;strong&gt;Universal Encoder&lt;/strong&gt;, or THE Embedding Model. This wouldn’t be just another model like &lt;code&gt;text-embedding-3-large&lt;/code&gt;; it would be a foundational piece of infrastructure for the entire AI ecosystem, akin to the GPS network or the TCP/IP protocol.&lt;/p&gt;
&lt;p&gt;This model, trained on a dataset far beyond the scale of any single company, would create this true distance-based, Matryoshka-style embedding space. This would be the definitive, canonical representation of knowledge.&lt;/p&gt;
&lt;h2 id="the-benefits-would-be-transformative"&gt;The Benefits Would Be Transformative
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Backward Compatibility and Continuous Improvement:&lt;/strong&gt; New versions of the Universal Encoder would be released as research progresses. Since each new version is just a better approximation of the same underlying Platonic representation, they should be largely backward compatible. This means you could swap in the new encoder and expect any pre-trained, task-specific models to work with the same or even better performance, with minimal re-training.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Simplified RAG and Vector Search:&lt;/strong&gt; Retrieval-Augmented Generation (RAG) and all vector search applications would be greatly simplified. You would only need a single base embedding model for any type of data. Your text, image, and audio databases would all exist within the same unified, coherent vector space, making cross-modal search and reasoning trivial.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Democratization of AI:&lt;/strong&gt; The colossal cost of training foundational models from scratch would be a one-time, collaborative effort. Researchers, startups, and even individuals could then build powerful, specialized AI applications by training only the small, inexpensive task models on top of the universal embeddings.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="a-call-for-a-new-direction"&gt;A Call for a New Direction
&lt;/h2&gt;&lt;p&gt;I truly believe this is the future we must build. With this approach, AI could become far more open, accessible, and environmentally sustainable.&lt;/p&gt;
&lt;p&gt;However, this vision is a direct threat to the power held by the big companies that currently dominate the AI space. Their moat is the sheer scale of their proprietary, monolithically trained models. A shared, universal encoder is not in their immediate financial interest.&lt;/p&gt;
&lt;p&gt;Therefore, this message is a call to action for the global ML community and AI enthusiasts. The creation of such a foundational model will likely not come from a single corporation, but from a decentralized, open-source effort. Only as a global community do we have a shot at creating an AI future that is more efficient, decentralized, and accessible for everyone.&lt;/p&gt;</description></item><item><title>SD-CN-Animation</title><link>https://volotat.github.io/projects/2023-03-17-sd-cn-animation/</link><pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2023-03-17-sd-cn-animation/</guid><description>&lt;img src="https://volotat.github.io/projects/2023-03-17-sd-cn-animation/ui_preview.png" alt="Featured image of post SD-CN-Animation" /&gt;&lt;p&gt;This project was developed as an extension for the &lt;a class="link" href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" target="_blank" rel="noopener"
&gt;Automatic1111 web UI&lt;/a&gt; to automate video stylization and enable text-to-video generation using &lt;a class="link" href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5" target="_blank" rel="noopener"
&gt;Stable Diffusion 1.5&lt;/a&gt; backbones. At the time of development, generated videos often suffered from severe flickering and temporal inconsistency. This framework addressed those issues by integrating the &lt;a class="link" href="https://github.com/princeton-vl/RAFT" target="_blank" rel="noopener"
&gt;RAFT&lt;/a&gt; optical flow estimation algorithm. By calculating the motion flow between frames, the system could warp the previously generated frame to match the motion of the next one, creating a stable base for the diffusion model. This process, combined with occlusion masks, ensured that only new parts of the scene were generated while maintaining the consistency of existing objects.&lt;/p&gt;
&lt;p&gt;The tool supported both video-to-video stylization and experimental text-to-video generation. In video-to-video mode, users could apply &lt;a class="link" href="https://huggingface.co/lllyasviel/ControlNet" target="_blank" rel="noopener"
&gt;ControlNet&lt;/a&gt; to guide the structure of the output, allowing for stable transformations like turning a real-life video into a watercolor painting or digital art while preserving the original motion. The text-to-video mode employed a custom &amp;ldquo;FloweR&amp;rdquo; method to hallucinate optical flow from static noise, attempting to generate continuous motion from text prompts alone.&lt;/p&gt;
&lt;p&gt;Development on this project was eventually discontinued as the field rapidly advanced. The emergence of modern, end-to-end text-to-video models provided much more coherent and faithful results than could be achieved by hacking image-based diffusion models, rendering this approach largely obsolete for general use cases.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Not Maintained&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/SD-CN-Animation" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Combining GPT-3 and Stable Diffusion to imagine a next level game engine.</title><link>https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/</link><pubDate>Fri, 16 Dec 2022 00:00:00 +0000</pubDate><guid>https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/</guid><description>&lt;p&gt;The Holy Grail of all RPG games always was an AI that could reliably reply to what the player says and react to his actions. This was openly pronounced right from the beginning of the genre when text-based games started being developed. However, despite decades of research, there still has not been a perfected AI for RPGs that can truly understand and react to natural language. With the appearance of GPT-3 and other large language models a glimmer of hope appeared. Yet as the world of RPGs is so large and open-ended, it requires an AI with a vast amount of understanding and context about the environment and the player’s actions in order to be able to produce realistic responses. This means that creating an AI which can reliably interact with players is an extremely difficult task — one which has yet to be achieved. Although &lt;a class="link" href="https://aidungeon.io/" target="_blank" rel="noopener"
&gt;AI Dungeon&lt;/a&gt; seems to be a good step forward.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/0_Tu0Tpn1RQg20HLYq.png"
width="640"
height="480"
srcset="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/0_Tu0Tpn1RQg20HLYq_hu_de769f9e24fd305a.png 480w, https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/0_Tu0Tpn1RQg20HLYq_hu_35f571e181ac915.png 1024w"
loading="lazy"
alt="In Fallout 1 players could try to ask characters about anything using the “Tell me about” feature. It didn’t work well."
class="gallery-image"
data-flex-grow="133"
data-flex-basis="320px"
&gt;&lt;/p&gt;
&lt;p&gt;There is another genre of games that could be a perfect petri dish for such an AI — &lt;a class="link" href="https://en.wikipedia.org/wiki/Dating_sim" target="_blank" rel="noopener"
&gt;&lt;strong&gt;Dating sims&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; Relying heavily on the text they are much closer to situations to what GPT-3 is used to deal with. They usually have a few characters and most of the time do not present any real challenge to the player, rather just telling a story the author of the game had in mind. Another selling point of dating sims is the great art that the game presents to the player. Here is where our new cool kid — Stable Diffusion — could show himself in full glory. The idea seems obvious, we should use GPT-3 to generate interactive stories and prompts for Stable Diffusion, that would generate images for that story. Even better, with technology like &lt;a class="link" href="https://dreambooth.github.io/" target="_blank" rel="noopener"
&gt;DreamBooth&lt;/a&gt; that allows us to introduce new concepts to image generation model, we should be able to produce coherent locations, characters and style of the images. But will it really work in practice?
To test this idea I decided to emulate a dating game within a simple text document.&lt;/p&gt;
&lt;p&gt;The GPT-3 prompt I used was looking in the following way:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The following is the dialog between the character Alice Lake (Jessica Chastain) and the player.
Alice Lake (Jessica Chastain) does not know the player yet, but is very happy to meet him and help with anything he is asking.
The following happened in the big house where the player woke up with huge pain in his head.
…
Short summary of the dialog so far: &amp;ldquo;[First GPT API call, do not need to call it if it is the first message]&amp;rdquo;
Alice: &amp;ldquo;[Second GPT API call]&amp;rdquo;
Description of an image shown to the player: &amp;ldquo;[Third GPT API call]&amp;rdquo;
Player: &amp;ldquo;[player&amp;rsquo;s input]&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;“…” — represents the previous state of the dialog. Because we have a “Short summary of the dialog so far” section that works like a long-term memory, we do not need to store more than the last two messages.&lt;/p&gt;
&lt;p&gt;Here is the first message from the character, description of an image shown to the player and the actual image generated from this description, as well as player’s answer to the character:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_S9ugIxbEZaTfCkO9FY-p8g.png"
width="911"
height="802"
srcset="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_S9ugIxbEZaTfCkO9FY-p8g_hu_9d35179417b5b4a8.png 480w, https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_S9ugIxbEZaTfCkO9FY-p8g_hu_4ed6039dbbe29053.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="113"
data-flex-basis="272px"
&gt;&lt;/p&gt;
&lt;p&gt;I used the &lt;a class="link" href="https://huggingface.co/Linaqruf/anything-v3.0" target="_blank" rel="noopener"
&gt;Anything-V3&lt;/a&gt; model as it could generate visually pleasing results consistently, without too much prompt engineering. To not waste time training the DreamBooth model for coherent rendering of characters I simply used a famous person, in my case this was Jessica Chastain, as a character anchor and vague description of the clothes in the prompt. In the end the prompt I used to generate images was the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prompt: [Description of an image from GPT-3] + “Detailed photo-realistic anime style. Inside a house. Tight red dress.”&lt;/p&gt;
&lt;p&gt;Negative: (((ugly))), fake image, blurry image, blur, corrupted image, old black and white photo, out of frame, without head, too close, cropped, collage, split image&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I tried to keep the first image that was generated, although I did allow myself to regenerate an image if it was completely off as it happened a couple of times.&lt;/p&gt;
&lt;p&gt;The first pleasant surprise I got was when I asked Alice’s character to give me a cup of coffee. I was expected to see another simple close up view of the character but instead got this:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_ZtlE7YdSKg6ILO5cmfKMOw.png"
width="911"
height="802"
srcset="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_ZtlE7YdSKg6ILO5cmfKMOw_hu_750145c0cbffb0b0.png 480w, https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_ZtlE7YdSKg6ILO5cmfKMOw_hu_c013a077f71182ce.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="113"
data-flex-basis="272px"
&gt;&lt;/p&gt;
&lt;p&gt;As we could see the description of an image does not contain two cups of coffee, so this was just a coincidence, although it was not the last such a surprise. When I asked Alice to find my documents I got an image of her actually looking around the house.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_Q8LnZMyGUOaQffr-0ggEEQ.png"
width="908"
height="826"
srcset="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_Q8LnZMyGUOaQffr-0ggEEQ_hu_2169cbde4fe0b059.png 480w, https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_Q8LnZMyGUOaQffr-0ggEEQ_hu_b9270b905cc19bd0.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="109"
data-flex-basis="263px"
&gt;&lt;/p&gt;
&lt;p&gt;At this point it was clear that the idea in general was working. As the next step I tried to see limitations of the system. I tried to “get rid” of the character and make it show me some inanimate object. I hoped that GPT-3 would generate a description of an image that does not contain Alice, but it kept her in nevertheless.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_u-WuVDmnsfDaArk_2u2T_Q.png"
width="910"
height="852"
srcset="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_u-WuVDmnsfDaArk_2u2T_Q_hu_a017c9382dfeeb3e.png 480w, https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_u-WuVDmnsfDaArk_2u2T_Q_hu_cdea1ddae66e02cf.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="106"
data-flex-basis="256px"
&gt;&lt;/p&gt;
&lt;p&gt;At some point GPT-3 decided to put the player into the image. Here is where the most important limitation became clear — for a coherent story not only the image should depend on the dialog, the dialog itself should also depend on the generated images.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_8If70qcQa3Ub-vG-V9GgMA.png"
width="910"
height="798"
srcset="https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_8If70qcQa3Ub-vG-V9GgMA_hu_fd3fe64e2d02a1f8.png 480w, https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/1_8If70qcQa3Ub-vG-V9GgMA_hu_709a4e0c1854e2bf.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="114"
data-flex-basis="273px"
&gt;&lt;/p&gt;
&lt;p&gt;The main takeaway of this little experiment that I got is that we are very close to creating believable characters powered by an AI. The only thing we really need is a multimodal approach as powerful as GPT-3 and Stable Diffusion combined, able to generate text and images as a coherent stream of tokens. It would also be beneficial to have some sort of memory in the form of learned special tokens that contain information of the previous events in a compressed form as the “Short summary of the dialog so far” section does. The good news, that new models like &lt;a class="link" href="https://arxiv.org/pdf/2204.14198.pdf" target="_blank" rel="noopener"
&gt;Flamingo&lt;/a&gt; from DeepMind could do exactly that.&lt;/p&gt;
&lt;p&gt;The whole story I got by interacting with “the game” is available in &lt;a class="link" href="https://docs.google.com/document/d/1G4f0ta9f2T_WtRbGY7qFEBbh2dH2zrnVt7yNK-KjKnk/" target="_blank" rel="noopener"
&gt;this document&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s see what the future holds for us…&lt;/p&gt;</description></item><item><title>Can Humans Speak the Language of the Machines?</title><link>https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/</link><pubDate>Tue, 06 Sep 2022 00:00:00 +0000</pubDate><guid>https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/</guid><description>&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_Tpm0MlWfLdNMGQ9b.png" alt="Featured image of post Can Humans Speak the Language of the Machines?" /&gt;&lt;h2 id="introduction"&gt;Introduction
&lt;/h2&gt;&lt;p&gt;In the movie Arrival (2016) a group of scientists tries to learn the language of extraterrestrial aliens that arrived on Earth and by doing so they change the way they think which allows the main character to obtain some extraordinary power. While such power is in the domain of fiction, the theory that our language dictates the way we think &lt;a class="link" href="https://en.wikipedia.org/wiki/Linguistic_relativity" target="_blank" rel="noopener"
&gt;has been widely explored in the literature&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There is a main crucial distinction in how people and current AI systems process information. All major human languages are based on discrete bags of words, while ML models almost always obtain and output information in terms of lists of real values. Because of this, all NLP models have a process of “vectorization” when a whole word, part of it, or single characters obtain their own vector representation that conveys their internal meaning accessible for further processing within the models. The main benefit of vectorization is that these vectors are continuous, therefore, could be adjusted (trained) through backpropagation. In a world of NLP models, the words “planet” and “moon” have some distance between each of them and could be gradually transformed from one to another.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_OiPWVzJqYD5qTEf7.png"
width="1400"
height="538"
srcset="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_OiPWVzJqYD5qTEf7_hu_3158852279ffec97.png 480w, https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_OiPWVzJqYD5qTEf7_hu_84228caddcd61232.png 1024w"
loading="lazy"
alt="Vector representations of words in latent space of NLP models"
class="gallery-image"
data-flex-grow="260"
data-flex-basis="624px"
&gt;&lt;/p&gt;
&lt;p&gt;Here is where the first glimpse of a new superpower will show up. Because of the discrete nature of our language people tend to think in terms of types, even when these types cannot be properly defined. The most obvious examples of it are in cosmology. There are some types of objects that at first glance fit very well into our discrete view of the world: planets, moons, stars, comets, asteroids, black holes and so on. But there are also some in-betweeny objects such as black dwarfs that are neither a planet nor a star but have properties of both depending from which angle you look at it. And there is Pluto… that was a planet, but lost its status when the definition of a planet was changed. The thing is, there is an ongoing debate that this new definition is not purely scientific and based on a recent observation of Pluto’s geological activity, &lt;a class="link" href="https://www.sciencedirect.com/science/article/pii/S0019103521004206" target="_blank" rel="noopener"
&gt;it should be a planet after all&lt;/a&gt;. The type system breakes when we find something that lies on the boundary between the types. And most of the time, a new type is created alongside a bunch of new boundaries that wait to be filled with new exceptions. It seems that it would be beneficial to have a language that does not have these boundaries in its nature and works more closely with how ML models work. So, here comes the question: &lt;strong&gt;could we create a language that is understandable by a human but works in a continuous fashion?&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="vector-based-language"&gt;Vector Based Language
&lt;/h2&gt;&lt;p&gt;In some sense, we already have a continuous language in the form of images. If we ask one human to draw a house and ask another what this image means, we have a high chance that the guess would be right. And a list of images might tell stories just as a sentence does. But there is a problem, for any word in a language there are infinite possibilities of depicting it in the form of an image, and then there are even more possibilities of interpreting the image back to words. So the language made of regular images would not be very reliable in conveying an exact meaning that was originally intended. To overcome this problem, we need a system that could generate some sort of an image for every word given and could guarantee that this image might be converted back to the original word exactly.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_qJX7THDcOFH_XBpi.png"
width="625"
height="790"
srcset="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_qJX7THDcOFH_XBpi_hu_f266c63775a7004b.png 480w, https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_qJX7THDcOFH_XBpi_hu_8301af913c7f316f.png 1024w"
loading="lazy"
alt="An example of a story told entirely with images"
class="gallery-image"
data-flex-grow="79"
data-flex-basis="189px"
&gt;&lt;/p&gt;
&lt;p&gt;The main tool we are going to use in the pursuit of developing such language is, once again, machine learning. The setup will look like this: there is a pre-trained language model that takes a word or a sentence as an input and produces some vector representation of it, there is also a second model that transforms this vector into some human-readable format, and lastly, there is a human who has to guess the original word that has been given. The human is allowed to take any time necessary to learn how the system works.&lt;/p&gt;
&lt;h2 id="architecture"&gt;Architecture
&lt;/h2&gt;&lt;p&gt;The general thing we want to achieve is to have a model that generates some embedding of the given text and then produces a visualization of these embeddings. To obtain embeddings of text, we will use &lt;a class="link" href="https://arxiv.org/abs/1803.11175" target="_blank" rel="noopener"
&gt;universal sentence encoder&lt;/a&gt; from Google, and to generate the visualization we will train generator network G. To do so, we need to have some approximation of the human visual system to make resulting images human-readable. In our case, this approximation would be a network V that we will initialize with mobile-net-v2 weights and allow these weights to change at the training stage.&lt;/p&gt;
&lt;p&gt;We are leaving the vision model trainable as it would bring another layer of regularization to the system. If we don’t do that and freeze the model, the decoding network tends to exploit inaccuracies in the visual model and settles on the solution that has a good reconstruction rate, yet all images are still pretty much the same, and only subtle differences are present that are very hard for a human to notice.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_ZY2XQdsCwSDpn3-aXAzv9w.png"
width="798"
height="904"
srcset="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_ZY2XQdsCwSDpn3-aXAzv9w_hu_cacc4d1e0d1d4571.png 480w, https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_ZY2XQdsCwSDpn3-aXAzv9w_hu_77ff654ceee10bbb.png 1024w"
loading="lazy"
alt="General representation of the system’s architecture"
class="gallery-image"
data-flex-grow="88"
data-flex-basis="211px"
&gt;&lt;/p&gt;
&lt;p&gt;We train this architecture with the following loss function:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_Rz4gSkKEn2uc4SFVKCEGSg.png"
width="1295"
height="619"
srcset="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_Rz4gSkKEn2uc4SFVKCEGSg_hu_9fef1db847a0b0ff.png 480w, https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_Rz4gSkKEn2uc4SFVKCEGSg_hu_c3e748cf832aff92.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="502px"
&gt;&lt;/p&gt;
&lt;p&gt;Where G — generator network, V — vision network, D — decoder network. I_1 and I_2 are embeddings of two randomly generated sentences.&lt;/p&gt;
&lt;p&gt;Here, the purpose of L_1 term is to make sure that distances in visual latent space are roughly the same as distances in word embeddings space. L_2 keeps images generated by G as far apart as possible in RBG space. L_3 keeps images generated by G as far apart as possible in visual space while keeping its mean at 0. L_4, the most important term, makes sure that reconstructed word embeddings are the same as initial ones. Values for α, β, γ are found empirically.&lt;/p&gt;
&lt;p&gt;To make the generator to be able to produce a meaningful image from any point of embedding space of the universal sentence encoder and prevent overfitting, we have to probe the space as densely as possible at the training stage. To do so, we will generate random sentences with the help of &lt;a class="link" href="https://www.nltk.org/" target="_blank" rel="noopener"
&gt;nltk&lt;/a&gt; library. Here are the examples of such generated sentences: ‘ideophone beanfield tritonymphal fatuism preambulate nonostentation overstrictly pachyhaemous’, ‘hyperapophysial’, ‘southern’, ‘episynaloephe subgenerically gleaning reformeress’, ‘trigonelline’, ‘commorant saltspoon’, ‘nonpopularity mammaliferous isobathythermal phenylglyoxylic insulate aortomalaxis desacralize spooky’, ‘speed garn nunciatory neologism’, ‘podobranchial fencible’, ‘epeirid gibaro’, ‘sleeved’, ‘demonographer probetting subduingly’, ‘velociously calpacked invaccinate acushla amixia unicolor’ and so on.&lt;/p&gt;
&lt;h2 id="testing"&gt;Testing
&lt;/h2&gt;&lt;p&gt;To test if our methods are doing well we are going to use the reconstruction rate metric. To calculate the metric, we need to collect &lt;a class="link" href="https://www.talkenglish.com/vocabulary/top-2000-vocabulary.aspx" target="_blank" rel="noopener"
&gt;2265 most common English words&lt;/a&gt; and expand this list with 10 digits. A random word gets converted to its embedding with the &lt;a class="link" href="https://arxiv.org/abs/1803.11175" target="_blank" rel="noopener"
&gt;universal sentence encoder&lt;/a&gt; and passed to the generator that generates the image representing the word. Then, depending on who is the test subject (human or machine) we use one of the following:&lt;/p&gt;
&lt;p&gt;For a machine, we pass generated images to the pre-trained visual network and then decode it with decoder network D to get back the word embedding. We compare this new embedding with all embeddings in the dictionary, and if the euclidian distance from the new embedding to the original embedding is the smallest, we give the point to the system. After checking all the words in such a way, we obtain:&lt;/p&gt;
&lt;p&gt;reconstruction rate = correct answers / number of words * 100%.&lt;/p&gt;
&lt;figure&gt;
&lt;div style="display: flex; justify-content: center; gap: 10px;"&gt;
&lt;img src="1_R_GQTs5EsJCZTU5SZP3HQg.png" style="width: 48%;"&gt;
&lt;img src="1_x59gDtlLr5RG36bl_HlpJw.png" style="width: 48%;"&gt;
&lt;/div&gt;
&lt;figcaption&gt;Presentation of Human Testing Interface&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;For human testing, we give a generated image to the test subject with a visual interface that presents the image with four possible answers. Human should learn through trial and error what word each image represents and report their best score. To make this task more convenient, at the beginning of the session, the testing interface asks what size of the dictionary the participant wants to use. The dictionary itself consists in a way that it’s growing from the most to less common English words. Volume of the dictionary is separated by the following levels: 10, 20, 40, 100, 200, 400, 1000, and 2275 (full dictionary). Here is a sample of the first 30 words presented in the dictionary:&lt;/p&gt;
&lt;p&gt;0, 1, 2, 3, 4, 5, 6, 7, 8, 9, the, of, and, to, a, in, is, you, are, for, that, or, it, as, be, on, your, with, can, have… and so on.&lt;/p&gt;
&lt;p&gt;In the human case, the reconstruction rate is calculated by the exact same formula. To check the accuracy, the test subject is supposed to answer 100 questions with full dictionary size. Although it is worth noting that the random baseline for humans is 25% as it is always possible to simply guess the right answer from the 4 options presented.&lt;/p&gt;
&lt;h2 id="results"&gt;Results
&lt;/h2&gt;&lt;p&gt;After training the system, it was producing the following images for respective input sentences:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_lgXj1-BGYkDVqLytrz9D9A.png"
width="1340"
height="1274"
srcset="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_lgXj1-BGYkDVqLytrz9D9A_hu_e6c6f08ff1eb2e9a.png 480w, https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_lgXj1-BGYkDVqLytrz9D9A_hu_d7b727979f31604d.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="105"
data-flex-basis="252px"
&gt;&lt;/p&gt;
&lt;p&gt;The reconstruction rate for the decoder network at the end of the training was 100%, and the loss value was 0.02.&lt;/p&gt;
&lt;p&gt;I did the experiment of learning this language on myself and was able to achieve a 43% reconstruction rate after two weeks of training. It was beneficial for me to start from a small number of words and gradually increase this number with less and less common words. The promise was, that at some point you do not really need to remember each image, the meaning of the word should be deducible from the geometry presented in the image. So learning such a language should be much easier than learning a typical natural language. While learning I used the 85% threshold to make sure that I’m ready to increase the volume of the dictionary.&lt;/p&gt;
&lt;p&gt;Here is how a two-week learning progress looked like for different dictionary sizes:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_aN_F1IG-q6nR7ECNjJRoyQ.png"
width="1059"
height="697"
srcset="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_aN_F1IG-q6nR7ECNjJRoyQ_hu_8c9399ef05ae7684.png 480w, https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_aN_F1IG-q6nR7ECNjJRoyQ_hu_204ad6b8ef0ccee0.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="151"
data-flex-basis="364px"
&gt;&lt;/p&gt;
&lt;p&gt;While bigger dictionaries became progressively harder to learn, the upward trend is clearly visible in all cases. This proves the most important point that answers the question that was set at the beginning. &lt;strong&gt;We can create a language that works in a continuous fashion and such language is learnable by a human.&lt;/strong&gt; While implications of this are yet to be known, in the next section, I will speculate on some that might be interesting to investigate further.&lt;/p&gt;
&lt;p&gt;The source code and pre-trained models are available on the project’s GitHub page: &lt;a class="link" href="https://github.com/volotat/Vector-Based-Language" target="_blank" rel="noopener"
&gt;https://github.com/volotat/Vector-Based-Language&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Full training history is available here: &lt;a class="link" href="https://github.com/volotat/Vector-Based-Language/blob/main/article/history.csv" target="_blank" rel="noopener"
&gt;https://github.com/volotat/Vector-Based-Language/blob/main/article/history.csv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="thoughts"&gt;Thoughts
&lt;/h2&gt;&lt;p&gt;To my knowledge, that is the only example of a synthetic language that possesses a notion of continuity where words and even sentences could blend into each other. I suggest that such language should be interesting even as an example that such a thing is possible in principle. I hope this may make someone fascinated by the idea to explore it further in their own right.&lt;/p&gt;
&lt;p&gt;I think that the most promising aspect of such language is the combination of it with some sort of brain-computer interface. The obvious downside of the language at the current stage is that there is no clear way to produce the image directly from the intent rather than words. With good enough BCI, this problem could be solved. Therefore such language might be the first way to store thoughts in human-readable format.&lt;/p&gt;
&lt;p&gt;With the rise of foundation models, and especially text2image generators, such as &lt;a class="link" href="https://github.com/CompVis/stable-diffusion" target="_blank" rel="noopener"
&gt;Stable Diffusion&lt;/a&gt;, there is a need to produce and store concepts that could not be easily conveyed with words but still could be represented as embeddings. There is recent work called &lt;a class="link" href="https://github.com/rinongal/textual_inversion" target="_blank" rel="noopener"
&gt;Textual Inversion&lt;/a&gt; that tries to solve this exact problem. With language like this, we could visualize these embeddings in a meaningful way and even use it as a common concept space for many models in the future.&lt;/p&gt;
&lt;p&gt;The language might be significantly improved to be more human-friendly. For example, we can use more advanced models such as CLIP (to be more specific, an image encoder part of it) as a visual model at the training stage to produce more readable images. Or we could use a set of predefined text-image pairs as anchors for more controllable outputs.&lt;/p&gt;
&lt;p&gt;And lastly, to really show that generated images represent the meaning of the words rather than the words themselves…&lt;/p&gt;
&lt;p&gt;&lt;img src="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_0yCretXbrfvCMniTT8T8LA.png"
width="1400"
height="284"
srcset="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_0yCretXbrfvCMniTT8T8LA_hu_61fbd085c439aa35.png 480w, https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/1_0yCretXbrfvCMniTT8T8LA_hu_1af17d4d56362676.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="492"
data-flex-basis="1183px"
&gt;&lt;/p&gt;</description></item></channel></rss>