<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects on Alexey Borsky</title><link>https://volotat.github.io/projects/</link><description>Recent content in Projects on Alexey Borsky</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 15 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://volotat.github.io/projects/index.xml" rel="self" type="application/rss+xml"/><item><title>4D Volumetric Retina Simulation</title><link>https://volotat.github.io/projects/2025-05-15-4d-volumetric-retina-simulation/</link><pubDate>Thu, 15 May 2025 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2025-05-15-4d-volumetric-retina-simulation/</guid><description>&lt;img src="https://volotat.github.io/projects/2025-05-15-4d-volumetric-retina-simulation/4D_eye_preview.png" alt="Featured image of post 4D Volumetric Retina Simulation" /&gt;&lt;p&gt;A physics-based 4D path tracing simulation visualizing how a four-dimensional being might perceive the world through a 3D volumetric retina.&lt;/p&gt;
&lt;p&gt;Current visualizations of 4D space often rely on wireframe projections or simple 3D cross-sections. This project takes a more biologically plausible approach: analogous to how we 3D beings perceive our world via 2D retinas, a 4D creature would likely possess a 3D (volumetric) retina.&lt;/p&gt;
&lt;p&gt;This simulation implements a custom 4D path tracing engine (using Python and Taichi for GPU acceleration) to model light interactions within a hyper-scene containing a rotating tesseract. It simulates image formation by casting 4D rays onto a defined 3D retinal volume.&lt;/p&gt;
&lt;p&gt;The simulation features physically-based rendering that models light bounces, shadows, and perspective in four spatial dimensions. Simulates a 3D sensor array rather than a flat plane. Implements a Gaussian fall-off for retinal sensitivity, mimicking foveal vision where the center of the 3D gaze is most acute. To make this comprehensible to human eyes, the 3D retinal image is composited from multiple depth slices, additively blended to represent the density of information a 4D being would process simultaneously.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/4DRender" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt; | &lt;a class="link" href="https://www.youtube.com/shorts/T697zlLvvHw" target="_blank" rel="noopener"
&gt;Video Showcase&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Anagnorisis</title><link>https://volotat.github.io/projects/2023-10-08-anagnorisis/</link><pubDate>Sun, 08 Oct 2023 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2023-10-08-anagnorisis/</guid><description>&lt;img src="https://volotat.github.io/projects/2023-10-08-anagnorisis/anagnorisis-screenshot%20from%202026-01-02.png" alt="Featured image of post Anagnorisis" /&gt;&lt;p&gt;Completely local data-management platform with built in trainable recommendation engine.&lt;/p&gt;
&lt;p&gt;The core idea is to create a self-hosted local-first media platform where you can rate your data, and the system trains a personal model to understand your preferences. This model then sorts your data based on your predicted interest, creating a personalized filter for any type of media you might have — images, music, videos, articles, and more.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Local First:&lt;/strong&gt; All data always stays on your device only. All models are trained and inferenced locally.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AI Powered:&lt;/strong&gt; Uses advanced embeddings (CLAP, SigLIP, Jina) to understand, search and filter your content and estimate preferences.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FullStack:&lt;/strong&gt; Built with Flask, Bulma, Transformers, and PyTorch. Uses simple Docker setup for easy deployment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open Source:&lt;/strong&gt; AGPL-3.0 license. Contributions, feedback and support are always welcome!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Active Development.&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/Anagnorisis" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>SD-CN-Animation</title><link>https://volotat.github.io/projects/2023-03-17-sd-cn-animation/</link><pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2023-03-17-sd-cn-animation/</guid><description>&lt;img src="https://volotat.github.io/projects/2023-03-17-sd-cn-animation/ui_preview.png" alt="Featured image of post SD-CN-Animation" /&gt;&lt;p&gt;This project was developed as an extension for the &lt;a class="link" href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" target="_blank" rel="noopener"
&gt;Automatic1111 web UI&lt;/a&gt; to automate video stylization and enable text-to-video generation using &lt;a class="link" href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5" target="_blank" rel="noopener"
&gt;Stable Diffusion 1.5&lt;/a&gt; backbones. At the time of development, generated videos often suffered from severe flickering and temporal inconsistency. This framework addressed those issues by integrating the &lt;a class="link" href="https://github.com/princeton-vl/RAFT" target="_blank" rel="noopener"
&gt;RAFT&lt;/a&gt; optical flow estimation algorithm. By calculating the motion flow between frames, the system could warp the previously generated frame to match the motion of the next one, creating a stable base for the diffusion model. This process, combined with occlusion masks, ensured that only new parts of the scene were generated while maintaining the consistency of existing objects.&lt;/p&gt;
&lt;p&gt;The tool supported both video-to-video stylization and experimental text-to-video generation. In video-to-video mode, users could apply &lt;a class="link" href="https://huggingface.co/lllyasviel/ControlNet" target="_blank" rel="noopener"
&gt;ControlNet&lt;/a&gt; to guide the structure of the output, allowing for stable transformations like turning a real-life video into a watercolor painting or digital art while preserving the original motion. The text-to-video mode employed a custom &amp;ldquo;FloweR&amp;rdquo; method to hallucinate optical flow from static noise, attempting to generate continuous motion from text prompts alone.&lt;/p&gt;
&lt;p&gt;Development on this project was eventually discontinued as the field rapidly advanced. The emergence of modern, end-to-end text-to-video models provided much more coherent and faithful results than could be achieved by hacking image-based diffusion models, rendering this approach largely obsolete for general use cases.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Not Maintained&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/SD-CN-Animation" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Vector Based Language</title><link>https://volotat.github.io/projects/2022-08-27-vector-based-language/</link><pubDate>Sat, 27 Aug 2022 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2022-08-27-vector-based-language/</guid><description>&lt;img src="https://volotat.github.io/projects/2022-08-27-vector-based-language/1_lgXj1-BGYkDVqLytrz9D9A.png" alt="Featured image of post Vector Based Language" /&gt;&lt;p&gt;Research project exploring the possibility of creating a continuous, visual language that hint at the possibility of direct understanding of the embedding spaces produced by ML models.&lt;/p&gt;
&lt;p&gt;Unlike traditional discrete languages, this system uses machine learning to generate unique visual representations (images) for any given text embedding. The goal is to create a language where concepts can blend into each other continuously, mirroring how neural networks process information.&lt;/p&gt;
&lt;p&gt;Words and sentences are represented as points in a continuous vector space, allowing for smooth transitions between concepts. Experiments show that humans can learn to interpret these generated visual embeddings with increasing accuracy over time. The visual language is designed to be perfectly reconstructible back into the original text embeddings by a decoder network crating a bridge between human cognition and the &amp;ldquo;black box&amp;rdquo; mechanized interpretation of the data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/" &gt;Read the Article&lt;/a&gt; | &lt;a class="link" href="https://github.com/volotat/Vector-Based-Language" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Generative Art Synthesizer</title><link>https://volotat.github.io/projects/2021-12-04-generative-art-synthesizer/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2021-12-04-generative-art-synthesizer/</guid><description>&lt;img src="https://volotat.github.io/projects/2021-12-04-generative-art-synthesizer/gas_preview.png" alt="Featured image of post Generative Art Synthesizer" /&gt;&lt;p&gt;A Python program that generates Python programs that generate generative art.&lt;/p&gt;
&lt;p&gt;Most generative art relies on stochastic processes where the initial seed and specific parameters are often lost, making exact reproduction difficult. GAS takes a different approach: instead of storing just the output or the parameters, it generates a fully deterministic, standalone Python script for each artwork. This ensures complete reproducibility—if you have the script, you have the art.&lt;/p&gt;
&lt;p&gt;The core mechanism involves initializing a tensor with coordinate data and then applying a random sequence of mathematical transformations (like &lt;code&gt;transit&lt;/code&gt;, &lt;code&gt;sin&lt;/code&gt;, &lt;code&gt;magnitude&lt;/code&gt;, &lt;code&gt;shift&lt;/code&gt;) to its channels. These operations are restricted to the [-1, 1] range to ensure stability. The final result is a composition of these channels converted into color space.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Self-Contained Art:&lt;/strong&gt; Each generated piece is a runnable Python script with zero external dependencies beyond standard scientific libraries (numpy, PIL).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deterministic:&lt;/strong&gt; The generated scripts contain no random elements; running the same script always produces the exact same image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Method-Based generation:&lt;/strong&gt; Uses a palette of composable mathematical functions (&lt;code&gt;sin&lt;/code&gt;, &lt;code&gt;prod&lt;/code&gt;, &lt;code&gt;soft_min&lt;/code&gt;, etc.) to &amp;ldquo;sculpt&amp;rdquo; the image in a high-dimensional channel space.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Aesthetic Scoring:&lt;/strong&gt; Includes a simple scoring model to estimate the visual quality of generated outputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/GAS" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>The ARC Game</title><link>https://volotat.github.io/projects/2021-07-26-the-arc-game/</link><pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2021-07-26-the-arc-game/</guid><description>&lt;img src="https://volotat.github.io/projects/2021-07-26-the-arc-game/level_example.jpg" alt="Featured image of post The ARC Game" /&gt;&lt;p&gt;The Abstraction and Reasoning Corpus made into a web game.&lt;/p&gt;
&lt;p&gt;The aim of this project is to create an easy-to-use interface for François Chollet&amp;rsquo;s &lt;a class="link" href="https://github.com/fchollet/ARC-AGI" target="_blank" rel="noopener"
&gt;Abstraction and Reasoning Corpus (ARC-AGI)&lt;/a&gt;, designed so that children as young as three years old can play with it. This tool explores the potential of using ARC as educational material for developing abstraction and reasoning skills in young kids, challenging cognitive abilities such as pattern recognition, logical reasoning, and problem-solving.&lt;/p&gt;
&lt;p&gt;The game presents visual tasks consisting of grid pairs that represent a transformation (input → output). The player must deduce the transformation rule from the examples and apply it to a test grid.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Child-Friendly Interface:&lt;/strong&gt; Simplified controls allow for painting, dragging to fill, and copying grids, making it accessible for early childhood development.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fixed Grid Sizes:&lt;/strong&gt; Unlike the original ARC where output size is part of the solution, here the output grid size is pre-set to reduce complexity and focus on the transformation logic.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Printable Version:&lt;/strong&gt; Tasks can be automatically formatted and printed on paper with adjusted colors (e.g., swapping black for white) for offline solving with markers or pencils.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Not Maintained.&lt;/p&gt;
&lt;h3 id="cited-in-scientific-literature"&gt;Cited in Scientific Literature
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="link" href="https://pub.tik.ee.ethz.ch/students/2021-HS/BA-2021-38.pdf" target="_blank" rel="noopener"
&gt;Abstraction and Reasoning Challenge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://arxiv.org/abs/2403.11793" target="_blank" rel="noopener"
&gt;Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://arxiv.org/pdf/2410.07866" target="_blank" rel="noopener"
&gt;System 2 Reasoning for Human-AI Alignment: Generality and Adaptivity via ARC-AG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://dl.acm.org/doi/abs/10.1145/3711896.3736831" target="_blank" rel="noopener"
&gt;Addressing and Visualizing Misalignments in Human Task-Solving Trajectories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class="link" href="https://volotat.github.io/ARC-Game/" target="_blank" rel="noopener"
&gt;Play the Game&lt;/a&gt; | &lt;a class="link" href="https://github.com/volotat/ARC-Game" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Differentiable Morphing</title><link>https://volotat.github.io/projects/2021-01-02-differentiable-morphing/</link><pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2021-01-02-differentiable-morphing/</guid><description>&lt;img src="https://volotat.github.io/projects/2021-01-02-differentiable-morphing/formula.jpg" alt="Featured image of post Differentiable Morphing" /&gt;&lt;p&gt;Image morphing without reference points by optimizing warp maps via gradient descent.&lt;/p&gt;
&lt;p&gt;This project introduces a &amp;ldquo;differentiable morphing&amp;rdquo; algorithm that can smoothly transition between any two images without requiring manual reference points or landmarks. Unlike traditional generative models that learn a distribution from a dataset, this approach uses a neural network as a temporary functional mapping to solve a specific optimization problem for a single pair of images.&lt;/p&gt;
&lt;p&gt;The algorithm finds a set of maps that transform the source image into the target image.&lt;/p&gt;
&lt;!--The algorithm finds a set of maps that transform the source image ($A$) into the target image ($B$) using the following logic:
$$ B = (A \times \text{mult\_map} + \text{add\_map}) \circ \text{warp\_map} $$
* **Mult Map:** Removes unnecessary parts and adjusts localized color balance.
* **Add Map:** Introduces new colors not present in the original image.
* **Warp Map:** Distorts the image geometry to handle rotation, scaling, and movement of objects.--&gt;
&lt;p&gt;By interpolating the strength of these maps, the system produces a smooth, seamless animation where features transform fluidly from one state to another.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;h3 id="cited-in-scientific-literature"&gt;Cited in Scientific Literature
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="link" href="https://www.sciencedirect.com/science/article/pii/S0167739X22000838" target="_blank" rel="noopener"
&gt;The explainability paradox: Challenges for xAI in digital pathology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://arxiv.org/abs/2311.06792" target="_blank" rel="noopener"
&gt;IMPUS: Image Morphing with Perceptually-Uniform Sampling Using Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://arxiv.org/abs/2507.01953" target="_blank" rel="noopener"
&gt;FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/DiffMorph" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>ConGAN</title><link>https://volotat.github.io/projects/2018-01-29-congan/</link><pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2018-01-29-congan/</guid><description>&lt;img src="https://volotat.github.io/projects/2018-01-29-congan/faces_table.png" alt="Featured image of post ConGAN" /&gt;&lt;p&gt;Continuous Adversarial Image Generator that can produce high-quality images with relatively few examples and resolution independence.&lt;/p&gt;
&lt;p&gt;This project implements a Generative Adversarial Network (GAN) that learns a continuous function mapping coordinates (x, y) and a latent vector z to an RGB color, rather than generating a fixed grid of pixels. By treating images as continuous functions, the model creates a &amp;ldquo;vector-like&amp;rdquo; representation that can be sampled at any resolution.&lt;/p&gt;
&lt;p&gt;The architecture differs from traditional GANs by using a coordinate-based generator. The system includes an &amp;ldquo;Ident&amp;rdquo; network to map images to a latent space, a Generator that predicts colors based on position and latent codes, and a Discriminator that validates pixel data in context.&lt;/p&gt;
&lt;p&gt;Results demonstrated that the model could generalize well from small datasets (10-20 images is enough for reasonable results, while training the network completely from the scratch) and produce smooth interpolations in the latent space, effectively hallucinating plausible variations between training examples.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/ConGAN" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Low Dimensional Autoencoder</title><link>https://volotat.github.io/projects/2017-11-18-low-dimensional-autoencoder/</link><pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate><guid>https://volotat.github.io/projects/2017-11-18-low-dimensional-autoencoder/</guid><description>&lt;img src="https://volotat.github.io/projects/2017-11-18-low-dimensional-autoencoder/latent_space.png" alt="Featured image of post Low Dimensional Autoencoder" /&gt;&lt;p&gt;An experimental approach to training autoencoders for meaningful low-dimensional data representation.&lt;/p&gt;
&lt;p&gt;This project was one of my first experiments in machine learning, serving as a deep dive into how autoencoders process information. The goal was to address the difficulty of encoding high-dimensional data (like images) into very low-dimensional latent spaces (like 2D) without the model getting stuck in local minima. Usually it takes at least 16 or more dimensions for bottleneck embeddings to get decent results, but I wanted to see if it was possible to achieve this in just 2 dimensions.&lt;/p&gt;
&lt;p&gt;The method involves training the autoencoder in a typical fashion, alongside with the encoder that trains on a dynamic set of target representations. It works by initially setting random points for each datapoint and then, at each step, stretching the latent space to touch the predefined space boundaries while applying a repulsion force to keep datapoints distinct. This prevents points from clustering too densely and encourages a more uniform distribution.&lt;/p&gt;
&lt;p&gt;The result, as seen in the project image, is a continuous 2D embedding space that successfully compacts the entire MNIST dataset, allowing for smooth transitions between different digits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Completed Experiment&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/volotat/Low-Dimensional-Autoencoder" target="_blank" rel="noopener"
&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;</description></item></channel></rss>