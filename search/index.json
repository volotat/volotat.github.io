[{"content":"Introduction What if I told you that a vast amount of the computation that goes into training every new Large Language Model is completely redundant?\nThis isn’t just a minor inefficiency, it’s a fundamental flaw in our current approach. We are spending billions of dollars and burning gigawatts of energy for every major AI company to teach their models the same fundamental understanding of the world, over and over again.\nHere is the claim I am going to justify: the hardest part of what all LLMs learn is NOT next-token prediction, but rather the arbitrary data compression into a meaningful vector representation. And here’s the catch — this representation is fundamentally the same for every model (up to symmetries and precision).\nThe work that inspires this claim is the Platonic Representation Hypothesis. This hypothesis suggests that for any given data, there exists a “perfect” or ideal mathematical representation — a Platonic form, if you will. All our current models are simply trying to find their own noisy, imperfect approximation of this one true representation.\nHow a “Perfect” LLM Should Look If the Platonic Representation Hypothesis holds true, it means we’re building our models upside down. We train monolithic, end-to-end models that learn to embed data and reason about it at the same time. Instead, we should separate these concerns.\nThe future of efficient AI should be built on a simple, two-part architecture:\nThe Universal Encoder: A single, global, state-of-the-art model that does only one thing: convert any piece of data (text, images, sound, etc., in a continuous sequence) into its perfect “Platonic” vector embedding. This model would be trained once on a colossal dataset and then frozen, serving as a foundational piece of public infrastructure. The Task Model: A much smaller, specialized model that takes these “perfect” embedding (that represents all the current context window that goes to the model) as an input and learns to perform a specific task. This could be next-token prediction, classification, image denoising (for diffusion models), or even complex, one-shot reasoning like question answering or code generation. All personalization, alignment, and RLHF would happen at this much cheaper, more efficient level. Think of it like this: today, every AI student has to first invent an entire language from scratch — its alphabet, vocabulary, and grammar — before they can even begin to learn to write an essay. In the new paradigm, they all share a common, universal language (the Universal Embeddings) with all the world’s concepts already encoded within it. The students (the Task Models) can then focus immediately on the creative act of writing the essay. Training these task-specific models would be orders of magnitude cheaper and faster.\nUniversal, Relational Embedding Space So, how do we find these universal embeddings and represent them in a way that is stable and suitable for any task?\nRight now, embeddings are encoded as a point in a very high-dimensional space. The problem is that the coordinate system of this space is set arbitrarily during the training process. This means every model has its own unique, incompatible space. Training the same model twice will generate two completely different spaces. You can’t just take an embedding from GPT-4 and use it in Gemini; their internal “languages” are different.\nRecent research on the Universal Geometry of Embeddings shows that while the coordinate systems are different, the underlying geometric relationships between the embeddings are remarkably similar. We can remap one model’s space onto another. But this is a patch, not a solution. It proves the point: what truly matters isn’t the absolute coordinates of a concept, but its relationships to all other concepts.\nWhat we truly care about are the distances.\nInstead of a vector representing a point in space, what if an embedding vector represented a list of distances?\nImagine trying to describe the location of your city. You could give its absolute GPS coordinates (e.g., 46.10° N, 19.66° E). This is the current approach — precise, but meaningless without the entire GPS coordinate system.\nAlternatively, you could describe it relationally: “It’s 180km from Budapest, 190km from Belgrade, 560km from Vienna…” This is a distance-based representation. It’s inherently meaningful.\nThis is how our Universal Embedding should work. Each vector would not be a coordinate, but a list of distances to a set of universal, canonical “landmark concepts” in the embedding space.\nembedding(\u0026quot;cat\u0026quot;) = [dist_to_concept_1, dist_to_concept_2, dist_to_concept_3, ...]\nWe don’t even need to know what these landmark concepts are explicitly. They can be learned by the model. The key is that they are ordered by importance, from the most general to the most specific. The first value might represent the distance to the abstract concept of “objectness,” the second to “living thing,” the tenth to “mammal,” and so on.\nThis structure has two incredible properties:\nIt’s Stable: Because it’s based on internal distances, it’s invariant to rotation or translation. Every training run would converge to the same relational representation, creating a stable and universal standard. It’s Inherently Composable: This is very similar to the idea of Matryoshka Representation Learning. If you need a smaller, less precise embedding, you can just take the first N values of the vector! You’re simply using the distances to the most important landmark concepts, giving you a coarse but still highly effective representation. We Need THE Embedding Model The centerpiece of this vision is the Universal Encoder, or THE Embedding Model. This wouldn’t be just another model like text-embedding-3-large; it would be a foundational piece of infrastructure for the entire AI ecosystem, akin to the GPS network or the TCP/IP protocol.\nThis model, trained on a dataset far beyond the scale of any single company, would create this true distance-based, Matryoshka-style embedding space. This would be the definitive, canonical representation of knowledge.\nThe Benefits Would Be Transformative Backward Compatibility and Continuous Improvement: New versions of the Universal Encoder would be released as research progresses. Since each new version is just a better approximation of the same underlying Platonic representation, they should be largely backward compatible. This means you could swap in the new encoder and expect any pre-trained, task-specific models to work with the same or even better performance, with minimal re-training. Simplified RAG and Vector Search: Retrieval-Augmented Generation (RAG) and all vector search applications would be greatly simplified. You would only need a single base embedding model for any type of data. Your text, image, and audio databases would all exist within the same unified, coherent vector space, making cross-modal search and reasoning trivial. Democratization of AI: The colossal cost of training foundational models from scratch would be a one-time, collaborative effort. Researchers, startups, and even individuals could then build powerful, specialized AI applications by training only the small, inexpensive task models on top of the universal embeddings. A Call for a New Direction I truly believe this is the future we must build. With this approach, AI could become far more open, accessible, and environmentally sustainable.\nHowever, this vision is a direct threat to the power held by the big companies that currently dominate the AI space. Their moat is the sheer scale of their proprietary, monolithically trained models. A shared, universal encoder is not in their immediate financial interest.\nTherefore, this message is a call to action for the global ML community and AI enthusiasts. The creation of such a foundational model will likely not come from a single corporation, but from a decentralized, open-source effort. Only as a global community do we have a shot at creating an AI future that is more efficient, decentralized, and accessible for everyone.\n","date":"2025-08-21T00:00:00Z","image":"https://volotat.github.io/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/0_4fPJHVhCQeYNef_n_hu_69999976cbe8eeb3.png","permalink":"https://volotat.github.io/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/","title":"There is a way to make training LLMs way cheaper and more accessible."},{"content":"A physics-based 4D path tracing simulation visualizing how a four-dimensional being might perceive the world through a 3D volumetric retina.\nCurrent visualizations of 4D space often rely on wireframe projections or simple 3D cross-sections. This project takes a more biologically plausible approach: analogous to how we 3D beings perceive our world via 2D retinas, a 4D creature would likely possess a 3D (volumetric) retina.\nThis simulation implements a custom 4D path tracing engine (using Python and Taichi for GPU acceleration) to model light interactions within a hyper-scene containing a rotating tesseract. It simulates image formation by casting 4D rays onto a defined 3D retinal volume.\nThe simulation features physically-based rendering that models light bounces, shadows, and perspective in four spatial dimensions. Simulates a 3D sensor array rather than a flat plane. Implements a Gaussian fall-off for retinal sensitivity, mimicking foveal vision where the center of the 3D gaze is most acute. To make this comprehensible to human eyes, the 3D retinal image is composited from multiple depth slices, additively blended to represent the density of information a 4D being would process simultaneously.\nStatus: Completed Experiment\nView on GitHub | Video Showcase\n","date":"2025-05-15T00:00:00Z","image":"https://volotat.github.io/projects/2025-05-15-4d-volumetric-retina-simulation/4D_eye_preview_hu_d02d548b443ba210.png","permalink":"https://volotat.github.io/projects/2025-05-15-4d-volumetric-retina-simulation/","title":"4D Volumetric Retina Simulation"},{"content":"Introduction There’s a shift happening in the world of programming right now, at this very moment. It feels deeper and more fundamental than anything that happened before. It’s a change in the very nature of how we create software, driven by the rapid development of Large Language Models (LLMs) in code generation. And as someone who has loved the craft of programming — the intricate logic, the elegant structures, the deep understanding required — I feel we’re standing nearby a dramatic transition, looking out at a future both majestic and horrifying.\nThe programming I knew, the kind that demanded you hold complex systems within your mind, is changing. Tools like Copilot and its increasingly sophisticated successors are becoming ubiquitous. More and more, our role is shifting. We are less the meticulous architects drawing every line, and more like pilots or shepherds, guiding powerful LLM agents, trying to keep them on track, nudging them away from breaking things too severely.\nFrom Architect to AI Whisperer Think about the traditional process: absorbing requirements, designing data structures and algorithms, carefully crafting functions and classes, debugging subtle interactions and trying to find the time for so bloody needed refactoring. There was an inherent incentive, almost a necessity, to maintain a holistic understanding of the project’s structure. Your mind was the primary repository for all of that.\nNow, that incentive is fading. Why spend hours meticulously mapping out a complex module when an LLM can generate a plausible version in seconds? The focus shifts from deep construction to high-level direction and validation. We feed the AI prompts, review the output, iterate, and integrate. It’s undeniably faster for many tasks, capable of producing boilerplate or even complex algorithms we might have struggled with. But with this speed comes a subtle erosion of that deep, internalised understanding. We are becoming experts at driving the AI, but perhaps less expert in the underlying terrain it traverses.\nThe Seductive Path of Trust and Obscurity Today, LLM-generated code often requires careful scrutiny. It fails in subtle ways, introduces security flaws, or misunderstands complex requirements. We are still the necessary gatekeepers, the human-in-the-loop ensuring quality.\nBut the trajectory is clear. LLMs are improving at an astonishing rate. With each iteration, they become more capable, their failures less frequent, their outputs more robust. As this happens, our trust inevitably grows. We’ll spend less time reviewing, more time accepting the generated code. The “good enough” becomes “surprisingly good,” and eventually, perhaps, “consistently better than I could do alone in the same timeframe”.\nThis increasing reliance is a one-way street. The more we trust the AI, the less comprehensively we understand the systems being built. Project complexity can balloon, supported by AI-generated scaffolding that no single human fully grasps. It will simply work, most of the time.\nThe Linguistic Singularity This leads to the most radical and unsettling part of this transformation. Once AI can generate robust, well-behaved code almost perfectly, the constraints of human-readable programming languages becomes a liability.\nWhy force an AI, capable of processing information in ways we can barely imagine, to express its logic in syntaxes designed for human cognition (like Python, Java, or Rust)? The next logical step is for programming languages — or whatever replaces them — to adapt towards the AI.\nImagine languages optimised not for human eyes, but for maximum efficiency in AI generation, verification, and execution. They might look like incomprehensible “gibberish” to us — dense, symbolic, perhaps multi-dimensional data structures rather than linear text. These new forms of “code” would be incredibly efficient, highly optimised, inherently less prone to certain types of errors, and performant beyond our current benchmarks.\nBut they would be utterly alien. We would quickly find ourselves in a situation where comprehending the generated code, even if we desperately wanted to, becomes impractical, then difficult, and finally, fundamentally impossible. The very concept of “reading the source” could become meaningless.\nBeyond Code Itself: The AI Mind Simulation Taking this speculation a step further, perhaps the notion of a distinct “programming language” itself dissolves. Future AI systems might not need to explicitly generate code in any intermediate format. They could potentially simulate the desired system directly within their own complex internal states, manifesting the results directly as functional behaviour or user interfaces — the “pretty graphics” we interact with.\nThere would be no source code to inspect, no language to learn. Just an input (our request) and an output (the working software or system), with an impenetrable, hyper-complex process happening within the AI’s “mind.” We would interact with technology whose inner workings are not just unknown, but potentially unknowable by the human intellect.\nNot Our Future There’s a terrifying grandeur to this potential future. Imagine complex global systems — logistics, scientific research, resource management — running with near-perfect efficiency, orchestrated by AI far exceeding human capabilities. Problems currently intractable could be solved. This is the majestic part.\nThe horror lies in the loss of understanding, control, and agency. We become entirely dependent on systems we cannot interrogate, debug, or truly direct. What happens when these perfectly optimised, incomprehensible systems exhibit emergent behaviour we didn’t anticipate? Who fixes the unreadable “gibberish” when the AI fails in a novel way? Does the craft of programming, as a human endeavour of creation and understanding, simply cease to exist?\nI don’t have the answers. But I feel this transition viscerally, right now. The way I write code, think about systems, and even identify as a programmer is evolving under the immense gravity of AI. It’s exciting, promising, and deeply unsettling all at once. The programming I knew and loved is changing, fundamentally and irrevocably. There’s likely no going back.\nIt seems, we are the last generation that have seen a miracle of human-readable code.\n","date":"2025-04-25T00:00:00Z","image":"https://volotat.github.io/p/the-sunset-of-human-readable-code/0_WnVe9xDIVdAL4WG4_hu_46ba1d9c3b634ed1.png","permalink":"https://volotat.github.io/p/the-sunset-of-human-readable-code/","title":"The Sunset of Human-Readable Code."},{"content":"Introduction In today\u0026rsquo;s digital age, an increasing amount of personal data is stored on cloud services, often beyond our direct control. While these platforms offer convenience, they also pose significant risks regarding data ownership, privacy, and longevity. Recent incidents, such as the cyberattacks on the Internet Archive, highlight the vulnerability of digital repositories. In October 2024, the Internet Archive suffered a data breach that exposed information from over 31 million user accounts. This breach was accompanied by DDoS attacks, leading to intermittent service disruptions. Furthermore in March 2023, a U.S. District Court ruled that the Internet Archive\u0026rsquo;s practice of lending digitized books online constituted copyright infringement, leading to a permanent injunction against such activities. These events underscore how external factors can jeopardize access to information stored online.\nMoreover, data loss from cloud services is not uncommon. Service outages, cyberattacks, and policy changes can result in the sudden unavailability of personal data. For instance, breaches targeting cloud storage providers have exposed sensitive user information, emphasizing the precarious nature of entrusting data to third-party platforms.\nAn external hard drive could provide up to 32TB of storage as of january 2025\nYour Data as a Reflection of Your Personality Every digital interaction we engage in - be it social media activity, spending habits, or smartphone usage - leaves behind a footprint that mirrors aspects of our personality. Similarly, patterns in smartphone usage have been linked to the Big Five personality traits, providing insights into an individual\u0026rsquo;s behavior and preferences. These digital traces are not just passive records, they actively contribute to the construction of our digital personas. Machine learning algorithms can analyze this data to predict personality traits, highlighting the intimate connection between our data and our identities.\nThe Case for Local Storage To mitigate these risks, transitioning to local storage solutions is a prudent strategy. By maintaining data on personal devices or dedicated local servers, individuals retain full control over their information, ensuring accessibility regardless of external circumstances. This approach not only safeguards against unforeseen disruptions but also enhances privacy and data security.\nRecognizing the need for accessible local storage management, the Anagnorisis project is developing as a way to assist users in organizing and preserving their data effectively. The idea behind Anagnorisis is to offer a set of tools that facilitate migration from cloud services such as Notion, Spotify, Pinterest or Youtube to local environments, allowing users to easily find, rate and enjoy their data just as effectively as with any of these services. By providing a user-friendly interface and AI-powered features, Anagnorisis ensures that personal data remains under the user\u0026rsquo;s control, protected from external threats and accessible for future generations.\nDigital Immortality Through Data Preservation While preserving the human mind in its entirety remains beyond current technological capabilities, each piece of data we create serves as a fragment of our identity. By conscientiously preserving this data, we offer future generations the opportunity to reconstruct a digital approximation of our personalities. This form of digital immortality is attainable today, allowing us to leave behind a legacy that extends beyond our physical existence. The ghost of our past self that can support our descendants way beyond our physical death.\nMoreover, by engaging in this conscious attempt to preserve our personality, we may begin to treat our data differently. Instead of hoarding vast amounts of trivial, mostly situational information, that happened to be stored on our SSD and hard drives, we might prioritize storing only the most important and meaningful parts of it. This approach encourages us to reflect on the value of our contributions and fosters a more intentional interaction with our digital environment. Writing down our thoughts more frequently - perhaps not for others to read but as a way to create more data points. These records can serve as the basis for training future machine learning models, creating digital constructs that closely resemble our true selves.\nIn the end While cloud services offer undeniable convenience, the potential risks associated with relinquishing control over personal data cannot be overlooked. Embracing local storage solutions, supported by tools like Anagnorisis, provides a secure and reliable alternative, preserving the integrity and accessibility of our digital lives.\nGithub: https://github.com/volotat/Anagnorisis\n","date":"2025-01-23T00:00:00Z","image":"https://volotat.github.io/p/anagnorisis-part-3-why-should-you-go-local/External-Hard-Drive-Hero_hu_e8962455af4efeec.png","permalink":"https://volotat.github.io/p/anagnorisis-part-3-why-should-you-go-local/","title":"Anagnorisis. Part 3: Why Should You Go Local?"},{"content":"Introduction One of the main ideas of Anagnorisis is to have completely open recommendation engine available for users to not only view but also modify as they wish. When a user uses the music player integrated into the project, they have the ability to influence how often songs are played by rating them and based on those ratings training personal model that can estimate the ratings of the user.\nIn particular the mechanism looks like this:\nUser sets up the path to local music folder Rates some of the songs according to their own preferences After gathering some data goes to “Train” page and press “Train music evaluator” to train the preference model After the training is complete one can enjoy their music, but this time, when the music is selected it will be rated by the model (in case it was not rated by the user already) and therefore adjust the probability of it occurring in the playlist.\nTo calculate the actual probability, the music recommendation system combines various factors — user ratings, model predictions, play history, and skips, as well as time since the last play of the song — into a mathematical formula that determines the probability of each song being selected. By balancing these influences, the system provides a personalized and dynamic music recommendation experience. Below is a more detailed explanation of the math behind the algorithm.\nMusic recommendation system Music recommendations are based on the two scores: user scores and internal score. While the user score is static, the internal score is dependent on the act of listening. The initial internal score is always 0, it gets higher if the user listens to the whole song and gets lower if the user skips it.\nCalculating probabilies for song selection User Ratings and Missing Data The starting point is user ratings. If a song has a rating, it uses that; if not, it uses a fallback mechanism.\nUser rating: $ R_u $ Model rating: $ R_m $ Mean user rating: $ R_{\\text{mean}} $ For each song, the system checks whether a user or model rating exists. If neither is present, the system assumes the rating is the mean value of all user ratings.\n$$ R = \\begin{cases} R_u \u0026 \\text{if } R_u \\text{ exists} \\\\ R_m \u0026 \\text{if } R_u \\text{ does not exist and } R_m \\text{ exists} \\\\ R_{\\text{mean}} \u0026 \\text{if neither } R_u \\text{ nor } R_m \\text{ exists} \\end{cases} $$This ensures every song gets a score.\nAdjusting and Normalizing Scores Once the ratings are gathered, they need to be adjusted for fairness. The system normalizes them and adds some weight to higher-rated songs. The normalization process is:\nEnsure no score is zero by adding a small constant (0.1) to the score:\n$ R' = \\max(0.1, R) $\nAmplify high ratings by normalizing the scores and then squaring them to give higher-rated songs more weight:\n$ R_{\\text{adjusted}} = \\left( \\frac{R'}{10} \\right)^2 $\nThis squaring makes songs with higher ratings much more likely to be chosen, while ensuring lower-rated songs still have a small chance.\nSkip and Play History The next adjustment comes from considering how often a song has been played versus skipped:\nFull plays: $ P_f $ Skips: $ P_s $ The skip score is calculated using the difference between full plays and skips, normalized by an empirically chosen factor of 5.\n$ S_{\\text{skip}} = \\sigma \\left( \\frac{5 + P_f - P_s}{5} \\right) $\nwhere σ(x) is the sigmoid function:\n$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $\nThis ensures that the skip score stays in a meaningful range between 0 and 1.\nTime Since Last Played To promote variety, the system boosts the chances for songs that haven’t been played recently. The system calculates a last played score based on how recently the song was played compared to other songs.\nIndex of song in the list sorted by last played: $ I_{\\text{last}} $ Total number of songs: $ N $ The last played score is calculated as:\n$ S_{\\text{time}} = \\frac{I_{\\text{last}}}{N} $\nSongs that haven’t been played in a long time get a higher score, while recently played songs get a lower score.\nFinal Score Calculation Now that we have all the factors:\nAdjusted rating: $ R_{\\text{adjusted}} $ Skip score: $ S_{\\text{skip}} $ Last played score: $ S_{\\text{time}} $ The final score for each song is a product of these three components:\n$ S_{\\text{final}} = R_{\\text{adjusted}} \\times S_{\\text{skip}} \\times S_{\\text{time}} $\nThis final score \\( S_{\\text{final}} \\) represents how likely a song is to be selected.\nCalculating Probabilities Once the final scores are calculated for all songs, they are converted into probabilities. The probability of a song $ i $ being selected is its score divided by the sum of all scores:\n$ P_i = \\frac{S_{\\text{final}, i}}{\\sum_{j=1}^{N} S_{\\text{final}, j}} $\nThis ensures that the probabilities sum to 1, and each song’s chance of being selected is proportional to its score.\nRandom Selection Finally, the system uses these probabilities to randomly select a song. A song with a higher probability $ P_i $ is more likely to be chosen, but even songs with lower probabilities still have a chance based on their final score.\nExample Calculation Let’s say we have three songs with the following details:\nSong User Rating Full Plays Skips Last Played (days ago) A 8 10 2 7 B 6 5 3 2 C None 2 5 30 Adjusted Ratings:\nSong A: $ R_{\\text{adjusted}} = \\left( \\frac{8}{10} \\right)^2 = 0.64 $ Song B: $ R_{\\text{adjusted}} = \\left( \\frac{6}{10} \\right)^2 = 0.36 $ Song C uses the mean rating (say 7): $ R_{\\text{adjusted}} = \\left( \\frac{7}{10} \\right)^2 = 0.49 $ Skip Scores:\nSong A: $ S_{\\text{skip}} = \\sigma \\left( \\frac{5 + 10 - 2}{5} \\right) = \\sigma(2.6) \\approx 0.93 $ Song B: $ S_{\\text{skip}} = \\sigma \\left( \\frac{5 + 5 - 3}{5} \\right) = \\sigma(1.4) \\approx 0.80 $ Song C: $ S_{\\text{skip}} = \\sigma \\left( \\frac{5 + 2 - 5}{5} \\right) = \\sigma(0.4) \\approx 0.60 $ Last Played Scores (let’s assume there are 3 songs total):\nSong A (played 7 days ago): $ S_{\\text{time}} = \\frac{2}{3} \\approx 0.67 $ Song B (played 2 days ago): $ S_{\\text{time}} = \\frac{1}{3} \\approx 0.33 $ Song C (played 30 days ago): $ S_{\\text{time}} = \\frac{3}{3} = 1.0 $ Final Scores:\nSong A: $ S_{\\text{final}} = 0.64 \\times 0.93 \\times 0.67 \\approx 0.40 $ Song B: $ S_{\\text{final}} = 0.36 \\times 0.80 \\times 0.33 \\approx 0.095 $ Song C: $ S_{\\text{final}} = 0.49 \\times 0.60 \\times 1.0 \\approx 0.29 $ Probabilities:\n$ P_A = \\frac{0.40}{0.40 + 0.095 + 0.29} \\approx 0.50 $\n$ P_B = \\frac{0.095}{0.40 + 0.095 + 0.29} \\approx 0.12 $\n$ P_C = \\frac{0.29}{0.40 + 0.095 + 0.29} \\approx 0.36 $\nIn this case, Song A has a 50% chance of being selected, Song B has a 12% chance, and Song C has a 36% chance.\nFuture plans In future I would also like to add more different options of building a current playlist. One of the ideas is a “chain mode” that takes selected by the user song as a seed and finds the most similar song, based on the embeddings, then finds the most similar to a next song and so on.\nIn case there would be a CLIP-like model for audio in the future, I would also like to implement prompt-based playlist generation. One could simply describe the mood of the music they would like to listen and the proper playlist be generated on the fly from this description.\nGithub: https://github.com/volotat/Anagnorisis\n","date":"2024-09-22T00:00:00Z","image":"https://volotat.github.io/p/anagnorisis-part-2-the-music-recommendation-algorithm/AI_hu_a9a08695f260b374.jpg","permalink":"https://volotat.github.io/p/anagnorisis-part-2-the-music-recommendation-algorithm/","title":"Anagnorisis. Part 2: The Music Recommendation Algorithm."},{"content":"Introduction Many years ago there was a music streaming service called Grooveshark. While being one of the first of such services it allowed things that would be unheard of today — free access to a huge music library and an option for any user to upload their own music to the website. This created quite a unique environment where you can easily find even the most obscure music you like to listen to. Furthermore there was a \u0026ldquo;radio mode\u0026rdquo; that automatically selected the music that you would like to listen, basically the same as any other today\u0026rsquo;s streaming service with one little exception — it was actually good. I was able to listen to music for hours and almost always it was the music I like that allowed me to discover many new bands that I like till this day. Unfortunately, on April 30, 2015 the site was shut down as part of a settlement of the copyright infringement lawsuits between the service and Universal Music Group, Sony Music Entertainment, and Warner Music Group.\nSince then I have tried many other alternatives such as spotify, pandora, youtube music and others. None of them ever worked for me as well as Grooveshark. First of all for a long time many of them simply were not working for my country of residence, but even when they start working there was a problem — recommendations worked really badly. Mostly the recommendations were good only for the first few songs, but after that it always started to diverge into \u0026ldquo;popular\u0026rdquo; or rather \u0026ldquo;promoted\u0026rdquo; music that I don\u0026rsquo;t really like. And yes, it did not matter if it was paid or free version. The suspicion started to grow, that many services use music recommendation engines as subtle manipulation to promote some particular artists rather than satisfying my needs as a customer.\nThis led me into rethinking the whole idea of recommendation algorithms and how much of it is what we want to see, and how much of it is what the people in control of it want or don\u0026rsquo;t want to show.\nControlling the Algorithm. Not the other way around. There are several different ways of how we can sort information. Some popular social media, such as Reddit heavily rely on direct human feedback for it. While this approach is clearly working it also requires a lot of moderation efforts. And because of its nature, posts on reddit get more attention when they satisfy the interest of the whole community rather than a particular person.\nAnother widespread approach that recommendation systems use is Collaborative filtering. It uses personal feedback from some users to predict preferences of other users. Websites such as Spotify or Youtube in its core heavily rely on this approach or at least relied in the past. The main drawback of this approach that you do need a lot of users and their personal data to make it work and even then there are no guarantee that it will always work for all users as they might have some set of interests that were not seen previously.\nFinally with the rise of Machine Learning a new approach has risen — Embedding-based recommender systems. While this is an umbrella term that implies many different techniques, the general idea behind it is to use embeddings generated by some ML model directly from the data to then predict how well this data is suited for a particular user. This is the only approach that could work without a huge user base, using only a single user feedback. As such, it opens a new exciting possibility — a recommendation system that works completely locally on users\u0026rsquo; personal devices. If implemented as an open source project such a recommendation system would be completely open to the user and controlled only by the user.\nThe Next Frontier Let\u0026rsquo;s spend a minute trying to imagine how a \u0026ldquo;perfect\u0026rdquo; recommendation system might work. For a brief moment imagine that our system is totally agnostic to what kind of data it can process. No matter if it is music, videos, news or anything else. To extract embeddings of the data we will use some ML model that could take any data as an input and produce a meaningful embedding of it.\nHere is some basic principle that I would like this system to satisfy:\nFirst of all, it should give valuable recommendations. Obviously. It should allow to sort and filter the vast amount of information. All the data about the user and about the user\u0026rsquo;s preferences should stay on their own personal devices and should be accessible only by the users. It should learn from the user\u0026rsquo;s feedback and change its recommendations alongside changing interests of the user. We can use our ML model to generate embedding from the big pile of data, and present some of this data to the user in the form of UI interface. Like a music player, for example. Then we collect user feedback about the data and train another ML model that takes embeddings as inputs and predicts some score representing how relevant this data is for the user. We then can use this new model to suggest better recommendations, collect more data and repeat this process again and again, providing a more and more satisfying experience. And all of those steps might be performed locally, just as we wished earlier.\nHere is the little diagram showing how the dataflow in such system may look like:\nI particularly like the option of using p2p Network as a main datasource of such system. In this case we can completely move away from centralized services that control the flow of information and give users a choice to choose what kind of data they need, without any need for content moderation or sharing their personal data. To speed up calculations and not spend time and resources downloading unnecessary data, the data embeddings might be precalculated on the data provider side. So we can check if the data is valuable for us at first and only then accessing it.\nTo give a rating for a particular embedding we can train a model from scratch that takes embeddings as an input and predicts a value that estimates a score that a user would give to the data themselves. Right now Anagnorisis trains a separate evaluation network for each type of data, but in the future I would like to explore a more general approach, for example a multimodal transformer that could take text and embeddings as its input and produce interest scores as an output. While it would be computationally much slower than using some specialized model, it opens up new amazing possibilities that will be discussed later in this series.\nGithub: https://github.com/volotat/Anagnorisis\n","date":"2024-05-12T00:00:00Z","image":"https://volotat.github.io/p/anagnorisis-part-1-a-vision-for-better-information-management/cover_hu_e7135cbc7824aba.jpg","permalink":"https://volotat.github.io/p/anagnorisis-part-1-a-vision-for-better-information-management/","title":"Anagnorisis. Part 1: A Vision for Better Information Management."},{"content":"Completely local data-management platform with built in trainable recommendation engine.\nThe core idea is to create a self-hosted local-first media platform where you can rate your data, and the system trains a personal model to understand your preferences. This model then sorts your data based on your predicted interest, creating a personalized filter for any type of media you might have — images, music, videos, articles, and more.\nLocal First: All data always stays on your device only. All models are trained and inferenced locally. AI Powered: Uses advanced embeddings (CLAP, SigLIP, Jina) to understand, search and filter your content and estimate preferences. FullStack: Built with Flask, Bulma, Transformers, and PyTorch. Uses simple Docker setup for easy deployment. Open Source: AGPL-3.0 license. Contributions, feedback and support are always welcome! Status: Active Development.\nView on GitHub\n","date":"2023-10-08T00:00:00Z","image":"https://volotat.github.io/projects/2023-10-08-anagnorisis/anagnorisis-screenshot%20from%202026-01-02_hu_9d9f6081ce3f604a.png","permalink":"https://volotat.github.io/projects/2023-10-08-anagnorisis/","title":"Anagnorisis"},{"content":"This project was developed as an extension for the Automatic1111 web UI to automate video stylization and enable text-to-video generation using Stable Diffusion 1.5 backbones. At the time of development, generated videos often suffered from severe flickering and temporal inconsistency. This framework addressed those issues by integrating the RAFT optical flow estimation algorithm. By calculating the motion flow between frames, the system could warp the previously generated frame to match the motion of the next one, creating a stable base for the diffusion model. This process, combined with occlusion masks, ensured that only new parts of the scene were generated while maintaining the consistency of existing objects.\nThe tool supported both video-to-video stylization and experimental text-to-video generation. In video-to-video mode, users could apply ControlNet to guide the structure of the output, allowing for stable transformations like turning a real-life video into a watercolor painting or digital art while preserving the original motion. The text-to-video mode employed a custom \u0026ldquo;FloweR\u0026rdquo; method to hallucinate optical flow from static noise, attempting to generate continuous motion from text prompts alone.\nDevelopment on this project was eventually discontinued as the field rapidly advanced. The emergence of modern, end-to-end text-to-video models provided much more coherent and faithful results than could be achieved by hacking image-based diffusion models, rendering this approach largely obsolete for general use cases.\nStatus: Not Maintained\nView on GitHub\n","date":"2023-03-17T00:00:00Z","image":"https://volotat.github.io/projects/2023-03-17-sd-cn-animation/ui_preview_hu_1b671970e12e9272.png","permalink":"https://volotat.github.io/projects/2023-03-17-sd-cn-animation/","title":"SD-CN-Animation"},{"content":"The Holy Grail of all RPG games always was an AI that could reliably reply to what the player says and react to his actions. This was openly pronounced right from the beginning of the genre when text-based games started being developed. However, despite decades of research, there still has not been a perfected AI for RPGs that can truly understand and react to natural language. With the appearance of GPT-3 and other large language models a glimmer of hope appeared. Yet as the world of RPGs is so large and open-ended, it requires an AI with a vast amount of understanding and context about the environment and the player’s actions in order to be able to produce realistic responses. This means that creating an AI which can reliably interact with players is an extremely difficult task — one which has yet to be achieved. Although AI Dungeon seems to be a good step forward.\nThere is another genre of games that could be a perfect petri dish for such an AI — Dating sims. Relying heavily on the text they are much closer to situations to what GPT-3 is used to deal with. They usually have a few characters and most of the time do not present any real challenge to the player, rather just telling a story the author of the game had in mind. Another selling point of dating sims is the great art that the game presents to the player. Here is where our new cool kid — Stable Diffusion — could show himself in full glory. The idea seems obvious, we should use GPT-3 to generate interactive stories and prompts for Stable Diffusion, that would generate images for that story. Even better, with technology like DreamBooth that allows us to introduce new concepts to image generation model, we should be able to produce coherent locations, characters and style of the images. But will it really work in practice? To test this idea I decided to emulate a dating game within a simple text document.\nThe GPT-3 prompt I used was looking in the following way:\nThe following is the dialog between the character Alice Lake (Jessica Chastain) and the player. Alice Lake (Jessica Chastain) does not know the player yet, but is very happy to meet him and help with anything he is asking. The following happened in the big house where the player woke up with huge pain in his head. … Short summary of the dialog so far: \u0026ldquo;[First GPT API call, do not need to call it if it is the first message]\u0026rdquo; Alice: \u0026ldquo;[Second GPT API call]\u0026rdquo; Description of an image shown to the player: \u0026ldquo;[Third GPT API call]\u0026rdquo; Player: \u0026ldquo;[player\u0026rsquo;s input]\u0026rdquo;\n“…” — represents the previous state of the dialog. Because we have a “Short summary of the dialog so far” section that works like a long-term memory, we do not need to store more than the last two messages.\nHere is the first message from the character, description of an image shown to the player and the actual image generated from this description, as well as player’s answer to the character:\nI used the Anything-V3 model as it could generate visually pleasing results consistently, without too much prompt engineering. To not waste time training the DreamBooth model for coherent rendering of characters I simply used a famous person, in my case this was Jessica Chastain, as a character anchor and vague description of the clothes in the prompt. In the end the prompt I used to generate images was the following:\nPrompt: [Description of an image from GPT-3] + “Detailed photo-realistic anime style. Inside a house. Tight red dress.”\nNegative: (((ugly))), fake image, blurry image, blur, corrupted image, old black and white photo, out of frame, without head, too close, cropped, collage, split image\nI tried to keep the first image that was generated, although I did allow myself to regenerate an image if it was completely off as it happened a couple of times.\nThe first pleasant surprise I got was when I asked Alice’s character to give me a cup of coffee. I was expected to see another simple close up view of the character but instead got this:\nAs we could see the description of an image does not contain two cups of coffee, so this was just a coincidence, although it was not the last such a surprise. When I asked Alice to find my documents I got an image of her actually looking around the house.\nAt this point it was clear that the idea in general was working. As the next step I tried to see limitations of the system. I tried to “get rid” of the character and make it show me some inanimate object. I hoped that GPT-3 would generate a description of an image that does not contain Alice, but it kept her in nevertheless.\nAt some point GPT-3 decided to put the player into the image. Here is where the most important limitation became clear — for a coherent story not only the image should depend on the dialog, the dialog itself should also depend on the generated images.\nThe main takeaway of this little experiment that I got is that we are very close to creating believable characters powered by an AI. The only thing we really need is a multimodal approach as powerful as GPT-3 and Stable Diffusion combined, able to generate text and images as a coherent stream of tokens. It would also be beneficial to have some sort of memory in the form of learned special tokens that contain information of the previous events in a compressed form as the “Short summary of the dialog so far” section does. The good news, that new models like Flamingo from DeepMind could do exactly that.\nThe whole story I got by interacting with “the game” is available in this document.\nLet’s see what the future holds for us…\n","date":"2022-12-16T00:00:00Z","permalink":"https://volotat.github.io/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/","title":"Combining GPT-3 and Stable Diffusion to imagine a next level game engine."},{"content":"Introduction In the movie Arrival (2016) a group of scientists tries to learn the language of extraterrestrial aliens that arrived on Earth and by doing so they change the way they think which allows the main character to obtain some extraordinary power. While such power is in the domain of fiction, the theory that our language dictates the way we think has been widely explored in the literature.\nThere is a main crucial distinction in how people and current AI systems process information. All major human languages are based on discrete bags of words, while ML models almost always obtain and output information in terms of lists of real values. Because of this, all NLP models have a process of “vectorization” when a whole word, part of it, or single characters obtain their own vector representation that conveys their internal meaning accessible for further processing within the models. The main benefit of vectorization is that these vectors are continuous, therefore, could be adjusted (trained) through backpropagation. In a world of NLP models, the words “planet” and “moon” have some distance between each of them and could be gradually transformed from one to another.\nHere is where the first glimpse of a new superpower will show up. Because of the discrete nature of our language people tend to think in terms of types, even when these types cannot be properly defined. The most obvious examples of it are in cosmology. There are some types of objects that at first glance fit very well into our discrete view of the world: planets, moons, stars, comets, asteroids, black holes and so on. But there are also some in-betweeny objects such as black dwarfs that are neither a planet nor a star but have properties of both depending from which angle you look at it. And there is Pluto… that was a planet, but lost its status when the definition of a planet was changed. The thing is, there is an ongoing debate that this new definition is not purely scientific and based on a recent observation of Pluto’s geological activity, it should be a planet after all. The type system breakes when we find something that lies on the boundary between the types. And most of the time, a new type is created alongside a bunch of new boundaries that wait to be filled with new exceptions. It seems that it would be beneficial to have a language that does not have these boundaries in its nature and works more closely with how ML models work. So, here comes the question: could we create a language that is understandable by a human but works in a continuous fashion?\nVector Based Language In some sense, we already have a continuous language in the form of images. If we ask one human to draw a house and ask another what this image means, we have a high chance that the guess would be right. And a list of images might tell stories just as a sentence does. But there is a problem, for any word in a language there are infinite possibilities of depicting it in the form of an image, and then there are even more possibilities of interpreting the image back to words. So the language made of regular images would not be very reliable in conveying an exact meaning that was originally intended. To overcome this problem, we need a system that could generate some sort of an image for every word given and could guarantee that this image might be converted back to the original word exactly.\nThe main tool we are going to use in the pursuit of developing such language is, once again, machine learning. The setup will look like this: there is a pre-trained language model that takes a word or a sentence as an input and produces some vector representation of it, there is also a second model that transforms this vector into some human-readable format, and lastly, there is a human who has to guess the original word that has been given. The human is allowed to take any time necessary to learn how the system works.\nArchitecture The general thing we want to achieve is to have a model that generates some embedding of the given text and then produces a visualization of these embeddings. To obtain embeddings of text, we will use universal sentence encoder from Google, and to generate the visualization we will train generator network G. To do so, we need to have some approximation of the human visual system to make resulting images human-readable. In our case, this approximation would be a network V that we will initialize with mobile-net-v2 weights and allow these weights to change at the training stage.\nWe are leaving the vision model trainable as it would bring another layer of regularization to the system. If we don’t do that and freeze the model, the decoding network tends to exploit inaccuracies in the visual model and settles on the solution that has a good reconstruction rate, yet all images are still pretty much the same, and only subtle differences are present that are very hard for a human to notice.\nWe train this architecture with the following loss function:\nWhere G — generator network, V — vision network, D — decoder network. I_1 and I_2 are embeddings of two randomly generated sentences.\nHere, the purpose of L_1 term is to make sure that distances in visual latent space are roughly the same as distances in word embeddings space. L_2 keeps images generated by G as far apart as possible in RBG space. L_3 keeps images generated by G as far apart as possible in visual space while keeping its mean at 0. L_4, the most important term, makes sure that reconstructed word embeddings are the same as initial ones. Values for α, β, γ are found empirically.\nTo make the generator to be able to produce a meaningful image from any point of embedding space of the universal sentence encoder and prevent overfitting, we have to probe the space as densely as possible at the training stage. To do so, we will generate random sentences with the help of nltk library. Here are the examples of such generated sentences: ‘ideophone beanfield tritonymphal fatuism preambulate nonostentation overstrictly pachyhaemous’, ‘hyperapophysial’, ‘southern’, ‘episynaloephe subgenerically gleaning reformeress’, ‘trigonelline’, ‘commorant saltspoon’, ‘nonpopularity mammaliferous isobathythermal phenylglyoxylic insulate aortomalaxis desacralize spooky’, ‘speed garn nunciatory neologism’, ‘podobranchial fencible’, ‘epeirid gibaro’, ‘sleeved’, ‘demonographer probetting subduingly’, ‘velociously calpacked invaccinate acushla amixia unicolor’ and so on.\nTesting To test if our methods are doing well we are going to use the reconstruction rate metric. To calculate the metric, we need to collect 2265 most common English words and expand this list with 10 digits. A random word gets converted to its embedding with the universal sentence encoder and passed to the generator that generates the image representing the word. Then, depending on who is the test subject (human or machine) we use one of the following:\nFor a machine, we pass generated images to the pre-trained visual network and then decode it with decoder network D to get back the word embedding. We compare this new embedding with all embeddings in the dictionary, and if the euclidian distance from the new embedding to the original embedding is the smallest, we give the point to the system. After checking all the words in such a way, we obtain:\nreconstruction rate = correct answers / number of words * 100%.\nPresentation of Human Testing Interface For human testing, we give a generated image to the test subject with a visual interface that presents the image with four possible answers. Human should learn through trial and error what word each image represents and report their best score. To make this task more convenient, at the beginning of the session, the testing interface asks what size of the dictionary the participant wants to use. The dictionary itself consists in a way that it’s growing from the most to less common English words. Volume of the dictionary is separated by the following levels: 10, 20, 40, 100, 200, 400, 1000, and 2275 (full dictionary). Here is a sample of the first 30 words presented in the dictionary:\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9, the, of, and, to, a, in, is, you, are, for, that, or, it, as, be, on, your, with, can, have… and so on.\nIn the human case, the reconstruction rate is calculated by the exact same formula. To check the accuracy, the test subject is supposed to answer 100 questions with full dictionary size. Although it is worth noting that the random baseline for humans is 25% as it is always possible to simply guess the right answer from the 4 options presented.\nResults After training the system, it was producing the following images for respective input sentences:\nThe reconstruction rate for the decoder network at the end of the training was 100%, and the loss value was 0.02.\nI did the experiment of learning this language on myself and was able to achieve a 43% reconstruction rate after two weeks of training. It was beneficial for me to start from a small number of words and gradually increase this number with less and less common words. The promise was, that at some point you do not really need to remember each image, the meaning of the word should be deducible from the geometry presented in the image. So learning such a language should be much easier than learning a typical natural language. While learning I used the 85% threshold to make sure that I’m ready to increase the volume of the dictionary.\nHere is how a two-week learning progress looked like for different dictionary sizes:\nWhile bigger dictionaries became progressively harder to learn, the upward trend is clearly visible in all cases. This proves the most important point that answers the question that was set at the beginning. We can create a language that works in a continuous fashion and such language is learnable by a human. While implications of this are yet to be known, in the next section, I will speculate on some that might be interesting to investigate further.\nThe source code and pre-trained models are available on the project’s GitHub page: https://github.com/volotat/Vector-Based-Language\nFull training history is available here: https://github.com/volotat/Vector-Based-Language/blob/main/article/history.csv\nThoughts To my knowledge, that is the only example of a synthetic language that possesses a notion of continuity where words and even sentences could blend into each other. I suggest that such language should be interesting even as an example that such a thing is possible in principle. I hope this may make someone fascinated by the idea to explore it further in their own right.\nI think that the most promising aspect of such language is the combination of it with some sort of brain-computer interface. The obvious downside of the language at the current stage is that there is no clear way to produce the image directly from the intent rather than words. With good enough BCI, this problem could be solved. Therefore such language might be the first way to store thoughts in human-readable format.\nWith the rise of foundation models, and especially text2image generators, such as Stable Diffusion, there is a need to produce and store concepts that could not be easily conveyed with words but still could be represented as embeddings. There is recent work called Textual Inversion that tries to solve this exact problem. With language like this, we could visualize these embeddings in a meaningful way and even use it as a common concept space for many models in the future.\nThe language might be significantly improved to be more human-friendly. For example, we can use more advanced models such as CLIP (to be more specific, an image encoder part of it) as a visual model at the training stage to produce more readable images. Or we could use a set of predefined text-image pairs as anchors for more controllable outputs.\nAnd lastly, to really show that generated images represent the meaning of the words rather than the words themselves…\n","date":"2022-09-06T00:00:00Z","image":"https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_Tpm0MlWfLdNMGQ9b_hu_548f263270ea8f6b.png","permalink":"https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/","title":"Can Humans Speak the Language of the Machines?"},{"content":"Research project exploring the possibility of creating a continuous, visual language that hint at the possibility of direct understanding of the embedding spaces produced by ML models.\nUnlike traditional discrete languages, this system uses machine learning to generate unique visual representations (images) for any given text embedding. The goal is to create a language where concepts can blend into each other continuously, mirroring how neural networks process information.\nWords and sentences are represented as points in a continuous vector space, allowing for smooth transitions between concepts. Experiments show that humans can learn to interpret these generated visual embeddings with increasing accuracy over time. The visual language is designed to be perfectly reconstructible back into the original text embeddings by a decoder network crating a bridge between human cognition and the \u0026ldquo;black box\u0026rdquo; mechanized interpretation of the data.\nStatus: Completed Experiment\nRead the Article | View on GitHub\n","date":"2022-08-27T00:00:00Z","image":"https://volotat.github.io/projects/2022-08-27-vector-based-language/1_lgXj1-BGYkDVqLytrz9D9A_hu_36bf6c29f64a1064.png","permalink":"https://volotat.github.io/projects/2022-08-27-vector-based-language/","title":"Vector Based Language"},{"content":"Disclaimer: this text is prepared for readers who already familiar with the ARC challenge. If you don’t know what it is, you can take a look at the github page of the challenge and the original paper “On the Measure of Intelligence” by François Chollet.\nThe main idea is to produce a system that consists of two differentiable parts — a programmer and a computer.\nThe first part of the system — programmer — takes three inputs: an input data that should be processed, an output data that should be generated and the current state of the program that supposed to covent an input data into an output data. The task of the programmer is to generate a better version of the program.\nThe second part of the system — computer — takes two inputs: an input data and the program. The task of the computer is to apply the program to the input data and produce an output.\nIn actuality the programmer and the computer are both the neural networks and the program is a N-dimensional vector. Because both of the parts are differentiable we could backpropagate through the whole system to train both programmer and computer to work coherently. Furthermore we may propagate through several steps of the pipeline to get better results. With this architecture we could have a system that may take an arbitrary number of examples and produce a solution to a new input based on the program it comes up with.\nTo test this idea I created a small dataset resembling the ARC, that was partially created by hand and partially generated on the fly, but with some additional restrictions: all tasks have 6x6 grids, each cell has only two states (black and white), all tasks have exactly 5 examples.\nHere is a little glimpse of how the network performed on the tasks:\nEach column (3x6) here represent one task. Inside the task first colum represent an input, second — an output and the third — a guess of the networks considering all previous examples and current input given.\nIn my experiments this system worked really well for procedurally generated tasks, a bit worse on the in distribution data and horrible on the out of distribution data. In other words, it could solve the type of tasks it saw, but almost always failed on novel ones. Although the scale of the experiments was miniscule and all networks were composed of several fully-connected layers, so there are still a potential to make it work.\n","date":"2022-07-07T00:00:00Z","permalink":"https://volotat.github.io/p/neurogrammer-architecture-arc-challenge/","title":"NeuroGrammer — a proposal of an architecture for solving the ARC challenge."},{"content":"A Python program that generates Python programs that generate generative art.\nMost generative art relies on stochastic processes where the initial seed and specific parameters are often lost, making exact reproduction difficult. GAS takes a different approach: instead of storing just the output or the parameters, it generates a fully deterministic, standalone Python script for each artwork. This ensures complete reproducibility—if you have the script, you have the art.\nThe core mechanism involves initializing a tensor with coordinate data and then applying a random sequence of mathematical transformations (like transit, sin, magnitude, shift) to its channels. These operations are restricted to the [-1, 1] range to ensure stability. The final result is a composition of these channels converted into color space.\nSelf-Contained Art: Each generated piece is a runnable Python script with zero external dependencies beyond standard scientific libraries (numpy, PIL). Deterministic: The generated scripts contain no random elements; running the same script always produces the exact same image. Method-Based generation: Uses a palette of composable mathematical functions (sin, prod, soft_min, etc.) to \u0026ldquo;sculpt\u0026rdquo; the image in a high-dimensional channel space. Aesthetic Scoring: Includes a simple scoring model to estimate the visual quality of generated outputs. Status: Completed Experiment\nView on GitHub\n","date":"2021-12-04T00:00:00Z","image":"https://volotat.github.io/projects/2021-12-04-generative-art-synthesizer/gas_preview_hu_11e99014d38b73ee.png","permalink":"https://volotat.github.io/projects/2021-12-04-generative-art-synthesizer/","title":"Generative Art Synthesizer"},{"content":"The Abstraction and Reasoning Corpus made into a web game.\nThe aim of this project is to create an easy-to-use interface for François Chollet\u0026rsquo;s Abstraction and Reasoning Corpus (ARC-AGI), designed so that children as young as three years old can play with it. This tool explores the potential of using ARC as educational material for developing abstraction and reasoning skills in young kids, challenging cognitive abilities such as pattern recognition, logical reasoning, and problem-solving.\nThe game presents visual tasks consisting of grid pairs that represent a transformation (input → output). The player must deduce the transformation rule from the examples and apply it to a test grid.\nChild-Friendly Interface: Simplified controls allow for painting, dragging to fill, and copying grids, making it accessible for early childhood development. Fixed Grid Sizes: Unlike the original ARC where output size is part of the solution, here the output grid size is pre-set to reduce complexity and focus on the transformation logic. Printable Version: Tasks can be automatically formatted and printed on paper with adjusted colors (e.g., swapping black for white) for offline solving with markers or pencils. Status: Not Maintained.\nCited in Scientific Literature Abstraction and Reasoning Challenge Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus System 2 Reasoning for Human-AI Alignment: Generality and Adaptivity via ARC-AG Addressing and Visualizing Misalignments in Human Task-Solving Trajectories Play the Game | View on GitHub\n","date":"2021-07-26T00:00:00Z","image":"https://volotat.github.io/projects/2021-07-26-the-arc-game/level_example_hu_e41ccaa0179eb00f.jpg","permalink":"https://volotat.github.io/projects/2021-07-26-the-arc-game/","title":"The ARC Game"},{"content":"Image morphing without reference points by optimizing warp maps via gradient descent.\nThis project introduces a \u0026ldquo;differentiable morphing\u0026rdquo; algorithm that can smoothly transition between any two images without requiring manual reference points or landmarks. Unlike traditional generative models that learn a distribution from a dataset, this approach uses a neural network as a temporary functional mapping to solve a specific optimization problem for a single pair of images.\nThe algorithm finds a set of maps that transform the source image into the target image.\nBy interpolating the strength of these maps, the system produces a smooth, seamless animation where features transform fluidly from one state to another.\nStatus: Completed Experiment\nCited in Scientific Literature The explainability paradox: Challenges for xAI in digital pathology IMPUS: Image Morphing with Perceptually-Uniform Sampling Using Diffusion Models FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model View on GitHub\n","date":"2021-01-02T00:00:00Z","image":"https://volotat.github.io/projects/2021-01-02-differentiable-morphing/formula_hu_384c19c16a6fb39e.jpg","permalink":"https://volotat.github.io/projects/2021-01-02-differentiable-morphing/","title":"Differentiable Morphing"},{"content":"Continuous Adversarial Image Generator that can produce high-quality images with relatively few examples and resolution independence.\nThis project implements a Generative Adversarial Network (GAN) that learns a continuous function mapping coordinates (x, y) and a latent vector z to an RGB color, rather than generating a fixed grid of pixels. By treating images as continuous functions, the model creates a \u0026ldquo;vector-like\u0026rdquo; representation that can be sampled at any resolution.\nThe architecture differs from traditional GANs by using a coordinate-based generator. The system includes an \u0026ldquo;Ident\u0026rdquo; network to map images to a latent space, a Generator that predicts colors based on position and latent codes, and a Discriminator that validates pixel data in context.\nResults demonstrated that the model could generalize well from small datasets (10-20 images is enough for reasonable results, while training the network completely from the scratch) and produce smooth interpolations in the latent space, effectively hallucinating plausible variations between training examples.\nStatus: Completed Experiment\nView on GitHub\n","date":"2018-01-29T00:00:00Z","image":"https://volotat.github.io/projects/2018-01-29-congan/faces_table_hu_a46473d62aa852b.png","permalink":"https://volotat.github.io/projects/2018-01-29-congan/","title":"ConGAN"},{"content":"An experimental approach to training autoencoders for meaningful low-dimensional data representation.\nThis project was one of my first experiments in machine learning, serving as a deep dive into how autoencoders process information. The goal was to address the difficulty of encoding high-dimensional data (like images) into very low-dimensional latent spaces (like 2D) without the model getting stuck in local minima. Usually it takes at least 16 or more dimensions for bottleneck embeddings to get decent results, but I wanted to see if it was possible to achieve this in just 2 dimensions.\nThe method involves training the autoencoder in a typical fashion, alongside with the encoder that trains on a dynamic set of target representations. It works by initially setting random points for each datapoint and then, at each step, stretching the latent space to touch the predefined space boundaries while applying a repulsion force to keep datapoints distinct. This prevents points from clustering too densely and encourages a more uniform distribution.\nThe result, as seen in the project image, is a continuous 2D embedding space that successfully compacts the entire MNIST dataset, allowing for smooth transitions between different digits.\nStatus: Completed Experiment\nView on GitHub\n","date":"2017-11-18T00:00:00Z","image":"https://volotat.github.io/projects/2017-11-18-low-dimensional-autoencoder/latent_space_hu_4fbb826fa6fa9301.png","permalink":"https://volotat.github.io/projects/2017-11-18-low-dimensional-autoencoder/","title":"Low Dimensional Autoencoder"}]