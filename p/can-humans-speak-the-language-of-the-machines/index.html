<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="This article explores the concept of creating a continuous language that bridges human communication and machine understanding. By leveraging machine learning techniques, it proposes a novel approach to language design that could enhance human cognition and interaction with AI systems."><title>Alexey Borsky | Can Humans Speak the Language of the Machines?</title><link rel=canonical href=https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/><link rel=stylesheet href=/scss/style.min.833d6eed45de56f48306bf57268d5b8cdfc8a60e8e7bdc99810464fcd033f7c6.css><meta property='og:title' content="Can Humans Speak the Language of the Machines?"><meta property='og:description' content="This article explores the concept of creating a continuous language that bridges human communication and machine understanding. By leveraging machine learning techniques, it proposes a novel approach to language design that could enhance human cognition and interaction with AI systems."><meta property='og:url' content='https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/'><meta property='og:site_name' content='Alexey Borsky'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='Machine Learning'><meta property='article:tag' content='Transhumanism'><meta property='article:tag' content='AI'><meta property='article:tag' content='Data Science'><meta property='article:tag' content='Neural Networks'><meta property='article:published_time' content='2022-09-06T00:00:00+00:00'><meta property='article:modified_time' content='2022-09-06T00:00:00+00:00'><meta property='og:image' content='https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_Tpm0MlWfLdNMGQ9b.png'><meta name=twitter:title content="Can Humans Speak the Language of the Machines?"><meta name=twitter:description content="This article explores the concept of creating a continuous language that bridges human communication and machine understanding. By leveraging machine learning techniques, it proposes a novel approach to language design that could enhance human cognition and interaction with AI systems."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://volotat.github.io/p/can-humans-speak-the-language-of-the-machines/0_Tpm0MlWfLdNMGQ9b.png'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_e5db16934e59d865.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Alexey Borsky</a></h1><h2 class=site-description>Self taught programmer and ML engineer. Active open-source contributor. Mostly interested in procedural generation and generative AI.</h2></div></header><ol class=menu-social><li><a href=https://github.com/volotat target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://x.com/volotat target=_blank title=X rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-x"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4l11.733 16H20L8.267 4z"/><path d="M4 20l6.768-6.768m2.46-2.46L20 4"/></svg></a></li><li><a href=https://www.youtube.com/@volotat target=_blank title=YouTube rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-youtube"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M2 8a4 4 0 014-4h12a4 4 0 014 4v8a4 4 0 01-4 4H6a4 4 0 01-4-4V8z"/><path d="M10 9l5 3-5 3z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/projects/><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-file-code"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14 3v4a1 1 0 001 1h4"/><path d="M17 21H7a2 2 0 01-2-2V5a2 2 0 012-2h7l5 5v11a2 2 0 01-2 2z"/><path d="M10 13l-1 2 1 2"/><path d="M14 13l1 2-1 2"/></svg>
<span>Projects</span></a></li><li><a href=/archives/><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-notebook"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M6 4h11a2 2 0 012 2v12a2 2 0 01-2 2H6a1 1 0 01-1-1V5a1 1 0 011-1m3 0v18"/><path d="M13 8h2"/><path d="M13 12h2"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#vector-based-language>Vector Based Language</a></li><li><a href=#architecture>Architecture</a></li><li><a href=#testing>Testing</a></li><li><a href=#results>Results</a></li><li><a href=#thoughts>Thoughts</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/can-humans-speak-the-language-of-the-machines/><img src=/p/can-humans-speak-the-language-of-the-machines/0_Tpm0MlWfLdNMGQ9b_hu_1972e6e8c2a47ef3.png srcset="/p/can-humans-speak-the-language-of-the-machines/0_Tpm0MlWfLdNMGQ9b_hu_1972e6e8c2a47ef3.png 800w, /p/can-humans-speak-the-language-of-the-machines/0_Tpm0MlWfLdNMGQ9b_hu_828ec5498f898d97.png 1600w" width=800 height=450 loading=lazy alt="Featured image of post Can Humans Speak the Language of the Machines?"></a></div><div class=article-details><header class=article-category><a href=/categories/ideas/ style=background-color:#e69326;color:#fff>Ideas
</a><a href=/categories/projects/ style=background-color:#2a9d8f;color:#fff>Projects
</a><a href=/categories/highlights/ style=background-color:#3577ce;color:#fff>Highlights</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/can-humans-speak-the-language-of-the-machines/>Can Humans Speak the Language of the Machines?</a></h2><h3 class=article-subtitle>This article explores the concept of creating a continuous language that bridges human communication and machine understanding. By leveraging machine learning techniques, it proposes a novel approach to language design that could enhance human cognition and interaction with AI systems.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published datetime=2022-09-06T00:00:00Z>Sep 06, 2022</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>10 minute read</time></div></footer></div></header><section class=article-content><h2 id=introduction>Introduction</h2><p>In the movie Arrival (2016) a group of scientists tries to learn the language of extraterrestrial aliens that arrived on Earth and by doing so they change the way they think which allows the main character to obtain some extraordinary power. While such power is in the domain of fiction, the theory that our language dictates the way we think <a class=link href=https://en.wikipedia.org/wiki/Linguistic_relativity target=_blank rel=noopener>has been widely explored in the literature</a>.</p><p>There is a main crucial distinction in how people and current AI systems process information. All major human languages are based on discrete bags of words, while ML models almost always obtain and output information in terms of lists of real values. Because of this, all NLP models have a process of “vectorization” when a whole word, part of it, or single characters obtain their own vector representation that conveys their internal meaning accessible for further processing within the models. The main benefit of vectorization is that these vectors are continuous, therefore, could be adjusted (trained) through backpropagation. In a world of NLP models, the words “planet” and “moon” have some distance between each of them and could be gradually transformed from one to another.</p><p><img src=/p/can-humans-speak-the-language-of-the-machines/0_OiPWVzJqYD5qTEf7.png width=1400 height=538 srcset="/p/can-humans-speak-the-language-of-the-machines/0_OiPWVzJqYD5qTEf7_hu_3158852279ffec97.png 480w, /p/can-humans-speak-the-language-of-the-machines/0_OiPWVzJqYD5qTEf7_hu_84228caddcd61232.png 1024w" loading=lazy alt="Vector representations of words in latent space of NLP models" class=gallery-image data-flex-grow=260 data-flex-basis=624px></p><p>Here is where the first glimpse of a new superpower will show up. Because of the discrete nature of our language people tend to think in terms of types, even when these types cannot be properly defined. The most obvious examples of it are in cosmology. There are some types of objects that at first glance fit very well into our discrete view of the world: planets, moons, stars, comets, asteroids, black holes and so on. But there are also some in-betweeny objects such as black dwarfs that are neither a planet nor a star but have properties of both depending from which angle you look at it. And there is Pluto… that was a planet, but lost its status when the definition of a planet was changed. The thing is, there is an ongoing debate that this new definition is not purely scientific and based on a recent observation of Pluto’s geological activity, <a class=link href=https://www.sciencedirect.com/science/article/pii/S0019103521004206 target=_blank rel=noopener>it should be a planet after all</a>. The type system breakes when we find something that lies on the boundary between the types. And most of the time, a new type is created alongside a bunch of new boundaries that wait to be filled with new exceptions. It seems that it would be beneficial to have a language that does not have these boundaries in its nature and works more closely with how ML models work. So, here comes the question: <strong>could we create a language that is understandable by a human but works in a continuous fashion?</strong></p><h2 id=vector-based-language>Vector Based Language</h2><p>In some sense, we already have a continuous language in the form of images. If we ask one human to draw a house and ask another what this image means, we have a high chance that the guess would be right. And a list of images might tell stories just as a sentence does. But there is a problem, for any word in a language there are infinite possibilities of depicting it in the form of an image, and then there are even more possibilities of interpreting the image back to words. So the language made of regular images would not be very reliable in conveying an exact meaning that was originally intended. To overcome this problem, we need a system that could generate some sort of an image for every word given and could guarantee that this image might be converted back to the original word exactly.</p><p><img src=/p/can-humans-speak-the-language-of-the-machines/0_qJX7THDcOFH_XBpi.png width=625 height=790 srcset="/p/can-humans-speak-the-language-of-the-machines/0_qJX7THDcOFH_XBpi_hu_f266c63775a7004b.png 480w, /p/can-humans-speak-the-language-of-the-machines/0_qJX7THDcOFH_XBpi_hu_8301af913c7f316f.png 1024w" loading=lazy alt="An example of a story told entirely with images" class=gallery-image data-flex-grow=79 data-flex-basis=189px></p><p>The main tool we are going to use in the pursuit of developing such language is, once again, machine learning. The setup will look like this: there is a pre-trained language model that takes a word or a sentence as an input and produces some vector representation of it, there is also a second model that transforms this vector into some human-readable format, and lastly, there is a human who has to guess the original word that has been given. The human is allowed to take any time necessary to learn how the system works.</p><h2 id=architecture>Architecture</h2><p>The general thing we want to achieve is to have a model that generates some embedding of the given text and then produces a visualization of these embeddings. To obtain embeddings of text, we will use <a class=link href=https://arxiv.org/abs/1803.11175 target=_blank rel=noopener>universal sentence encoder</a> from Google, and to generate the visualization we will train generator network G. To do so, we need to have some approximation of the human visual system to make resulting images human-readable. In our case, this approximation would be a network V that we will initialize with mobile-net-v2 weights and allow these weights to change at the training stage.</p><p>We are leaving the vision model trainable as it would bring another layer of regularization to the system. If we don’t do that and freeze the model, the decoding network tends to exploit inaccuracies in the visual model and settles on the solution that has a good reconstruction rate, yet all images are still pretty much the same, and only subtle differences are present that are very hard for a human to notice.</p><p><img src=/p/can-humans-speak-the-language-of-the-machines/1_ZY2XQdsCwSDpn3-aXAzv9w.png width=798 height=904 srcset="/p/can-humans-speak-the-language-of-the-machines/1_ZY2XQdsCwSDpn3-aXAzv9w_hu_cacc4d1e0d1d4571.png 480w, /p/can-humans-speak-the-language-of-the-machines/1_ZY2XQdsCwSDpn3-aXAzv9w_hu_77ff654ceee10bbb.png 1024w" loading=lazy alt="General representation of the system’s architecture" class=gallery-image data-flex-grow=88 data-flex-basis=211px></p><p>We train this architecture with the following loss function:</p><p><img src=/p/can-humans-speak-the-language-of-the-machines/1_Rz4gSkKEn2uc4SFVKCEGSg.png width=1295 height=619 srcset="/p/can-humans-speak-the-language-of-the-machines/1_Rz4gSkKEn2uc4SFVKCEGSg_hu_9fef1db847a0b0ff.png 480w, /p/can-humans-speak-the-language-of-the-machines/1_Rz4gSkKEn2uc4SFVKCEGSg_hu_c3e748cf832aff92.png 1024w" loading=lazy class=gallery-image data-flex-grow=209 data-flex-basis=502px></p><p>Where G — generator network, V — vision network, D — decoder network. I_1 and I_2 are embeddings of two randomly generated sentences.</p><p>Here, the purpose of L_1 term is to make sure that distances in visual latent space are roughly the same as distances in word embeddings space. L_2 keeps images generated by G as far apart as possible in RBG space. L_3 keeps images generated by G as far apart as possible in visual space while keeping its mean at 0. L_4, the most important term, makes sure that reconstructed word embeddings are the same as initial ones. Values for α, β, γ are found empirically.</p><p>To make the generator to be able to produce a meaningful image from any point of embedding space of the universal sentence encoder and prevent overfitting, we have to probe the space as densely as possible at the training stage. To do so, we will generate random sentences with the help of <a class=link href=https://www.nltk.org/ target=_blank rel=noopener>nltk</a> library. Here are the examples of such generated sentences: ‘ideophone beanfield tritonymphal fatuism preambulate nonostentation overstrictly pachyhaemous’, ‘hyperapophysial’, ‘southern’, ‘episynaloephe subgenerically gleaning reformeress’, ‘trigonelline’, ‘commorant saltspoon’, ‘nonpopularity mammaliferous isobathythermal phenylglyoxylic insulate aortomalaxis desacralize spooky’, ‘speed garn nunciatory neologism’, ‘podobranchial fencible’, ‘epeirid gibaro’, ‘sleeved’, ‘demonographer probetting subduingly’, ‘velociously calpacked invaccinate acushla amixia unicolor’ and so on.</p><h2 id=testing>Testing</h2><p>To test if our methods are doing well we are going to use the reconstruction rate metric. To calculate the metric, we need to collect <a class=link href=https://www.talkenglish.com/vocabulary/top-2000-vocabulary.aspx target=_blank rel=noopener>2265 most common English words</a> and expand this list with 10 digits. A random word gets converted to its embedding with the <a class=link href=https://arxiv.org/abs/1803.11175 target=_blank rel=noopener>universal sentence encoder</a> and passed to the generator that generates the image representing the word. Then, depending on who is the test subject (human or machine) we use one of the following:</p><p>For a machine, we pass generated images to the pre-trained visual network and then decode it with decoder network D to get back the word embedding. We compare this new embedding with all embeddings in the dictionary, and if the euclidian distance from the new embedding to the original embedding is the smallest, we give the point to the system. After checking all the words in such a way, we obtain:</p><p>reconstruction rate = correct answers / number of words * 100%.</p><figure><div style=display:flex;justify-content:center;gap:10px><img src=1_R_GQTs5EsJCZTU5SZP3HQg.png style=width:48%>
<img src=1_x59gDtlLr5RG36bl_HlpJw.png style=width:48%></div><figcaption>Presentation of Human Testing Interface</figcaption></figure><p>For human testing, we give a generated image to the test subject with a visual interface that presents the image with four possible answers. Human should learn through trial and error what word each image represents and report their best score. To make this task more convenient, at the beginning of the session, the testing interface asks what size of the dictionary the participant wants to use. The dictionary itself consists in a way that it’s growing from the most to less common English words. Volume of the dictionary is separated by the following levels: 10, 20, 40, 100, 200, 400, 1000, and 2275 (full dictionary). Here is a sample of the first 30 words presented in the dictionary:</p><p>0, 1, 2, 3, 4, 5, 6, 7, 8, 9, the, of, and, to, a, in, is, you, are, for, that, or, it, as, be, on, your, with, can, have… and so on.</p><p>In the human case, the reconstruction rate is calculated by the exact same formula. To check the accuracy, the test subject is supposed to answer 100 questions with full dictionary size. Although it is worth noting that the random baseline for humans is 25% as it is always possible to simply guess the right answer from the 4 options presented.</p><h2 id=results>Results</h2><p>After training the system, it was producing the following images for respective input sentences:</p><p><img src=/p/can-humans-speak-the-language-of-the-machines/1_lgXj1-BGYkDVqLytrz9D9A.png width=1340 height=1274 srcset="/p/can-humans-speak-the-language-of-the-machines/1_lgXj1-BGYkDVqLytrz9D9A_hu_e6c6f08ff1eb2e9a.png 480w, /p/can-humans-speak-the-language-of-the-machines/1_lgXj1-BGYkDVqLytrz9D9A_hu_d7b727979f31604d.png 1024w" loading=lazy class=gallery-image data-flex-grow=105 data-flex-basis=252px></p><p>The reconstruction rate for the decoder network at the end of the training was 100%, and the loss value was 0.02.</p><p>I did the experiment of learning this language on myself and was able to achieve a 43% reconstruction rate after two weeks of training. It was beneficial for me to start from a small number of words and gradually increase this number with less and less common words. The promise was, that at some point you do not really need to remember each image, the meaning of the word should be deducible from the geometry presented in the image. So learning such a language should be much easier than learning a typical natural language. While learning I used the 85% threshold to make sure that I’m ready to increase the volume of the dictionary.</p><p>Here is how a two-week learning progress looked like for different dictionary sizes:</p><p><img src=/p/can-humans-speak-the-language-of-the-machines/1_aN_F1IG-q6nR7ECNjJRoyQ.png width=1059 height=697 srcset="/p/can-humans-speak-the-language-of-the-machines/1_aN_F1IG-q6nR7ECNjJRoyQ_hu_8c9399ef05ae7684.png 480w, /p/can-humans-speak-the-language-of-the-machines/1_aN_F1IG-q6nR7ECNjJRoyQ_hu_204ad6b8ef0ccee0.png 1024w" loading=lazy class=gallery-image data-flex-grow=151 data-flex-basis=364px></p><p>While bigger dictionaries became progressively harder to learn, the upward trend is clearly visible in all cases. This proves the most important point that answers the question that was set at the beginning. <strong>We can create a language that works in a continuous fashion and such language is learnable by a human.</strong> While implications of this are yet to be known, in the next section, I will speculate on some that might be interesting to investigate further.</p><p>The source code and pre-trained models are available on the project’s GitHub page: <a class=link href=https://github.com/volotat/Vector-Based-Language target=_blank rel=noopener>https://github.com/volotat/Vector-Based-Language</a></p><p>Full training history is available here: <a class=link href=https://github.com/volotat/Vector-Based-Language/blob/main/article/history.csv target=_blank rel=noopener>https://github.com/volotat/Vector-Based-Language/blob/main/article/history.csv</a></p><h2 id=thoughts>Thoughts</h2><p>To my knowledge, that is the only example of a synthetic language that possesses a notion of continuity where words and even sentences could blend into each other. I suggest that such language should be interesting even as an example that such a thing is possible in principle. I hope this may make someone fascinated by the idea to explore it further in their own right.</p><p>I think that the most promising aspect of such language is the combination of it with some sort of brain-computer interface. The obvious downside of the language at the current stage is that there is no clear way to produce the image directly from the intent rather than words. With good enough BCI, this problem could be solved. Therefore such language might be the first way to store thoughts in human-readable format.</p><p>With the rise of foundation models, and especially text2image generators, such as <a class=link href=https://github.com/CompVis/stable-diffusion target=_blank rel=noopener>Stable Diffusion</a>, there is a need to produce and store concepts that could not be easily conveyed with words but still could be represented as embeddings. There is recent work called <a class=link href=https://github.com/rinongal/textual_inversion target=_blank rel=noopener>Textual Inversion</a> that tries to solve this exact problem. With language like this, we could visualize these embeddings in a meaningful way and even use it as a common concept space for many models in the future.</p><p>The language might be significantly improved to be more human-friendly. For example, we can use more advanced models such as CLIP (to be more specific, an image encoder part of it) as a visual model at the training stage to produce more readable images. Or we could use a set of predefined text-image pairs as anchors for more controllable outputs.</p><p>And lastly, to really show that generated images represent the meaning of the words rather than the words themselves…</p><p><img src=/p/can-humans-speak-the-language-of-the-machines/1_0yCretXbrfvCMniTT8T8LA.png width=1400 height=284 srcset="/p/can-humans-speak-the-language-of-the-machines/1_0yCretXbrfvCMniTT8T8LA_hu_61fbd085c439aa35.png 480w, /p/can-humans-speak-the-language-of-the-machines/1_0yCretXbrfvCMniTT8T8LA_hu_1af17d4d56362676.png 1024w" loading=lazy class=gallery-image data-flex-grow=492 data-flex-basis=1183px></p></section><footer class=article-footer><section class=article-tags><a href=/tags/machine-learning/>Machine Learning</a>
<a href=/tags/transhumanism/>Transhumanism</a>
<a href=/tags/ai/>AI</a>
<a href=/tags/data-science/>Data Science</a>
<a href=/tags/neural-networks/>Neural Networks</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=[".main-article",".widget--toc"];e.forEach(e=>{const t=document.querySelector(e);t&&renderMathInElement(t,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/projects/2022-08-27-vector-based-language/><div class=article-image><img src=/projects/2022-08-27-vector-based-language/1_lgXj1-BGYkDVqLytrz9D9A.249e78a1e6e495919e5015586b50b8f7_hu_fa51a422ca9aef12.png width=250 height=150 loading=lazy alt="Featured image of post Vector Based Language" data-hash="md5-JJ54oebklZGeUBVYa1C49w=="></div><div class=article-details><h2 class=article-title>Vector Based Language</h2></div></a></article><article class=has-image><a href=/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/><div class=article-image><img src=/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/0_4fPJHVhCQeYNef_n.da92564886320ed49d56f833caf2577a_hu_e2f6d36c91f45fd1.png width=250 height=150 loading=lazy alt="Featured image of post There is a way to make training LLMs way cheaper and more accessible." data-key=there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible data-hash="md5-2pJWSIYyDtSdVvgzyvJXeg=="></div><div class=article-details><h2 class=article-title>There is a way to make training LLMs way cheaper and more accessible.</h2></div></a></article><article><a href=/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/><div class=article-details><h2 class=article-title>Combining GPT-3 and Stable Diffusion to imagine a next level game engine.</h2></div></a></article><article class=has-image><a href=/p/anagnorisis-part-2-the-music-recommendation-algorithm/><div class=article-image><img src=/p/anagnorisis-part-2-the-music-recommendation-algorithm/AI.de83d307b10340cbb92561b9f067ad1a_hu_e4043a00b95d39be.jpg width=250 height=150 loading=lazy alt="Featured image of post Anagnorisis. Part 2: The Music Recommendation Algorithm." data-key=anagnorisis-part-2-the-music-recommendation-algorithm data-hash="md5-3oPTB7EDQMu5JWG58GetGg=="></div><div class=article-details><h2 class=article-title>Anagnorisis. Part 2: The Music Recommendation Algorithm.</h2></div></a></article><article class=has-image><a href=/p/anagnorisis-part-1-a-vision-for-better-information-management/><div class=article-image><img src=/p/anagnorisis-part-1-a-vision-for-better-information-management/cover.a153412765412022eb17513339d5d683_hu_fa197dff236b2a25.jpg width=250 height=150 loading=lazy alt="Featured image of post Anagnorisis. Part 1: A Vision for Better Information Management." data-key=anagnorisis-part-1-a-vision-for-better-information-management data-hash="md5-oVNBJ2VBICLrF1EzOdXWgw=="></div><div class=article-details><h2 class=article-title>Anagnorisis. Part 1: A Vision for Better Information Management.</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2017 -
2026 Alexey Borsky</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.33.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.c922af694cc257bf1ecc41c0dd7b0430f9114ec280ccf67cd2c6ad55f5316c4e.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>