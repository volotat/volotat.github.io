<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="A proposal for a new architecture in training Large Language Models that could drastically reduce costs and democratize access to advanced AI capabilities."><title>Alexey Borsky | There is a way to make training LLMs way cheaper and more accessible.</title><link rel=canonical href=https://volotat.github.io/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/><link rel=stylesheet href=/scss/style.min.833d6eed45de56f48306bf57268d5b8cdfc8a60e8e7bdc99810464fcd033f7c6.css><meta property='og:title' content="There is a way to make training LLMs way cheaper and more accessible."><meta property='og:description' content="A proposal for a new architecture in training Large Language Models that could drastically reduce costs and democratize access to advanced AI capabilities."><meta property='og:url' content='https://volotat.github.io/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/'><meta property='og:site_name' content='Alexey Borsky'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='AI'><meta property='article:tag' content='LLM'><meta property='article:tag' content='Machine Learning'><meta property='article:tag' content='Future Of Ai'><meta property='article:tag' content='Embedding'><meta property='article:published_time' content='2025-08-21T00:00:00+00:00'><meta property='article:modified_time' content='2025-08-21T00:00:00+00:00'><meta property='og:image' content='https://volotat.github.io/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/0_4fPJHVhCQeYNef_n.png'><meta name=twitter:title content="There is a way to make training LLMs way cheaper and more accessible."><meta name=twitter:description content="A proposal for a new architecture in training Large Language Models that could drastically reduce costs and democratize access to advanced AI capabilities."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://volotat.github.io/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/0_4fPJHVhCQeYNef_n.png'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_e5db16934e59d865.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Alexey Borsky</a></h1><h2 class=site-description>Self taught programmer and ML engineer. Active open-source contributor. Mostly interested in procedural generation and generative AI.</h2></div></header><ol class=menu-social><li><a href=https://github.com/volotat target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://x.com/volotat target=_blank title=X rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-x"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4l11.733 16H20L8.267 4z"/><path d="M4 20l6.768-6.768m2.46-2.46L20 4"/></svg></a></li><li><a href=https://www.youtube.com/@volotat target=_blank title=YouTube rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-youtube"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M2 8a4 4 0 014-4h12a4 4 0 014 4v8a4 4 0 01-4 4H6a4 4 0 01-4-4V8z"/><path d="M10 9l5 3-5 3z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/projects/><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-file-code"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14 3v4a1 1 0 001 1h4"/><path d="M17 21H7a2 2 0 01-2-2V5a2 2 0 012-2h7l5 5v11a2 2 0 01-2 2z"/><path d="M10 13l-1 2 1 2"/><path d="M14 13l1 2-1 2"/></svg>
<span>Projects</span></a></li><li><a href=/archives/><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-notebook"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M6 4h11a2 2 0 012 2v12a2 2 0 01-2 2H6a1 1 0 01-1-1V5a1 1 0 011-1m3 0v18"/><path d="M13 8h2"/><path d="M13 12h2"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#how-a-perfect-llm-should-look>How a “Perfect” LLM Should Look</a></li><li><a href=#universal-relational-embedding-space>Universal, Relational Embedding Space</a></li><li><a href=#we-need-the-embedding-model>We Need THE Embedding Model</a></li><li><a href=#the-benefits-would-be-transformative>The Benefits Would Be Transformative</a></li><li><a href=#a-call-for-a-new-direction>A Call for a New Direction</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/><img src=/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/0_4fPJHVhCQeYNef_n_hu_1fb472b0d7619e65.png srcset="/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/0_4fPJHVhCQeYNef_n_hu_1fb472b0d7619e65.png 800w, /p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/0_4fPJHVhCQeYNef_n_hu_c3cdae8acb995a9c.png 1600w" width=800 height=445 loading=lazy alt="Featured image of post There is a way to make training LLMs way cheaper and more accessible."></a></div><div class=article-details><header class=article-category><a href=/categories/ideas/ style=background-color:#e69326;color:#fff>Ideas</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/>There is a way to make training LLMs way cheaper and more accessible.</a></h2><h3 class=article-subtitle>A proposal for a new architecture in training Large Language Models that could drastically reduce costs and democratize access to advanced AI capabilities.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published datetime=2025-08-21T00:00:00Z>Aug 21, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>6 minute read</time></div></footer></div></header><section class=article-content><h2 id=introduction>Introduction</h2><p>What if I told you that a vast amount of the computation that goes into training every new Large Language Model is completely redundant?</p><p>This isn’t just a minor inefficiency, it’s a fundamental flaw in our current approach. We are spending billions of dollars and burning gigawatts of energy for every major AI company to teach their models the same fundamental understanding of the world, over and over again.</p><p>Here is the claim I am going to justify: <strong>the hardest part of what all LLMs learn is NOT next-token prediction, but rather the arbitrary data compression into a meaningful vector representation.</strong> And here’s the catch — this representation is fundamentally the same for every model (up to symmetries and precision).</p><p>The work that inspires this claim is <a class=link href=https://arxiv.org/pdf/2405.07987 target=_blank rel=noopener>the <strong>Platonic Representation Hypothesis</strong></a>. This hypothesis suggests that for any given data, there exists a “perfect” or ideal mathematical representation — a Platonic form, if you will. All our current models are simply trying to find their own noisy, imperfect approximation of this one true representation.</p><h2 id=how-a-perfect-llm-should-look>How a “Perfect” LLM Should Look</h2><p>If the Platonic Representation Hypothesis holds true, it means we’re building our models upside down. We train monolithic, end-to-end models that learn to embed data and reason about it at the same time. Instead, we should separate these concerns.</p><p>The future of efficient AI should be built on a simple, two-part architecture:</p><ol><li><strong>The Universal Encoder:</strong> A single, global, state-of-the-art model that does only one thing: convert any piece of data (text, images, sound, etc., in a continuous sequence) into its perfect “Platonic” vector embedding. This model would be trained once on a colossal dataset and then frozen, serving as a foundational piece of public infrastructure.</li><li><strong>The Task Model:</strong> A much smaller, specialized model that takes these “perfect” embedding (that represents all the current context window that goes to the model) as an input and learns to perform a specific task. This could be next-token prediction, classification, image denoising (for diffusion models), or even complex, one-shot reasoning like question answering or code generation. All personalization, alignment, and RLHF would happen at this much cheaper, more efficient level.</li></ol><p><img src=/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/1_x7iLYZUfjMjV7oNkTjRShA.png width=444 height=532 srcset="/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/1_x7iLYZUfjMjV7oNkTjRShA_hu_e5f393929ad642bc.png 480w, /p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/1_x7iLYZUfjMjV7oNkTjRShA_hu_b60bb99d25994b49.png 1024w" loading=lazy alt="A. — Current approach. B. — Proposed approach." class=gallery-image data-flex-grow=83 data-flex-basis=200px></p><p>Think of it like this: today, every AI student has to first invent an entire language from scratch — its alphabet, vocabulary, and grammar — before they can even begin to learn to write an essay. In the new paradigm, they all share a common, universal language (the Universal Embeddings) with all the world’s concepts already encoded within it. The students (the Task Models) can then focus immediately on the creative act of writing the essay. Training these task-specific models would be orders of magnitude cheaper and faster.</p><h2 id=universal-relational-embedding-space>Universal, Relational Embedding Space</h2><p>So, how do we find these universal embeddings and represent them in a way that is stable and suitable for any task?</p><p>Right now, embeddings are encoded as a point in a very high-dimensional space. The problem is that the coordinate system of this space is set arbitrarily during the training process. This means every model has its own unique, incompatible space. Training the same model twice will generate two completely different spaces. You can’t just take an embedding from GPT-4 and use it in Gemini; their internal “languages” are different.</p><p>Recent research on the <a class=link href=https://arxiv.org/pdf/2505.12540 target=_blank rel=noopener>Universal Geometry of Embeddings</a> shows that while the coordinate systems are different, the underlying geometric relationships <em>between</em> the embeddings are remarkably similar. We can remap one model’s space onto another. But this is a patch, not a solution. It proves the point: what truly matters isn’t the absolute coordinates of a concept, but its <em>relationships</em> to all other concepts.</p><p>What we truly care about are the <strong>distances</strong>.</p><p>Instead of a vector representing a point in space, what if an embedding vector represented a list of distances?</p><p>Imagine trying to describe the location of your city. You could give its absolute GPS coordinates (e.g., 46.10° N, 19.66° E). This is the current approach — precise, but meaningless without the entire GPS coordinate system.</p><p>Alternatively, you could describe it relationally: “It’s 180km from Budapest, 190km from Belgrade, 560km from Vienna…” This is a <em>distance-based</em> representation. It’s inherently meaningful.</p><p>This is how our Universal Embedding should work. Each vector would not be a coordinate, but a list of distances to a set of universal, canonical “landmark concepts” in the embedding space.</p><p><code>embedding("cat") = [dist_to_concept_1, dist_to_concept_2, dist_to_concept_3, ...]</code></p><p><img src=/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/1_Tgc3uuUVrYti-zqfKCahSw.png width=1400 height=700 srcset="/p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/1_Tgc3uuUVrYti-zqfKCahSw_hu_b7f7318b1358607c.png 480w, /p/there-is-a-way-to-make-training-llms-way-cheaper-and-more-accessible/1_Tgc3uuUVrYti-zqfKCahSw_hu_c5821459ded753bc.png 1024w" loading=lazy alt="captionless image" class=gallery-image data-flex-grow=200 data-flex-basis=480px></p><p>We don’t even need to know what these landmark concepts are explicitly. They can be learned by the model. <strong>The key is that they are ordered by importance, from the most general to the most specific.</strong> The first value might represent the distance to the abstract concept of “objectness,” the second to “living thing,” the tenth to “mammal,” and so on.</p><p>This structure has two incredible properties:</p><ol><li><strong>It’s Stable:</strong> Because it’s based on internal distances, it’s invariant to rotation or translation. Every training run would converge to the same relational representation, creating a stable and universal standard.</li><li><strong>It’s Inherently Composable:</strong> This is very similar to the idea of <a class=link href=https://huggingface.co/papers/2205.13147 target=_blank rel=noopener>Matryoshka Representation Learning</a>. If you need a smaller, less precise embedding, you can just take the first N values of the vector! You’re simply using the distances to the most important landmark concepts, giving you a coarse but still highly effective representation.</li></ol><h2 id=we-need-the-embedding-model>We Need THE Embedding Model</h2><p>The centerpiece of this vision is the <strong>Universal Encoder</strong>, or THE Embedding Model. This wouldn’t be just another model like <code>text-embedding-3-large</code>; it would be a foundational piece of infrastructure for the entire AI ecosystem, akin to the GPS network or the TCP/IP protocol.</p><p>This model, trained on a dataset far beyond the scale of any single company, would create this true distance-based, Matryoshka-style embedding space. This would be the definitive, canonical representation of knowledge.</p><h2 id=the-benefits-would-be-transformative>The Benefits Would Be Transformative</h2><ul><li><strong>Backward Compatibility and Continuous Improvement:</strong> New versions of the Universal Encoder would be released as research progresses. Since each new version is just a better approximation of the same underlying Platonic representation, they should be largely backward compatible. This means you could swap in the new encoder and expect any pre-trained, task-specific models to work with the same or even better performance, with minimal re-training.</li><li><strong>Simplified RAG and Vector Search:</strong> Retrieval-Augmented Generation (RAG) and all vector search applications would be greatly simplified. You would only need a single base embedding model for any type of data. Your text, image, and audio databases would all exist within the same unified, coherent vector space, making cross-modal search and reasoning trivial.</li><li><strong>Democratization of AI:</strong> The colossal cost of training foundational models from scratch would be a one-time, collaborative effort. Researchers, startups, and even individuals could then build powerful, specialized AI applications by training only the small, inexpensive task models on top of the universal embeddings.</li></ul><h2 id=a-call-for-a-new-direction>A Call for a New Direction</h2><p>I truly believe this is the future we must build. With this approach, AI could become far more open, accessible, and environmentally sustainable.</p><p>However, this vision is a direct threat to the power held by the big companies that currently dominate the AI space. Their moat is the sheer scale of their proprietary, monolithically trained models. A shared, universal encoder is not in their immediate financial interest.</p><p>Therefore, this message is a call to action for the global ML community and AI enthusiasts. The creation of such a foundational model will likely not come from a single corporation, but from a decentralized, open-source effort. Only as a global community do we have a shot at creating an AI future that is more efficient, decentralized, and accessible for everyone.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/ai/>AI</a>
<a href=/tags/llm/>LLM</a>
<a href=/tags/machine-learning/>Machine Learning</a>
<a href=/tags/future-of-ai/>Future of Ai</a>
<a href=/tags/embedding/>Embedding</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=[".main-article",".widget--toc"];e.forEach(e=>{const t=document.querySelector(e);t&&renderMathInElement(t,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/combining-gpt-3-and-stable-diffusion-to-imagine-a-next-level-game-engine/><div class=article-details><h2 class=article-title>Combining GPT-3 and Stable Diffusion to imagine a next level game engine.</h2></div></a></article><article class=has-image><a href=/p/can-humans-speak-the-language-of-the-machines/><div class=article-image><img src=/p/can-humans-speak-the-language-of-the-machines/0_Tpm0MlWfLdNMGQ9b.16940e0aa6d9e94f6f5a9503d19df360_hu_8e81a329be74c091.png width=250 height=150 loading=lazy alt="Featured image of post Can Humans Speak the Language of the Machines?" data-key=can-humans-speak-the-language-of-the-machines data-hash="md5-FpQOCqbZ6U9vWpUD0Z3zYA=="></div><div class=article-details><h2 class=article-title>Can Humans Speak the Language of the Machines?</h2></div></a></article><article><a href=/p/neurogrammer-architecture-arc-challenge/><div class=article-details><h2 class=article-title>NeuroGrammer — a proposal of an architecture for solving the ARC challenge.</h2></div></a></article><article class=has-image><a href=/p/the-sunset-of-human-readable-code/><div class=article-image><img src=/p/the-sunset-of-human-readable-code/0_WnVe9xDIVdAL4WG4.d9feb90c6821f131d33df6e9d654a2c8_hu_f5b068aab46c37b5.png width=250 height=150 loading=lazy alt="Featured image of post The Sunset of Human-Readable Code." data-key=the-sunset-of-human-readable-code data-hash="md5-2f65DGgh8THTPfbp1lSiyA=="></div><div class=article-details><h2 class=article-title>The Sunset of Human-Readable Code.</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2017 -
2026 Alexey Borsky</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.33.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.c922af694cc257bf1ecc41c0dd7b0430f9114ec280ccf67cd2c6ad55f5316c4e.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>